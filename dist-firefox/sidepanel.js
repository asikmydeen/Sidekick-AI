var __defProp=Object.defineProperty,__publicField=(obj,key,value)=>((obj,key,value)=>key in obj?__defProp(obj,key,{enumerable:!0,configurable:!0,writable:!0,value:value}):obj[key]=value)(obj,"symbol"!=typeof key?key+"":key,value);!function(){const relList=document.createElement("link").relList;if(!(relList&&relList.supports&&relList.supports("modulepreload"))){for(const link of document.querySelectorAll('link[rel="modulepreload"]'))processPreload(link);new MutationObserver(mutations=>{for(const mutation of mutations)if("childList"===mutation.type)for(const node of mutation.addedNodes)"LINK"===node.tagName&&"modulepreload"===node.rel&&processPreload(node)}).observe(document,{childList:!0,subtree:!0})}function processPreload(link){if(link.ep)return;link.ep=!0;const fetchOpts=function(link){const fetchOpts={};return link.integrity&&(fetchOpts.integrity=link.integrity),link.referrerPolicy&&(fetchOpts.referrerPolicy=link.referrerPolicy),"use-credentials"===link.crossOrigin?fetchOpts.credentials="include":"anonymous"===link.crossOrigin?fetchOpts.credentials="omit":fetchOpts.credentials="same-origin",fetchOpts}(link);fetch(link.href,fetchOpts)}}();const DEFAULT_QUICK_PROMPTS="Summarize|Summarize the key points of the following content:\nExplain|Explain this concept in simple terms:\nFix Grammar|Please fix the grammar in this text:\nCode Review|Review this code for bugs and improvements:";function createId(){return Date.now().toString(36)+Math.random().toString(36).substr(2)}let state={provider:"",model:"",apiKey:"",endpoint:"http://localhost:11434",awsAccessKey:"",awsSecretKey:"",awsSessionToken:"",awsRegion:"us-east-1",systemPrompt:"",temperature:.7,quickPrompts:DEFAULT_QUICK_PROMPTS,theme:"light",autoRead:!1,currentSessionId:null,sessions:[],providerCredentials:{openai:{apiKey:"",selectedModel:"",models:[]},openrouter:{apiKey:"",selectedModel:"",models:[]},anthropic:{apiKey:"",selectedModel:"",models:[]},huggingface:{apiKey:"",selectedModel:"",models:[],task:"chat",taskModels:{}},ollama:{endpoint:"http://localhost:11434",selectedModel:"",models:[]}}};const HF_TASK_MODELS={chat:["meta-llama/Llama-3.2-3B-Instruct","meta-llama/Llama-3.1-8B-Instruct","Qwen/Qwen2.5-72B-Instruct","mistralai/Mistral-7B-Instruct-v0.3","deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"],"text-to-image":["Tongyi-MAI/Z-Image-Turbo","black-forest-labs/FLUX.1-schnell","black-forest-labs/FLUX.1-dev","stabilityai/stable-diffusion-xl-base-1.0","runwayml/stable-diffusion-v1-5"],"image-to-image":["black-forest-labs/FLUX.1-dev","black-forest-labs/FLUX.1-schnell","stabilityai/stable-diffusion-xl-refiner-1.0","timbrooks/instruct-pix2pix","lllyasviel/sd-controlnet-canny"],"image-to-text":["Salesforce/blip-image-captioning-large","Salesforce/blip-image-captioning-base","nlpconnect/vit-gpt2-image-captioning","microsoft/git-base"],"text-to-speech":["facebook/mms-tts-eng","espnet/kan-bayashi_ljspeech_vits","microsoft/speecht5_tts","suno/bark-small"],"speech-to-text":["openai/whisper-large-v3","openai/whisper-medium","openai/whisper-small","facebook/wav2vec2-large-960h-lv60-self"],"text-to-video":["ali-vilab/text-to-video-ms-1.7b","cerspense/zeroscope_v2_576w","ByteDance/AnimateDiff-Lightning","hotshotco/Hotshot-XL"]};function getProviderCredentials(provider){return state.providerCredentials&&state.providerCredentials[provider]?{...state.providerCredentials[provider]}:null}function updateProviderCredentials(provider,credentials){state.providerCredentials||(state.providerCredentials={}),state.providerCredentials[provider]||(state.providerCredentials[provider]={}),state.providerCredentials[provider]={...state.providerCredentials[provider],...credentials},saveState()}function getCurrentProviderCredentials(){return getProviderCredentials(state.provider)}function saveState(){chrome.storage.local.set({chatState:state})}function updateState(updates){state={...state,...updates},saveState()}function createNewSession(){const newSession={id:createId(),title:"New Chat",messages:[],lastModified:Date.now()};return state.sessions.unshift(newSession),state.currentSessionId=newSession.id,saveState(),newSession}function getCurrentSession(){return state.sessions.find(s=>s.id===state.currentSessionId)}function updateCurrentSession(messages){const session=getCurrentSession();if(session){if(session.messages=messages,session.lastModified=Date.now(),"New Chat"===session.title&&messages.length>0){const firstUserMsg=messages.find(m=>"user"===m.role);if(firstUserMsg){let txt=firstUserMsg.content;Array.isArray(txt)&&(txt=txt.find(p=>"text"===p.type)?.text||"Image"),session.title=txt.slice(0,30)+(txt.length>30?"...":"")}}saveState()}}function updateSessionTitle(sessionId,title){const session=state.sessions.find(s=>s.id===sessionId);session&&title&&(session.title=title,session.needsAITitle=!1,saveState())}const elements={container:document.querySelector(".container"),configSection:document.getElementById("configSection"),chatSection:document.getElementById("chatSection"),advancedSettings:document.getElementById("advancedSettings"),providerSelect:document.getElementById("provider"),apiKeyInput:document.getElementById("apiKey"),apiKeyGroup:document.getElementById("apiKeyGroup"),endpointInput:document.getElementById("endpoint"),endpointGroup:document.getElementById("endpointGroup"),hfTaskGroup:document.getElementById("hfTaskGroup"),hfTaskSelect:document.getElementById("hfTask"),hfTaskHint:document.getElementById("hfTaskHint"),hfProviderGroup:document.getElementById("hfProviderGroup"),hfProviderSelect:document.getElementById("hfProvider"),fetchModelsBtn:document.getElementById("fetchModelsBtn"),fetchStatus:document.getElementById("fetchStatus"),modelSelect:document.getElementById("model"),modelSelectionDiv:document.getElementById("modelSelection"),manualModelGroup:document.getElementById("manualModelGroup"),manualModelInput:document.getElementById("manualModel"),saveModelBtn:document.getElementById("saveModelBtn"),modelHint:document.getElementById("modelHint"),systemPromptInput:document.getElementById("systemPrompt"),temperatureInput:document.getElementById("temperature"),tempValueLabel:document.getElementById("tempValue"),quickPromptsInput:document.getElementById("quickPromptsConfig"),autoReadInput:document.getElementById("autoRead"),startChatBtn:document.getElementById("startChatBtn"),settingsBtn:document.getElementById("settingsBtn"),historyBtn:document.getElementById("historyBtn"),newChatBtn:document.getElementById("newChatBtn"),historySidebar:document.getElementById("historySidebar"),historySearch:document.getElementById("historySearch"),sessionList:document.getElementById("sessionList"),closeSidebarBtn:document.getElementById("closeSidebarBtn"),clearHistoryBtn:document.getElementById("clearHistoryBtn"),exportBtn:document.getElementById("exportBtn"),expandBtn:document.getElementById("expandBtn"),collapseBtn:document.getElementById("collapseBtn"),themeBtn:document.getElementById("themeBtn"),chatHistory:document.getElementById("chatHistory"),messageInput:document.getElementById("messageInput"),sendBtn:document.getElementById("sendBtn"),stopBtn:document.getElementById("stopBtn"),fileBtn:document.getElementById("fileBtn"),fileInput:document.getElementById("fileInput"),includePageContent:document.getElementById("includePageContent"),pageContextBtn:document.getElementById("pageContextBtn"),promptChipsContainer:document.getElementById("promptChips"),tokenCount:document.getElementById("tokenCount"),attachmentPreview:document.getElementById("attachmentPreview"),mediaGallery:document.getElementById("mediaGallery"),galleryGrid:document.getElementById("galleryGrid"),mediaLightbox:document.getElementById("mediaLightbox"),closeLightbox:document.getElementById("closeLightbox"),lightboxPrompt:document.getElementById("lightboxPrompt"),lightboxDownload:document.getElementById("lightboxDownload")};function applyTheme(theme){"dark"===theme?(document.body.classList.add("dark-mode"),elements.themeBtn.textContent="‚òÄÔ∏è"):(document.body.classList.remove("dark-mode"),elements.themeBtn.textContent="üåô")}function showStatus(msg,type){elements.fetchStatus.textContent=msg,elements.fetchStatus.className=`status-msg ${type}`}function toggleView(view){"chat"===view?(elements.configSection.classList.add("hidden"),elements.chatSection.classList.remove("hidden"),elements.settingsBtn.style.display="block",elements.historyBtn.classList.remove("hidden"),elements.newChatBtn.classList.remove("hidden"),elements.exportBtn.classList.remove("hidden")):(elements.configSection.classList.remove("hidden"),elements.chatSection.classList.add("hidden"),elements.settingsBtn.style.display="none",elements.historyBtn.classList.add("hidden"),elements.newChatBtn.classList.add("hidden"),elements.exportBtn.classList.add("hidden"))}function toggleSidebar(show){show?elements.historySidebar.classList.add("open"):elements.historySidebar.classList.remove("open")}function downloadMedia(dataUrl,filename){const a=document.createElement("a");a.href=dataUrl,a.download=filename,document.body.appendChild(a),a.click(),document.body.removeChild(a)}function openLightbox(url,type,prompt=""){if(!elements.mediaLightbox)return;const lightboxContent=elements.mediaLightbox.querySelector(".lightbox-content");if(lightboxContent.innerHTML="","video"===type){const video=document.createElement("video");video.src=url,video.controls=!0,video.autoplay=!0,video.className="lightbox-video",lightboxContent.appendChild(video)}else if("image"===type){const img=document.createElement("img");img.src=url,img.className="lightbox-image",lightboxContent.appendChild(img)}elements.lightboxPrompt&&(elements.lightboxPrompt.textContent=prompt),elements.lightboxDownload&&(elements.lightboxDownload.onclick=()=>{downloadMedia(url,`generated-${type}.${"video"===type?"mp4":"png"}`)}),elements.mediaLightbox.classList.remove("hidden");const handleEscape=e=>{"Escape"===e.key&&closeLightbox()};document.addEventListener("keydown",handleEscape),elements.mediaLightbox._escapeHandler=handleEscape}function closeLightbox(){if(!elements.mediaLightbox)return;elements.mediaLightbox.classList.add("hidden");const lightboxContent=elements.mediaLightbox.querySelector(".lightbox-content");lightboxContent&&(lightboxContent.innerHTML=""),elements.mediaLightbox._escapeHandler&&document.removeEventListener("keydown",elements.mediaLightbox._escapeHandler)}function isFullPageMode(){return window.innerWidth>500}function createMediaCard(type,url,prompt){const card=document.createElement("div");if(card.className=`media-card ${type}-card`,"image"===type){const img=document.createElement("img");img.src=url,img.alt=prompt||"Generated image",img.onclick=()=>openLightbox(url,"image",prompt),card.appendChild(img)}else if("video"===type){const video=document.createElement("video");video.src=url,video.muted=!0,video.loop=!0,video.onmouseenter=()=>video.play(),video.onmouseleave=()=>video.pause(),video.onclick=()=>openLightbox(url,"video",prompt),card.appendChild(video)}const overlay=document.createElement("div");return overlay.className="media-card-overlay",overlay.innerHTML=`\n    <span class="media-type-badge">${"image"===type?"üé®":"üé¨"}</span>\n    <p class="media-prompt">${prompt?prompt.slice(0,50)+(prompt.length>50?"...":""):""}</p>\n  `,card.appendChild(overlay),card}function renderSessionList(sessions,currentId,onSwitch,onDelete,searchQuery=""){elements.sessionList.innerHTML="";const filteredSessions=searchQuery.trim()?sessions.filter(session=>{const query=searchQuery.toLowerCase();return!!(session.title||"").toLowerCase().includes(query)||session.messages?.some(msg=>("string"==typeof msg.content?msg.content:msg.content?.find(c=>"text"===c.type)?.text||"").toLowerCase().includes(query))}):sessions;if(0===filteredSessions.length){const noResults=document.createElement("div");return noResults.className="no-results",noResults.textContent=searchQuery?"No chats found":"No chat history yet",void elements.sessionList.appendChild(noResults)}filteredSessions.forEach(session=>{const item=document.createElement("div");item.className="session-item "+(session.id===currentId?"active":"");let displayTitle=session.title||"New Chat";const title=document.createElement("span");title.className="session-title",title.textContent=displayTitle,title.onclick=()=>onSwitch(session.id);const delBtn=document.createElement("button");delBtn.textContent="√ó",delBtn.className="session-delete",delBtn.onclick=e=>{e.stopPropagation(),onDelete(session.id)},item.appendChild(title),item.appendChild(delBtn),elements.sessionList.appendChild(item)})}function populateModelSelect(models){elements.modelSelect.innerHTML='<option value="" disabled selected>Select a model...</option>',models.forEach(m=>{const option=document.createElement("option");option.value=m,option.textContent=m,elements.modelSelect.appendChild(option)})}elements.closeLightbox&&elements.closeLightbox.addEventListener("click",closeLightbox);let messageCallbacks={onEdit:null,onRegenerate:null};function renderChat(messages,modelName){elements.chatHistory.innerHTML="";const intro=document.createElement("div");intro.className="status-msg",intro.textContent=`Chatting with ${modelName}`,elements.chatHistory.appendChild(intro),messages.forEach((msg,index)=>{appendMessageToDOM(msg.role,msg.content,null,!1,index)}),scrollToBottom(),updateTokenCount(messages)}function appendMessageToDOM(role,content,id=null,isLoading=!1,msgIndex=-1){const div=document.createElement("div");div.className=`message ${role}`,id&&(div.id=id),msgIndex>=0&&(div.dataset.msgIndex=msgIndex);let textContent="";if("string"==typeof content?textContent=content:Array.isArray(content)&&(textContent=content.find(c=>"text"===c.type)?.text||""),isLoading)div.innerHTML='<div class="typing-dots"><span></span><span></span><span></span></div>';else{Array.isArray(content)?content.forEach(part=>{if("text"===part.type){const p=document.createElement("div");p.innerHTML=parseMarkdown(part.text),div.appendChild(p)}else if("image_url"===part.type){const img=document.createElement("img");img.src=part.image_url.url,img.className="chat-image",div.appendChild(img)}else if("generated_image"===part.type){const container=document.createElement("div");container.className="generated-media-container";const img=document.createElement("img");img.src=part.url,img.className="generated-image",img.alt=part.prompt||"Generated image",img.onclick=()=>openLightbox(part.url,"image",part.prompt),container.appendChild(img);const expandBtn=document.createElement("button");expandBtn.className="media-btn",expandBtn.textContent="üî≤ Full View",expandBtn.onclick=()=>openLightbox(part.url,"image",part.prompt),container.appendChild(expandBtn);const downloadBtn=document.createElement("button");downloadBtn.className="media-btn",downloadBtn.textContent="‚¨áÔ∏è Download",downloadBtn.onclick=()=>downloadMedia(part.url,"generated-image.png"),container.appendChild(downloadBtn),div.appendChild(container)}else if("generated_audio"===part.type){const container=document.createElement("div");container.className="generated-media-container";const audio=document.createElement("audio");audio.src=part.url,audio.controls=!0,audio.className="generated-audio",container.appendChild(audio);const downloadBtn=document.createElement("button");downloadBtn.className="media-btn",downloadBtn.textContent="‚¨áÔ∏è Download",downloadBtn.onclick=()=>downloadMedia(part.url,"generated-audio.wav"),container.appendChild(downloadBtn),div.appendChild(container)}else if("generated_video"===part.type){const container=document.createElement("div");container.className="generated-media-container video-container";const video=document.createElement("video");video.src=part.url,video.controls=!0,video.autoplay=!1,video.loop=!0,video.muted=!1,video.className="generated-video",video.title=part.prompt||"Generated video",container.appendChild(video);const expandBtn=document.createElement("button");expandBtn.className="media-btn",expandBtn.textContent="üî≤ Full View",expandBtn.onclick=()=>openLightbox(part.url,"video",part.prompt),container.appendChild(expandBtn);const downloadBtn=document.createElement("button");downloadBtn.className="media-btn",downloadBtn.textContent="‚¨áÔ∏è Download",downloadBtn.onclick=()=>downloadMedia(part.url,"generated-video.mp4"),container.appendChild(downloadBtn),div.appendChild(container)}else if("audio_input"===part.type){const audio=document.createElement("audio");audio.src=part.url,audio.controls=!0,audio.className="input-audio",div.appendChild(audio)}}):content&&(div.innerHTML=parseMarkdown(content));const controls=document.createElement("div");if(controls.className="msg-controls","user"===role&&msgIndex>=0){const editBtn=document.createElement("button");editBtn.className="msg-btn edit-btn",editBtn.textContent="‚úèÔ∏è",editBtn.title="Edit message",editBtn.onclick=()=>{messageCallbacks.onEdit&&messageCallbacks.onEdit(msgIndex,textContent)},controls.appendChild(editBtn)}if("assistant"===role){const getTextContent=()=>{if(textContent)return textContent;return(div.querySelector("div")||div).textContent||""},speakBtn=document.createElement("button");if(speakBtn.className="msg-btn",speakBtn.textContent="üîä",speakBtn.title="Read aloud",speakBtn.onclick=()=>toggleSpeech(getTextContent(),speakBtn),controls.appendChild(speakBtn),msgIndex>=0){const regenBtn=document.createElement("button");regenBtn.className="msg-btn regen-btn",regenBtn.textContent="üîÑ",regenBtn.title="Regenerate response",regenBtn.onclick=()=>{messageCallbacks.onRegenerate&&messageCallbacks.onRegenerate(msgIndex)},controls.appendChild(regenBtn)}const copyBtn=document.createElement("button");copyBtn.className="msg-btn",copyBtn.textContent="üìã",copyBtn.title="Copy response",copyBtn.onclick=()=>{navigator.clipboard.writeText(getTextContent()).then(()=>{copyBtn.textContent="‚úì",setTimeout(()=>copyBtn.textContent="üìã",2e3)})},controls.appendChild(copyBtn)}controls.childNodes.length>0&&div.appendChild(controls)}elements.chatHistory.appendChild(div),div.querySelectorAll(".copy-btn").forEach(btn=>{btn.addEventListener("click",e=>{const code=e.target.parentElement.querySelector("code").textContent;navigator.clipboard.writeText(code);const originalText=e.target.textContent;e.target.textContent="Copied!",e.target.classList.add("copied"),setTimeout(()=>{e.target.textContent=originalText,e.target.classList.remove("copied")},2e3)})}),scrollToBottom()}function updateStreamingMessage(id,text){const el=document.getElementById(id);el&&(el.innerHTML=parseMarkdown(text),scrollToBottom())}function finalizeMessageInDOM(id,content){const el=document.getElementById(id);if(el){el.innerHTML="";let textContent="";"string"==typeof content&&(textContent=content);const p=document.createElement("div");p.innerHTML=parseMarkdown(textContent),el.appendChild(p);const controls=document.createElement("div");controls.className="msg-controls";const speakBtn=document.createElement("button");speakBtn.className="msg-btn",speakBtn.textContent="üîä",speakBtn.title="Read aloud",speakBtn.onclick=()=>toggleSpeech(textContent,speakBtn),controls.appendChild(speakBtn),el.appendChild(controls),el.querySelectorAll(".copy-btn").forEach(btn=>{btn.addEventListener("click",e=>{const code=e.target.parentElement.querySelector("code").textContent;navigator.clipboard.writeText(code);const originalText=e.target.textContent;e.target.textContent="Copied!",e.target.classList.add("copied"),setTimeout(()=>{e.target.textContent=originalText,e.target.classList.remove("copied")},2e3)})}),scrollToBottom()}}function toggleSpeech(text,btn){if(window.speechSynthesis.speaking)return window.speechSynthesis.cancel(),void document.querySelectorAll(".msg-btn.speaking").forEach(b=>{b.classList.remove("speaking"),b.textContent="üîä"});speakText(text,btn)}function speakText(text,btn=null){window.speechSynthesis.cancel();const cleanText=text.replace(/[*#`_\[\]]/g,""),utterance=new SpeechSynthesisUtterance(cleanText);utterance.lang="en-US",utterance.rate=1,utterance.onstart=()=>{btn&&(btn.classList.add("speaking"),btn.textContent="‚èπ")},utterance.onend=()=>{btn&&(btn.classList.remove("speaking"),btn.textContent="üîä")},utterance.onerror=()=>{btn&&(btn.classList.remove("speaking"),btn.textContent="üîä")},window.speechSynthesis.speak(utterance)}function stopSpeaking(){window.speechSynthesis.cancel()}function updateTokenCount(messages){let totalTxt="";messages.forEach(m=>{"string"==typeof m.content?totalTxt+=m.content:Array.isArray(m.content)&&m.content.forEach(c=>{"text"===c.type&&(totalTxt+=c.text),"image_url"===c.type&&(totalTxt+=" ".repeat(4e3))})});const count=(text=totalTxt,Math.ceil(text.length/4));var text;const percent=Math.min(count/128e3*100,100).toFixed(1);elements.tokenCount.textContent=count>=1e3?`${(count/1e3).toFixed(1)}k`:count.toString(),elements.tokenCount.title=`${count.toLocaleString()} tokens (~${percent}% of context)`}function removeMessage(id){const el=document.getElementById(id);el&&el.remove()}function toggleLoading(isLoading){isLoading?(elements.sendBtn.classList.add("hidden"),elements.stopBtn.classList.remove("hidden"),elements.messageInput.disabled=!0):(elements.sendBtn.classList.remove("hidden"),elements.stopBtn.classList.add("hidden"),elements.messageInput.disabled=!1,elements.messageInput.focus())}function scrollToBottom(){elements.chatHistory.scrollTop=elements.chatHistory.scrollHeight}function autoResizeInput(){const input=elements.messageInput;input.style.height="auto",input.style.height=input.scrollHeight+"px",""===input.value&&(input.style.height="")}function renderAttachments(attachments,onRemove){elements.attachmentPreview.innerHTML="",attachments.forEach((att,index)=>{const thumb=document.createElement("div");if(thumb.className="preview-thumb","audio"===att.type){const audioIcon=document.createElement("div");audioIcon.className="audio-preview-icon",audioIcon.textContent="üéµ",audioIcon.title=att.name||"Audio file",thumb.appendChild(audioIcon)}else{const img=document.createElement("img");img.src=att.base64,thumb.appendChild(img)}const btn=document.createElement("button");btn.className="remove-btn",btn.textContent="√ó",btn.onclick=()=>onRemove(index),thumb.appendChild(btn),elements.attachmentPreview.appendChild(thumb)})}function parseMarkdown(text){if(!text)return"";let safeText=text.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;");const thinkBlocks=[];safeText=safeText.replace(/&lt;think&gt;([\s\S]*?)&lt;\/think&gt;/g,(_match,content)=>(thinkBlocks.push({content:content.trim(),isComplete:!0}),`__THINKBLOCK_${thinkBlocks.length-1}__`)),safeText=safeText.replace(/&lt;think&gt;([\s\S]*)$/g,(_match,content)=>(thinkBlocks.push({content:content.trim(),isComplete:!1}),`__THINKBLOCK_${thinkBlocks.length-1}__`));const codeBlocks=[];return safeText=safeText.replace(/```([\s\S]*?)```/g,(_match,code)=>(codeBlocks.push(code),`__CODEBLOCK_${codeBlocks.length-1}__`)),safeText=safeText.replace(/^### (.*$)/gm,"<h3>$1</h3>"),safeText=safeText.replace(/^## (.*$)/gm,"<h2>$1</h2>"),safeText=safeText.replace(/^# (.*$)/gm,"<h1>$1</h1>"),safeText=safeText.replace(/^[\*\-] (.*$)/gm,"<ul><li>$1</li></ul>"),safeText=safeText.replace(/`([^`]+)`/g,'<code style="background:rgba(128,128,128,0.2);padding:2px 4px;border-radius:4px;">$1</code>'),safeText=safeText.replace(/\*\*([^*]+)\*\*/g,"<strong>$1</strong>"),safeText=safeText.replace(/\*([^*]+)\*/g,"<em>$1</em>"),safeText=safeText.replace(/\[([^\]]+)\]\(([^)]+)\)/g,'<a href="$2" target="_blank">$1</a>'),safeText=safeText.replace(/\n/g,"<br>"),safeText=safeText.replace(/__CODEBLOCK_(\d+)__/g,(_match,index)=>`<div class="code-wrapper"><button class="copy-btn">Copy</button><pre><code>${codeBlocks[index]}</code></pre></div>`),safeText=safeText.replace(/__THINKBLOCK_(\d+)__/g,(_match,index)=>{const blockId=`think-${Date.now()}-${index}`,block=thinkBlocks[index],content=block.content.replace(/\n/g,"<br>").replace(/&lt;br&gt;/g,"<br>"),streamingClass=block.isComplete?"":" streaming",statusText=block.isComplete?"Thinking Process":"Thinking...";return`<div class="think-block${streamingClass}${block.isComplete?"":" expanded"}" id="${blockId}" data-think-block="true">\n      <div class="think-header">\n        <span class="think-icon">${block.isComplete?"üí≠":"‚è≥"}</span>\n        <span class="think-label">${statusText}</span>\n        <span class="think-toggle">‚ñº</span>\n      </div>\n      <div class="think-content">\n        <div class="think-content-inner">${content||'<em style="opacity:0.6">No thinking content</em>'}</div>\n      </div>\n    </div>`}),safeText}var __defProp2=Object.defineProperty,__export=(target,all)=>{for(var name in all)__defProp2(target,name,{get:all[name],enumerable:!0})},tasks_exports={};__export(tasks_exports,{audioClassification:()=>audioClassification,audioToAudio:()=>audioToAudio,automaticSpeechRecognition:()=>automaticSpeechRecognition,chatCompletion:()=>chatCompletion,chatCompletionStream:()=>chatCompletionStream,documentQuestionAnswering:()=>documentQuestionAnswering,featureExtraction:()=>featureExtraction,fillMask:()=>fillMask,imageClassification:()=>imageClassification,imageSegmentation:()=>imageSegmentation,imageToImage:()=>imageToImage$1,imageToText:()=>imageToText$1,imageToVideo:()=>imageToVideo,objectDetection:()=>objectDetection,questionAnswering:()=>questionAnswering,request:()=>request,sentenceSimilarity:()=>sentenceSimilarity,streamingRequest:()=>streamingRequest,summarization:()=>summarization,tableQuestionAnswering:()=>tableQuestionAnswering,tabularClassification:()=>tabularClassification,tabularRegression:()=>tabularRegression,textClassification:()=>textClassification,textGeneration:()=>textGeneration,textGenerationStream:()=>textGenerationStream,textToImage:()=>textToImage$1,textToSpeech:()=>textToSpeech$1,textToVideo:()=>textToVideo$1,tokenClassification:()=>tokenClassification,translation:()=>translation,visualQuestionAnswering:()=>visualQuestionAnswering,zeroShotClassification:()=>zeroShotClassification,zeroShotImageClassification:()=>zeroShotImageClassification});var HF_HUB_URL="https://huggingface.co",HF_ROUTER_URL="https://router.huggingface.co",HF_ROUTER_AUTO_ENDPOINT=`${HF_ROUTER_URL}/v1`,HF_HEADER_X_BILL_TO="X-HF-Bill-To",HARDCODED_MODEL_INFERENCE_MAPPING={baseten:{},"black-forest-labs":{},cerebras:{},clarifai:{},cohere:{},"fal-ai":{},"featherless-ai":{},"fireworks-ai":{},groq:{},"hf-inference":{},hyperbolic:{},nebius:{},novita:{},nscale:{},openai:{},publicai:{},ovhcloud:{},replicate:{},sambanova:{},scaleway:{},together:{},wavespeed:{},"zai-org":{}},InferenceClientError=class extends Error{constructor(message){super(message),this.name="InferenceClientError"}},InferenceClientInputError=class extends InferenceClientError{constructor(message){super(message),this.name="InputError"}},InferenceClientRoutingError=class extends InferenceClientError{constructor(message){super(message),this.name="RoutingError"}},InferenceClientHttpRequestError=class extends InferenceClientError{constructor(message,httpRequest,httpResponse){super(message),__publicField(this,"httpRequest"),__publicField(this,"httpResponse"),this.httpRequest={...httpRequest,...httpRequest.headers?{headers:{...httpRequest.headers,..."Authorization"in httpRequest.headers?{Authorization:"Bearer [redacted]"}:void 0}}:void 0},this.httpResponse=httpResponse}},InferenceClientProviderApiError=class extends InferenceClientHttpRequestError{constructor(message,httpRequest,httpResponse){super(message,httpRequest,httpResponse),this.name="ProviderApiError"}},InferenceClientHubApiError=class extends InferenceClientHttpRequestError{constructor(message,httpRequest,httpResponse){super(message,httpRequest,httpResponse),this.name="HubApiError"}},InferenceClientProviderOutputError=class extends InferenceClientError{constructor(message){super(message),this.name="ProviderOutputError"}};function toArray(obj){return Array.isArray(obj)?obj:[obj]}var TaskProviderHelper=class{constructor(provider,baseUrl,clientSideRoutingOnly=!1){__publicField(this,"provider"),__publicField(this,"baseUrl"),__publicField(this,"clientSideRoutingOnly"),this.provider=provider,this.baseUrl=baseUrl,this.clientSideRoutingOnly=clientSideRoutingOnly}makeBaseUrl(params){return"provider-key"!==params.authMethod?`${HF_ROUTER_URL}/${this.provider}`:this.baseUrl}makeBody(params){return"data"in params.args&&params.args.data?params.args.data:JSON.stringify(this.preparePayload(params))}makeUrl(params){return`${this.makeBaseUrl(params)}/${this.makeRoute(params).replace(/^\/+/,"")}`}prepareHeaders(params,isBinary){const headers={};return"none"!==params.authMethod&&(headers.Authorization=`Bearer ${params.accessToken}`),isBinary||(headers["Content-Type"]="application/json"),headers}},BaseConversationalTask=class extends TaskProviderHelper{constructor(provider,baseUrl,clientSideRoutingOnly=!1){super(provider,baseUrl,clientSideRoutingOnly)}makeRoute(){return"v1/chat/completions"}preparePayload(params){return{...params.args,model:params.model}}async getResponse(response){if("object"==typeof response&&Array.isArray(response?.choices)&&"number"==typeof response?.created&&"string"==typeof response?.id&&"string"==typeof response?.model&&(void 0===response.system_fingerprint||null===response.system_fingerprint||"string"==typeof response.system_fingerprint)&&"object"==typeof response?.usage)return response;throw new InferenceClientProviderOutputError("Expected ChatCompletionOutput")}},BaseTextGenerationTask=class extends TaskProviderHelper{constructor(provider,baseUrl,clientSideRoutingOnly=!1){super(provider,baseUrl,clientSideRoutingOnly)}preparePayload(params){return{...params.args,model:params.model}}makeRoute(){return"v1/completions"}async getResponse(response){const res=toArray(response);if(Array.isArray(res)&&res.length>0&&res.every(x=>"object"==typeof x&&!!x&&"generated_text"in x&&"string"==typeof x.generated_text))return res[0];throw new InferenceClientProviderOutputError("Expected Array<{generated_text: string}>")}},AutoRouterConversationalTask=class extends BaseConversationalTask{constructor(){super("auto","https://router.huggingface.co")}makeBaseUrl(params){if("hf-token"!==params.authMethod)throw new InferenceClientRoutingError("Cannot select auto-router when using non-Hugging Face API key.");return this.baseUrl}};function base64FromBytes(arr){if(globalThis.Buffer)return globalThis.Buffer.from(arr).toString("base64");{const bin=[];return arr.forEach(byte=>{bin.push(String.fromCharCode(byte))}),globalThis.btoa(bin.join(""))}}function typedInclude(arr,v){return arr.includes(v)}function omit(o,props){const propsArr=Array.isArray(props)?props:[props];return function(o,props){return Object.assign({},...props.map(prop=>{if(void 0!==o[prop])return{[prop]:o[prop]}}))}(o,Object.keys(o).filter(prop=>!typedInclude(propsArr,prop)))}var EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS=["feature-extraction","sentence-similarity"],HFInferenceTask=class extends TaskProviderHelper{constructor(){super("hf-inference",`${HF_ROUTER_URL}/hf-inference`)}preparePayload(params){return params.args}makeUrl(params){return params.model.startsWith("http://")||params.model.startsWith("https://")?params.model:super.makeUrl(params)}makeRoute(params){return params.task&&["feature-extraction","sentence-similarity"].includes(params.task)?`models/${params.model}/pipeline/${params.task}`:`models/${params.model}`}async getResponse(response){return response}},globalLogger=console;function getLogger(){return globalLogger}var inferenceProviderMappingCache=new Map;async function fetchInferenceProviderMappingForModel(modelId,accessToken,options){let inferenceProviderMapping;if(inferenceProviderMappingCache.has(modelId))inferenceProviderMapping=inferenceProviderMappingCache.get(modelId);else{const url=`${HF_HUB_URL}/api/models/${modelId}?expand[]=inferenceProviderMapping`,resp=await(options?.fetch??fetch)(url,{headers:accessToken?.startsWith("hf_")?{Authorization:`Bearer ${accessToken}`}:{}});if(!resp.ok){if(!resp.headers.get("Content-Type")?.startsWith("application/json"))throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}`,{url:url,method:"GET"},{requestId:resp.headers.get("x-request-id")??"",status:resp.status,body:await resp.text()});{const error=await resp.json();if("error"in error&&"string"==typeof error.error)throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}: ${error.error}`,{url:url,method:"GET"},{requestId:resp.headers.get("x-request-id")??"",status:resp.status,body:error})}}let payload=null;try{payload=await resp.json()}catch{throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}: malformed API response, invalid JSON`,{url:url,method:"GET"},{requestId:resp.headers.get("x-request-id")??"",status:resp.status,body:await resp.text()})}if(!payload?.inferenceProviderMapping)throw new InferenceClientHubApiError(`We have not been able to find inference provider information for model ${modelId}.`,{url:url,method:"GET"},{requestId:resp.headers.get("x-request-id")??"",status:resp.status,body:await resp.text()});inferenceProviderMapping=function(modelId,inferenceProviderMapping){return inferenceProviderMapping?Array.isArray(inferenceProviderMapping)?inferenceProviderMapping:Object.entries(inferenceProviderMapping).map(([provider,mapping])=>({provider:provider,hfModelId:modelId,providerId:mapping.providerId,status:mapping.status,task:mapping.task,adapter:mapping.adapter,adapterWeightsPath:mapping.adapterWeightsPath})):[]}(modelId,payload.inferenceProviderMapping),inferenceProviderMappingCache.set(modelId,inferenceProviderMapping)}return inferenceProviderMapping}async function resolveProvider(provider,modelId,endpointUrl){const logger=getLogger();if(endpointUrl){if(provider)throw new InferenceClientInputError("Specifying both endpointUrl and provider is not supported.");return"hf-inference"}if(provider||(logger.log("Defaulting to 'auto' which will select the first provider available for the model, sorted by the user's order in https://hf.co/settings/inference-providers."),provider="auto"),"auto"===provider){if(!modelId)throw new InferenceClientInputError("Specifying a model is required when provider is 'auto'");const mappings=await fetchInferenceProviderMappingForModel(modelId);provider=mappings[0]?.provider,logger.log("Auto selected provider:",provider)}if(!provider)throw new InferenceClientInputError(`No Inference Provider available for model ${modelId}.`);return provider}function delay(ms){return new Promise(resolve=>{setTimeout(()=>resolve(),ms)})}function isUrl(modelOrUrl){return/^http(s?):/.test(modelOrUrl)||modelOrUrl.startsWith("/")}var FAL_AI_SUPPORTED_BLOB_TYPES=["audio/mpeg","audio/mp4","audio/wav","audio/x-wav"],FalAITask=class extends TaskProviderHelper{constructor(url){super("fal-ai",url||"https://fal.run")}preparePayload(params){return params.args}makeRoute(params){return`/${params.model}`}prepareHeaders(params,binary){const headers={Authorization:"provider-key"!==params.authMethod?`Bearer ${params.accessToken}`:`Key ${params.accessToken}`};return binary||(headers["Content-Type"]="application/json"),headers}},FalAiQueueTask=class extends FalAITask{async getResponseFromQueueApi(response,url,headers){if(!url||!headers)throw new InferenceClientInputError(`URL and headers are required for ${this.task} task`);if(!response.request_id)throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai ${this.task} API: no request ID found in the response`);let status=response.status;const parsedUrl=new URL(url),baseUrl=`${parsedUrl.protocol}//${parsedUrl.host}${"router.huggingface.co"===parsedUrl.host?"/fal-ai":""}`,modelId=new URL(response.response_url).pathname,queryParams=parsedUrl.search,statusUrl=`${baseUrl}${modelId}/status${queryParams}`,resultUrl=`${baseUrl}${modelId}${queryParams}`;for(;"COMPLETED"!==status;){await delay(500);const statusResponse=await fetch(statusUrl,{headers:headers});if(!statusResponse.ok)throw new InferenceClientProviderApiError("Failed to fetch response status from fal-ai API",{url:statusUrl,method:"GET"},{requestId:statusResponse.headers.get("x-request-id")??"",status:statusResponse.status,body:await statusResponse.text()});try{status=(await statusResponse.json()).status}catch(error){throw new InferenceClientProviderOutputError("Failed to parse status response from fal-ai API: received malformed response")}}const resultResponse=await fetch(resultUrl,{headers:headers});let result;try{result=await resultResponse.json()}catch(error){throw new InferenceClientProviderOutputError("Failed to parse result response from fal-ai API: received malformed response")}return result}};function buildLoraPath(modelId,adapterWeightsPath){return`${HF_HUB_URL}/${modelId}/resolve/main/${adapterWeightsPath}`}var ReplicateTask=class extends TaskProviderHelper{constructor(url){super("replicate",url||"https://api.replicate.com")}makeRoute(params){return params.model.includes(":")?"v1/predictions":`v1/models/${params.model}/predictions`}preparePayload(params){return{input:{...omit(params.args,["inputs","parameters"]),...params.args.parameters,prompt:params.args.inputs},version:params.model.includes(":")?params.model.split(":")[1]:void 0}}prepareHeaders(params,binary){const headers={Authorization:`Bearer ${params.accessToken}`,Prefer:"wait"};return binary||(headers["Content-Type"]="application/json"),headers}makeUrl(params){const baseUrl=this.makeBaseUrl(params);return params.model.includes(":")?`${baseUrl}/v1/predictions`:`${baseUrl}/v1/models/${params.model}/predictions`}};async function buildImagesField(inputs,hasImages){const base=base64FromBytes(new Uint8Array(inputs instanceof ArrayBuffer?inputs:await inputs.arrayBuffer()));return{base:base,images:Array.isArray(hasImages)&&hasImages.every(value=>"string"==typeof value)?hasImages:[base]}}var WavespeedAITask=class extends TaskProviderHelper{constructor(url){super("wavespeed",url||"https://api.wavespeed.ai")}makeRoute(params){return`/api/v3/${params.model}`}preparePayload(params){const payload={...omit(params.args,["inputs","parameters"]),...params.args.parameters?omit(params.args.parameters,["images"]):void 0,prompt:params.args.inputs};return"lora"===params.mapping?.adapter&&(payload.loras=[{path:params.mapping.hfModelId,scale:1}]),payload}async getResponse(response,url,headers){if(!url||!headers)throw new InferenceClientInputError("Headers are required for WaveSpeed AI API calls");const parsedUrl=new URL(url),resultPath=new URL(response.data.urls.get).pathname,resultUrl=`${`${parsedUrl.protocol}//${parsedUrl.host}${"router.huggingface.co"===parsedUrl.host?"/wavespeed":""}`}${resultPath}`;for(;;){const resultResponse=await fetch(resultUrl,{headers:headers});if(!resultResponse.ok)throw new InferenceClientProviderApiError("Failed to fetch response status from WaveSpeed AI API",{url:resultUrl,method:"GET"},{requestId:resultResponse.headers.get("x-request-id")??"",status:resultResponse.status,body:await resultResponse.text()});const taskResult=(await resultResponse.json()).data;switch(taskResult.status){case"completed":{if(!taskResult.outputs?.[0])throw new InferenceClientProviderOutputError("Received malformed response from WaveSpeed AI API: No output URL in completed response");const mediaResponse=await fetch(taskResult.outputs[0]);if(!mediaResponse.ok)throw new InferenceClientProviderApiError("Failed to fetch generation output from WaveSpeed AI API",{url:taskResult.outputs[0],method:"GET"},{requestId:mediaResponse.headers.get("x-request-id")??"",status:mediaResponse.status,body:await mediaResponse.text()});return await mediaResponse.blob()}case"failed":throw new InferenceClientProviderOutputError(taskResult.error||"Task failed");default:await delay(500);continue}}}},PROVIDERS={baseten:{conversational:new class extends BaseConversationalTask{constructor(){super("baseten","https://inference.baseten.co")}}},"black-forest-labs":{"text-to-image":new class extends TaskProviderHelper{constructor(){super("black-forest-labs","https://api.us1.bfl.ai")}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,prompt:params.args.inputs}}prepareHeaders(params,binary){const headers={Authorization:"provider-key"!==params.authMethod?`Bearer ${params.accessToken}`:`X-Key ${params.accessToken}`};return binary||(headers["Content-Type"]="application/json"),headers}makeRoute(params){if(!params)throw new InferenceClientInputError("Params are required");return`/v1/${params.model}`}async getResponse(response,url,headers,outputType){const logger=getLogger(),urlObj=new URL(response.polling_url);for(let step=0;step<5;step++){await delay(1e3),logger.debug(`Polling Black Forest Labs API for the result... ${step+1}/5`),urlObj.searchParams.set("attempt",step.toString(10));const resp=await fetch(urlObj,{headers:{"Content-Type":"application/json"}});if(!resp.ok)throw new InferenceClientProviderApiError("Failed to fetch result from black forest labs API",{url:urlObj.toString(),method:"GET",headers:{"Content-Type":"application/json"}},{requestId:resp.headers.get("x-request-id")??"",status:resp.status,body:await resp.text()});const payload=await resp.json();if("object"==typeof payload&&payload&&"status"in payload&&"string"==typeof payload.status&&"Ready"===payload.status&&"result"in payload&&"object"==typeof payload.result&&payload.result&&"sample"in payload.result&&"string"==typeof payload.result.sample){if("json"===outputType)return payload.result;if("url"===outputType)return payload.result.sample;const image=await fetch(payload.result.sample);return await image.blob()}}throw new InferenceClientProviderOutputError("Timed out while waiting for the result from black forest labs API - aborting after 5 attempts")}}},cerebras:{conversational:new class extends BaseConversationalTask{constructor(){super("cerebras","https://api.cerebras.ai")}}},clarifai:{conversational:new class extends BaseConversationalTask{constructor(){super("clarifai","https://api.clarifai.com")}makeRoute(){return"/v2/ext/openai/v1/chat/completions"}prepareHeaders(params,isBinary){const headers={Authorization:"provider-key"!==params.authMethod?`Bearer ${params.accessToken}`:`Key ${params.accessToken}`};return isBinary||(headers["Content-Type"]="application/json"),headers}}},cohere:{conversational:new class extends BaseConversationalTask{constructor(){super("cohere","https://api.cohere.com")}makeRoute(){return"/compatibility/v1/chat/completions"}}},"fal-ai":{"text-to-image":new class extends FalAITask{preparePayload(params){const payload={...omit(params.args,["inputs","parameters"]),...params.args.parameters,sync_mode:!0,prompt:params.args.inputs};return"lora"===params.mapping?.adapter&&params.mapping.adapterWeightsPath&&(payload.loras=[{path:buildLoraPath(params.mapping.hfModelId,params.mapping.adapterWeightsPath),scale:1}],"fal-ai/lora"===params.mapping.providerId&&(payload.model_name="stabilityai/stable-diffusion-xl-base-1.0")),payload}async getResponse(response,url,headers,outputType){if("object"==typeof response&&"images"in response&&Array.isArray(response.images)&&response.images.length>0&&"url"in response.images[0]&&"string"==typeof response.images[0].url){if("json"===outputType)return{...response};if("url"===outputType)return response.images[0].url;const urlResponse=await fetch(response.images[0].url);return await urlResponse.blob()}throw new InferenceClientProviderOutputError("Received malformed response from Fal.ai text-to-image API")}},"text-to-speech":new class extends FalAITask{preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,text:params.args.inputs}}async getResponse(response){const res=response;if("string"!=typeof res?.audio?.url)throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai Text-to-Speech API: expected { audio: { url: string } } format, got instead: ${JSON.stringify(response)}`);const urlResponse=await fetch(res.audio.url);if(!urlResponse.ok)throw new InferenceClientProviderApiError(`Failed to fetch audio from ${res.audio.url}: ${urlResponse.statusText}`,{url:res.audio.url,method:"GET",headers:{"Content-Type":"application/json"}},{requestId:urlResponse.headers.get("x-request-id")??"",status:urlResponse.status,body:await urlResponse.text()});try{return await urlResponse.blob()}catch(error){throw new InferenceClientProviderApiError(`Failed to fetch audio from ${res.audio.url}: ${error instanceof Error?error.message:String(error)}`,{url:res.audio.url,method:"GET",headers:{"Content-Type":"application/json"}},{requestId:urlResponse.headers.get("x-request-id")??"",status:urlResponse.status,body:await urlResponse.text()})}}},"text-to-video":new class extends FalAiQueueTask{constructor(){super("https://queue.fal.run"),__publicField(this,"task"),this.task="text-to-video"}makeRoute(params){return"provider-key"!==params.authMethod?`/${params.model}?_subdomain=queue`:`/${params.model}`}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,prompt:params.args.inputs}}async getResponse(response,url,headers){const result=await this.getResponseFromQueueApi(response,url,headers);if("object"==typeof result&&result&&"video"in result&&"object"==typeof result.video&&result.video&&"url"in result.video&&"string"==typeof result.video.url&&isUrl(result.video.url)){const urlResponse=await fetch(result.video.url);return await urlResponse.blob()}throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai text-to-video API: expected { video: { url: string } } result format, got instead: ${JSON.stringify(result)}`)}},"image-to-image":new class extends FalAiQueueTask{constructor(){super("https://queue.fal.run"),__publicField(this,"task"),this.task="image-to-image"}makeRoute(params){return"provider-key"!==params.authMethod?`/${params.model}?_subdomain=queue`:`/${params.model}`}preparePayload(params){const payload=params.args;return"lora"===params.mapping?.adapter&&params.mapping.adapterWeightsPath&&(payload.loras=[{path:buildLoraPath(params.mapping.hfModelId,params.mapping.adapterWeightsPath),scale:1}]),payload}async preparePayloadAsync(args){const imageDataUrl=`data:${args.inputs instanceof Blob?args.inputs.type:"image/png"};base64,${base64FromBytes(new Uint8Array(args.inputs instanceof ArrayBuffer?args.inputs:await args.inputs.arrayBuffer()))}`;return{...omit(args,["inputs","parameters"]),image_url:imageDataUrl,...args.parameters,...args,image_urls:[imageDataUrl]}}async getResponse(response,url,headers){const result=await this.getResponseFromQueueApi(response,url,headers);if("object"==typeof result&&result&&"images"in result&&Array.isArray(result.images)&&result.images.length>0&&"object"==typeof result.images[0]&&result.images[0]&&"url"in result.images[0]&&"string"==typeof result.images[0].url&&isUrl(result.images[0].url)){const urlResponse=await fetch(result.images[0].url);return await urlResponse.blob()}throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai image-to-image API: expected { images: Array<{ url: string }> } result format, got instead: ${JSON.stringify(result)}`)}},"automatic-speech-recognition":new class extends FalAITask{prepareHeaders(params,binary){const headers=super.prepareHeaders(params,binary);return headers["Content-Type"]="application/json",headers}async getResponse(response){const res=response;if("string"!=typeof res?.text)throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai Automatic Speech Recognition API: expected { text: string } format, got instead: ${JSON.stringify(response)}`);return{text:res.text}}async preparePayloadAsync(args){const blob="data"in args&&args.data instanceof Blob?args.data:"inputs"in args?args.inputs:void 0,contentType=blob?.type;if(!contentType)throw new InferenceClientInputError("Unable to determine the input's content-type. Make sure your are passing a Blob when using provider fal-ai.");if(!FAL_AI_SUPPORTED_BLOB_TYPES.includes(contentType))throw new InferenceClientInputError(`Provider fal-ai does not support blob type ${contentType} - supported content types are: ${FAL_AI_SUPPORTED_BLOB_TYPES.join(", ")}`);const base64audio=base64FromBytes(new Uint8Array(await blob.arrayBuffer()));return{...omit(args,"data"in args?"data":"inputs"),audio_url:`data:${contentType};base64,${base64audio}`}}},"image-segmentation":new class extends FalAiQueueTask{constructor(){super("https://queue.fal.run"),__publicField(this,"task"),this.task="image-segmentation"}makeRoute(params){return"provider-key"!==params.authMethod?`/${params.model}?_subdomain=queue`:`/${params.model}`}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,sync_mode:!0}}async preparePayloadAsync(args){const blob="data"in args&&args.data instanceof Blob?args.data:"inputs"in args?args.inputs:void 0,mimeType=blob instanceof Blob?blob.type:"image/png",base64Image=base64FromBytes(new Uint8Array(blob instanceof ArrayBuffer?blob:await blob.arrayBuffer()));return{...omit(args,["inputs","parameters","data"]),...args.parameters,...args,image_url:`data:${mimeType};base64,${base64Image}`,sync_mode:!0}}async getResponse(response,url,headers){const result=await this.getResponseFromQueueApi(response,url,headers);if("object"==typeof result&&null!==result&&"image"in result&&"object"==typeof result.image&&null!==result.image&&"url"in result.image&&"string"==typeof result.image.url){const maskResponse=await fetch(result.image.url);if(!maskResponse.ok)throw new InferenceClientProviderApiError(`Failed to fetch segmentation mask from ${result.image.url}`,{url:result.image.url,method:"GET"},{requestId:maskResponse.headers.get("x-request-id")??"",status:maskResponse.status,body:await maskResponse.text()});const maskBlob=await maskResponse.blob(),maskArrayBuffer=await maskBlob.arrayBuffer();return[{label:"mask",score:1,mask:base64FromBytes(new Uint8Array(maskArrayBuffer))}]}throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai image-segmentation API: expected { image: { url: string } } format, got instead: ${JSON.stringify(response)}`)}},"image-to-video":new class extends FalAiQueueTask{constructor(){super("https://queue.fal.run"),__publicField(this,"task"),this.task="image-to-video"}makeRoute(params){return"provider-key"!==params.authMethod?`/${params.model}?_subdomain=queue`:`/${params.model}`}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,image_url:params.args.image_url}}async preparePayloadAsync(args){const mimeType=args.inputs instanceof Blob?args.inputs.type:"image/png";return{...omit(args,["inputs","parameters"]),image_url:`data:${mimeType};base64,${base64FromBytes(new Uint8Array(args.inputs instanceof ArrayBuffer?args.inputs:await args.inputs.arrayBuffer()))}`,...args.parameters,...args}}async getResponse(response,url,headers){const result=await this.getResponseFromQueueApi(response,url,headers);if("object"==typeof result&&null!==result&&"video"in result&&"object"==typeof result.video&&null!==result.video&&"url"in result.video&&"string"==typeof result.video.url&&"url"in result.video&&isUrl(result.video.url)){const urlResponse=await fetch(result.video.url);return await urlResponse.blob()}throw new InferenceClientProviderOutputError(`Received malformed response from Fal.ai image‚Äëto‚Äëvideo API: expected { video: { url: string } }, got: ${JSON.stringify(result)}`)}}},"featherless-ai":{conversational:new class extends BaseConversationalTask{constructor(){super("featherless-ai","https://api.featherless.ai")}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("featherless-ai","https://api.featherless.ai")}preparePayload(params){return{model:params.model,...omit(params.args,["inputs","parameters"]),...params.args.parameters?{max_tokens:params.args.parameters.max_new_tokens,...omit(params.args.parameters,"max_new_tokens")}:void 0,prompt:params.args.inputs}}async getResponse(response){if("object"==typeof response&&"choices"in response&&Array.isArray(response?.choices)&&"string"==typeof response?.model){return{generated_text:response.choices[0].text}}throw new InferenceClientProviderOutputError("Received malformed response from Featherless AI text generation API")}}},"hf-inference":{"text-to-image":new class extends HFInferenceTask{async getResponse(response,url,headers,outputType){if(!response)throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference text-to-image API: response is undefined");if("object"==typeof response){if("json"===outputType)return{...response};if("data"in response&&Array.isArray(response.data)&&response.data[0].b64_json){const base64Data=response.data[0].b64_json;if("url"===outputType)return`data:image/jpeg;base64,${base64Data}`;const base64Response=await fetch(`data:image/jpeg;base64,${base64Data}`);return await base64Response.blob()}if("output"in response&&Array.isArray(response.output)){if("url"===outputType)return response.output[0];const urlResponse=await fetch(response.output[0]);return await urlResponse.blob()}}if(response instanceof Blob){if("url"===outputType||"json"===outputType){const b64=await response.arrayBuffer().then(buf=>Buffer.from(buf).toString("base64"));return"url"===outputType?`data:image/jpeg;base64,${b64}`:{output:`data:image/jpeg;base64,${b64}`}}return response}throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference text-to-image API: expected a Blob")}},conversational:new class extends HFInferenceTask{makeUrl(params){let url;return url=params.model.startsWith("http://")||params.model.startsWith("https://")?params.model.trim():`${this.makeBaseUrl(params)}/models/${params.model}`,url=url.replace(/\/+$/,""),url.endsWith("/v1")?url+="/chat/completions":url.endsWith("/chat/completions")||(url+="/v1/chat/completions"),url}preparePayload(params){return{...params.args,model:params.model}}async getResponse(response){return response}},"text-generation":new class extends HFInferenceTask{async getResponse(response){const res=toArray(response);if(Array.isArray(res)&&res.every(x=>"generated_text"in x&&"string"==typeof x?.generated_text))return res?.[0];throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference text generation API: expected Array<{generated_text: string}>")}},"text-classification":new class extends HFInferenceTask{async getResponse(response){const output=response?.[0];if(Array.isArray(output)&&output.every(x=>"string"==typeof x?.label&&"number"==typeof x.score))return output;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference text-classification API: expected Array<{label: string, score: number}>")}},"question-answering":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)?response.every(elem=>"object"==typeof elem&&!!elem&&"string"==typeof elem.answer&&"number"==typeof elem.end&&"number"==typeof elem.score&&"number"==typeof elem.start):"object"==typeof response&&response&&"string"==typeof response.answer&&"number"==typeof response.end&&"number"==typeof response.score&&"number"==typeof response.start)return Array.isArray(response)?response[0]:response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference question-answering API: expected Array<{answer: string, end: number, score: number, start: number}>")}},"audio-classification":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"object"==typeof x&&null!==x&&"string"==typeof x.label&&"number"==typeof x.score))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference audio-classification API: expected Array<{label: string, score: number}> but received different format")}},"automatic-speech-recognition":new class extends HFInferenceTask{async getResponse(response){return response}async preparePayloadAsync(args){return"data"in args?args:{...omit(args,"inputs"),data:args.inputs}}},"fill-mask":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"number"==typeof x.score&&"string"==typeof x.sequence&&"number"==typeof x.token&&"string"==typeof x.token_str))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference fill-mask API: expected Array<{score: number, sequence: string, token: number, token_str: string}>")}},"feature-extraction":new class extends HFInferenceTask{async getResponse(response){const isNumArrayRec=(arr,maxDepth,curDepth=0)=>!(curDepth>maxDepth)&&(arr.every(x=>Array.isArray(x))?arr.every(x=>isNumArrayRec(x,maxDepth,curDepth+1)):arr.every(x=>"number"==typeof x));if(Array.isArray(response)&&isNumArrayRec(response,3,0))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference feature-extraction API: expected Array<number[][][] | number[][] | number[] | number>")}},"image-classification":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"string"==typeof x.label&&"number"==typeof x.score))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference image-classification API: expected Array<{label: string, score: number}>")}},"image-segmentation":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"string"==typeof x.label&&"string"==typeof x.mask&&(void 0===x.score||"number"==typeof x.score)))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference image-segmentation API: expected Array<{label: string, mask: string, score: number}>")}async preparePayloadAsync(args){return{...args,inputs:base64FromBytes(new Uint8Array(args.inputs instanceof ArrayBuffer?args.inputs:await args.inputs.arrayBuffer()))}}},"document-question-answering":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(elem=>!("object"!=typeof elem||!elem||"string"!=typeof elem?.answer||"number"!=typeof elem.end&&void 0!==elem.end||"number"!=typeof elem.score&&void 0!==elem.score||"number"!=typeof elem.start&&void 0!==elem.start)))return response[0];throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference document-question-answering API: expected Array<{answer: string, end: number, score: number, start: number}>")}},"image-to-text":new class extends HFInferenceTask{async getResponse(response){if("string"!=typeof response?.generated_text)throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference image-to-text API: expected {generated_text: string}");return response}},"object-detection":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"string"==typeof x.label&&"number"==typeof x.score&&"number"==typeof x.box.xmin&&"number"==typeof x.box.ymin&&"number"==typeof x.box.xmax&&"number"==typeof x.box.ymax))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference object-detection API: expected Array<{label: string, score: number, box: {xmin: number, ymin: number, xmax: number, ymax: number}}>")}},"audio-to-audio":new class extends HFInferenceTask{async getResponse(response){if(!Array.isArray(response))throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference audio-to-audio API: expected Array");if(!response.every(elem=>"object"==typeof elem&&elem&&"label"in elem&&"string"==typeof elem.label&&"content-type"in elem&&"string"==typeof elem["content-type"]&&"blob"in elem&&"string"==typeof elem.blob))throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference audio-to-audio API: expected Array<{label: string, audio: Blob}>");return response}},"zero-shot-image-classification":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"string"==typeof x.label&&"number"==typeof x.score))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference zero-shot-image-classification API: expected Array<{label: string, score: number}>")}},"zero-shot-classification":new class _HFInferenceZeroShotClassificationTask extends HFInferenceTask{async getResponse(response){if("object"==typeof response&&null!==response&&"labels"in response&&"scores"in response&&Array.isArray(response.labels)&&Array.isArray(response.scores)&&response.labels.length===response.scores.length&&response.labels.every(label=>"string"==typeof label)&&response.scores.every(score=>"number"==typeof score)){const scores=response.scores;return response.labels.map((label,index)=>({label:label,score:scores[index]}))}if(Array.isArray(response)&&response.every(_HFInferenceZeroShotClassificationTask.validateOutputElement))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference zero-shot-classification API: expected Array<{label: string, score: number}>")}static validateOutputElement(elem){return"object"==typeof elem&&!!elem&&"label"in elem&&"score"in elem&&"string"==typeof elem.label&&"number"==typeof elem.score}},"image-to-image":new class extends HFInferenceTask{async preparePayloadAsync(args){return args.parameters?{...args,inputs:base64FromBytes(new Uint8Array(args.inputs instanceof ArrayBuffer?args.inputs:await args.inputs.arrayBuffer()))}:{...args,model:args.model,data:args.inputs}}async getResponse(response){if(response instanceof Blob)return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference image-to-image API: expected Blob")}},"sentence-similarity":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"number"==typeof x))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference sentence-similarity API: expected Array<number>")}},"table-question-answering":new class _HFInferenceTableQuestionAnsweringTask extends HFInferenceTask{static validate(elem){return"object"==typeof elem&&!!elem&&"aggregator"in elem&&"string"==typeof elem.aggregator&&"answer"in elem&&"string"==typeof elem.answer&&"cells"in elem&&Array.isArray(elem.cells)&&elem.cells.every(x=>"string"==typeof x)&&"coordinates"in elem&&Array.isArray(elem.coordinates)&&elem.coordinates.every(coord=>Array.isArray(coord)&&coord.every(x=>"number"==typeof x))}async getResponse(response){if(Array.isArray(response)&&Array.isArray(response)?response.every(elem=>_HFInferenceTableQuestionAnsweringTask.validate(elem)):_HFInferenceTableQuestionAnsweringTask.validate(response))return Array.isArray(response)?response[0]:response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference table-question-answering API: expected {aggregator: string, answer: string, cells: string[], coordinates: number[][]}")}},"tabular-classification":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"number"==typeof x))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference tabular-classification API: expected Array<number>")}},"text-to-speech":new class extends HFInferenceTask{async getResponse(response){return response}},"token-classification":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"number"==typeof x.end&&"string"==typeof x.entity_group&&"number"==typeof x.score&&"number"==typeof x.start&&"string"==typeof x.word))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference token-classification API: expected Array<{end: number, entity_group: string, score: number, start: number, word: string}>")}},translation:new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"string"==typeof x?.translation_text))return 1===response?.length?response?.[0]:response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference translation API: expected Array<{translation_text: string}>")}},summarization:new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"string"==typeof x?.summary_text))return response?.[0];throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference summarization API: expected Array<{summary_text: string}>")}},"visual-question-answering":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(elem=>"object"==typeof elem&&!!elem&&"string"==typeof elem?.answer&&"number"==typeof elem.score))return response[0];throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference visual-question-answering API: expected Array<{answer: string, score: number}>")}},"tabular-regression":new class extends HFInferenceTask{async getResponse(response){if(Array.isArray(response)&&response.every(x=>"number"==typeof x))return response;throw new InferenceClientProviderOutputError("Received malformed response from HF-Inference tabular-regression API: expected Array<number>")}},"text-to-audio":new class extends HFInferenceTask{async getResponse(response){return response}}},"fireworks-ai":{conversational:new class extends BaseConversationalTask{constructor(){super("fireworks-ai","https://api.fireworks.ai")}makeRoute(){return"/inference/v1/chat/completions"}}},groq:{conversational:new class extends BaseConversationalTask{constructor(){super("groq","https://api.groq.com")}makeRoute(){return"/openai/v1/chat/completions"}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("groq","https://api.groq.com")}makeRoute(){return"/openai/v1/chat/completions"}}},hyperbolic:{"text-to-image":new class extends TaskProviderHelper{constructor(){super("hyperbolic","https://api.hyperbolic.xyz")}makeRoute(params){return"/v1/images/generations"}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,prompt:params.args.inputs,model_name:params.model}}async getResponse(response,url,headers,outputType){if("object"==typeof response&&"images"in response&&Array.isArray(response.images)&&response.images[0]&&"string"==typeof response.images[0].image)return"json"===outputType?{...response}:"url"===outputType?`data:image/jpeg;base64,${response.images[0].image}`:fetch(`data:image/jpeg;base64,${response.images[0].image}`).then(res=>res.blob());throw new InferenceClientProviderOutputError("Received malformed response from Hyperbolic text-to-image API")}},conversational:new class extends BaseConversationalTask{constructor(){super("hyperbolic","https://api.hyperbolic.xyz")}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("hyperbolic","https://api.hyperbolic.xyz")}makeRoute(){return"v1/chat/completions"}preparePayload(params){return{messages:[{content:params.args.inputs,role:"user"}],...params.args.parameters?{max_tokens:params.args.parameters.max_new_tokens,...omit(params.args.parameters,"max_new_tokens")}:void 0,...omit(params.args,["inputs","parameters"]),model:params.model}}async getResponse(response){if("object"==typeof response&&"choices"in response&&Array.isArray(response?.choices)&&"string"==typeof response?.model){return{generated_text:response.choices[0].message.content}}throw new InferenceClientProviderOutputError("Received malformed response from Hyperbolic text generation API")}}},nebius:{"text-to-image":new class extends TaskProviderHelper{constructor(){super("nebius","https://api.studio.nebius.ai")}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,response_format:"b64_json",prompt:params.args.inputs,model:params.model}}makeRoute(){return"v1/images/generations"}async getResponse(response,url,headers,outputType){if("object"==typeof response&&"data"in response&&Array.isArray(response.data)&&response.data.length>0&&"b64_json"in response.data[0]&&"string"==typeof response.data[0].b64_json){if("json"===outputType)return{...response};const base64Data=response.data[0].b64_json;return"url"===outputType?`data:image/jpeg;base64,${base64Data}`:fetch(`data:image/jpeg;base64,${base64Data}`).then(res=>res.blob())}throw new InferenceClientProviderOutputError("Received malformed response from Nebius text-to-image API")}},conversational:new class extends BaseConversationalTask{constructor(){super("nebius","https://api.studio.nebius.ai")}preparePayload(params){const payload=super.preparePayload(params),responseFormat=params.args.response_format;return"json_schema"===responseFormat?.type&&responseFormat.json_schema?.schema&&(payload.guided_json=responseFormat.json_schema.schema),payload}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("nebius","https://api.studio.nebius.ai")}preparePayload(params){return{...params.args,model:params.model,prompt:params.args.inputs}}async getResponse(response){if("object"==typeof response&&"choices"in response&&Array.isArray(response?.choices)&&response.choices.length>0&&"string"==typeof response.choices[0]?.text)return{generated_text:response.choices[0].text};throw new InferenceClientProviderOutputError("Received malformed response from Nebius text generation API")}},"feature-extraction":new class extends TaskProviderHelper{constructor(){super("nebius","https://api.studio.nebius.ai")}preparePayload(params){return{input:params.args.inputs,model:params.model}}makeRoute(){return"v1/embeddings"}async getResponse(response){return response.data.map(item=>item.embedding)}}},novita:{conversational:new class extends BaseConversationalTask{constructor(){super("novita","https://api.novita.ai")}makeRoute(){return"/v3/openai/chat/completions"}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("novita","https://api.novita.ai")}makeRoute(){return"/v3/openai/chat/completions"}},"text-to-video":new class extends TaskProviderHelper{constructor(){super("novita","https://api.novita.ai")}makeRoute(params){return`/v3/async/${params.model}`}preparePayload(params){const{num_inference_steps:num_inference_steps,...restParameters}=params.args.parameters??{};return{...omit(params.args,["inputs","parameters"]),...restParameters,steps:num_inference_steps,prompt:params.args.inputs}}async getResponse(response,url,headers){if(!url||!headers)throw new InferenceClientInputError("URL and headers are required for text-to-video task");const taskId=response.task_id;if(!taskId)throw new InferenceClientProviderOutputError("Received malformed response from Novita text-to-video API: no task ID found in the response");const parsedUrl=new URL(url),resultUrl=`${`${parsedUrl.protocol}//${parsedUrl.host}${"router.huggingface.co"===parsedUrl.host?"/novita":""}`}/v3/async/task-result?task_id=${taskId}`;let taskResult,status="";for(;"TASK_STATUS_SUCCEED"!==status&&"TASK_STATUS_FAILED"!==status;){await delay(500);const resultResponse=await fetch(resultUrl,{headers:headers});if(!resultResponse.ok)throw new InferenceClientProviderApiError("Failed to fetch task result",{url:resultUrl,method:"GET",headers:headers},{requestId:resultResponse.headers.get("x-request-id")??"",status:resultResponse.status,body:await resultResponse.text()});try{if(taskResult=await resultResponse.json(),!(taskResult&&"object"==typeof taskResult&&"task"in taskResult&&taskResult.task&&"object"==typeof taskResult.task&&"status"in taskResult.task&&"string"==typeof taskResult.task.status))throw new InferenceClientProviderOutputError("Received malformed response from Novita text-to-video API: failed to get task status");status=taskResult.task.status}catch(error){throw new InferenceClientProviderOutputError("Received malformed response from Novita text-to-video API: failed to parse task result")}}if("TASK_STATUS_FAILED"===status)throw new InferenceClientProviderOutputError("Novita text-to-video task failed");if("object"==typeof taskResult&&taskResult&&"videos"in taskResult&&"object"==typeof taskResult.videos&&taskResult.videos&&Array.isArray(taskResult.videos)&&taskResult.videos.length>0&&"video_url"in taskResult.videos[0]&&"string"==typeof taskResult.videos[0].video_url&&isUrl(taskResult.videos[0].video_url)){const urlResponse=await fetch(taskResult.videos[0].video_url);return await urlResponse.blob()}throw new InferenceClientProviderOutputError(`Received malformed response from Novita text-to-video API: expected { videos: [{ video_url: string }] } format, got instead: ${JSON.stringify(taskResult)}`)}}},nscale:{"text-to-image":new class extends TaskProviderHelper{constructor(){super("nscale","https://inference.api.nscale.com")}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,response_format:"b64_json",prompt:params.args.inputs,model:params.model}}makeRoute(){return"v1/images/generations"}async getResponse(response,url,headers,outputType){if("object"==typeof response&&"data"in response&&Array.isArray(response.data)&&response.data.length>0&&"b64_json"in response.data[0]&&"string"==typeof response.data[0].b64_json){if("json"===outputType)return{...response};const base64Data=response.data[0].b64_json;return"url"===outputType?`data:image/jpeg;base64,${base64Data}`:fetch(`data:image/jpeg;base64,${base64Data}`).then(res=>res.blob())}throw new InferenceClientProviderOutputError("Received malformed response from Nscale text-to-image API")}},conversational:new class extends BaseConversationalTask{constructor(){super("nscale","https://inference.api.nscale.com")}}},openai:{conversational:new class extends BaseConversationalTask{constructor(){super("openai","https://api.openai.com",!0)}}},ovhcloud:{conversational:new class extends BaseConversationalTask{constructor(){super("ovhcloud","https://oai.endpoints.kepler.ai.cloud.ovh.net")}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("ovhcloud","https://oai.endpoints.kepler.ai.cloud.ovh.net")}preparePayload(params){return{model:params.model,...omit(params.args,["inputs","parameters"]),...params.args.parameters?{max_tokens:params.args.parameters.max_new_tokens,...omit(params.args.parameters,"max_new_tokens")}:void 0,prompt:params.args.inputs}}async getResponse(response){if("object"==typeof response&&"choices"in response&&Array.isArray(response?.choices)&&"string"==typeof response?.model){return{generated_text:response.choices[0].text}}throw new InferenceClientProviderOutputError("Received malformed response from OVHcloud text generation API")}}},publicai:{conversational:new class extends BaseConversationalTask{constructor(){super("publicai","https://api.publicai.co")}}},replicate:{"text-to-image":new class extends ReplicateTask{preparePayload(params){return{input:{...omit(params.args,["inputs","parameters"]),...params.args.parameters,prompt:params.args.inputs,lora_weights:"lora"===params.mapping?.adapter&&params.mapping.adapterWeightsPath?`https://huggingface.co/${params.mapping.hfModelId}`:void 0},version:params.model.includes(":")?params.model.split(":")[1]:void 0}}async getResponse(res,url,headers,outputType){if("object"==typeof res&&"output"in res&&"string"==typeof res.output&&isUrl(res.output)){if("json"===outputType)return{...res};if("url"===outputType)return res.output;const urlResponse=await fetch(res.output);return await urlResponse.blob()}if("object"==typeof res&&"output"in res&&Array.isArray(res.output)&&res.output.length>0&&"string"==typeof res.output[0]){if("json"===outputType)return{...res};if("url"===outputType)return res.output[0];const urlResponse=await fetch(res.output[0]);return await urlResponse.blob()}throw new InferenceClientProviderOutputError("Received malformed response from Replicate text-to-image API")}},"text-to-speech":new class extends ReplicateTask{preparePayload(params){const payload=super.preparePayload(params),input=payload.input;if("object"==typeof input&&null!==input&&"prompt"in input){const inputObj=input;inputObj.text=inputObj.prompt,delete inputObj.prompt}return payload}async getResponse(response){if(response instanceof Blob)return response;if(response&&"object"==typeof response&&"output"in response){if("string"==typeof response.output){const urlResponse=await fetch(response.output);return await urlResponse.blob()}if(Array.isArray(response.output)){const urlResponse=await fetch(response.output[0]);return await urlResponse.blob()}}throw new InferenceClientProviderOutputError("Received malformed response from Replicate text-to-speech API")}},"text-to-video":new class extends ReplicateTask{async getResponse(response){if("object"==typeof response&&response&&"output"in response&&"string"==typeof response.output&&isUrl(response.output)){const urlResponse=await fetch(response.output);return await urlResponse.blob()}throw new InferenceClientProviderOutputError("Received malformed response from Replicate text-to-video API")}},"image-to-image":new class extends ReplicateTask{preparePayload(params){return{input:{...omit(params.args,["inputs","parameters"]),...params.args.parameters,input_image:params.args.inputs,lora_weights:"lora"===params.mapping?.adapter&&params.mapping.adapterWeightsPath?`https://huggingface.co/${params.mapping.hfModelId}`:void 0},version:params.model.includes(":")?params.model.split(":")[1]:void 0}}async preparePayloadAsync(args){const{inputs:inputs,...restArgs}=args,base64=base64FromBytes(new Uint8Array(await inputs.arrayBuffer()));return{...restArgs,inputs:`data:${inputs.type||"image/jpeg"};base64,${base64}`}}async getResponse(response){if("object"==typeof response&&response&&"output"in response&&Array.isArray(response.output)&&response.output.length>0&&"string"==typeof response.output[0]){const urlResponse=await fetch(response.output[0]);return await urlResponse.blob()}if("object"==typeof response&&response&&"output"in response&&"string"==typeof response.output&&isUrl(response.output)){const urlResponse=await fetch(response.output);return await urlResponse.blob()}throw new InferenceClientProviderOutputError("Received malformed response from Replicate image-to-image API")}},"automatic-speech-recognition":new class extends ReplicateTask{preparePayload(params){return{input:{...omit(params.args,["inputs","parameters"]),...params.args.parameters,audio:params.args.inputs},version:params.model.includes(":")?params.model.split(":")[1]:void 0}}async preparePayloadAsync(args){const blob="data"in args&&args.data instanceof Blob?args.data:"inputs"in args?args.inputs:void 0;if(!(blob&&blob instanceof Blob))throw new Error("Audio input must be a Blob");const base64=base64FromBytes(new Uint8Array(await blob.arrayBuffer())),audioInput=`data:${blob.type||"audio/wav"};base64,${base64}`;return{...omit(args,"data"in args?"data":"inputs"),inputs:audioInput}}async getResponse(response){if("string"==typeof response?.output)return{text:response.output};if(Array.isArray(response?.output)&&"string"==typeof response.output[0])return{text:response.output[0]};const out=response?.output;if(out&&"object"==typeof out){if("string"==typeof out.transcription)return{text:out.transcription};if("string"==typeof out.translation)return{text:out.translation};if("string"==typeof out.txt_file){const r=await fetch(out.txt_file);return{text:await r.text()}}}throw new InferenceClientProviderOutputError("Received malformed response from Replicate automatic-speech-recognition API")}}},sambanova:{conversational:new class extends BaseConversationalTask{constructor(){super("sambanova","https://api.sambanova.ai")}preparePayload(params){const responseFormat=params.args.response_format;"json_schema"===responseFormat?.type&&responseFormat.json_schema&&(responseFormat.json_schema.strict??1)&&(responseFormat.json_schema.strict=!1);return super.preparePayload(params)}},"feature-extraction":new class extends TaskProviderHelper{constructor(){super("sambanova","https://api.sambanova.ai")}makeRoute(){return"/v1/embeddings"}async getResponse(response){if("object"==typeof response&&"data"in response&&Array.isArray(response.data))return response.data.map(item=>item.embedding);throw new InferenceClientProviderOutputError("Received malformed response from Sambanova feature-extraction (embeddings) API")}preparePayload(params){return{model:params.model,input:params.args.inputs,...params.args}}}},scaleway:{conversational:new class extends BaseConversationalTask{constructor(){super("scaleway","https://api.scaleway.ai")}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("scaleway","https://api.scaleway.ai")}preparePayload(params){return{model:params.model,...params.args,prompt:params.args.inputs}}async getResponse(response){if("object"==typeof response&&null!==response&&"choices"in response&&Array.isArray(response.choices)&&response.choices.length>0){const completion=response.choices[0];if("object"==typeof completion&&completion&&"text"in completion&&completion.text&&"string"==typeof completion.text)return{generated_text:completion.text}}throw new InferenceClientProviderOutputError("Received malformed response from Scaleway text generation API")}},"feature-extraction":new class extends TaskProviderHelper{constructor(){super("scaleway","https://api.scaleway.ai")}preparePayload(params){return{input:params.args.inputs,model:params.model}}makeRoute(){return"v1/embeddings"}async getResponse(response){return response.data.map(item=>item.embedding)}}},together:{"text-to-image":new class extends TaskProviderHelper{constructor(){super("together","https://api.together.xyz")}makeRoute(){return"v1/images/generations"}preparePayload(params){return{...omit(params.args,["inputs","parameters"]),...params.args.parameters,prompt:params.args.inputs,response_format:"base64",model:params.model}}async getResponse(response,url,headers,outputType){if("object"==typeof response&&"data"in response&&Array.isArray(response.data)&&response.data.length>0&&"b64_json"in response.data[0]&&"string"==typeof response.data[0].b64_json){if("json"===outputType)return{...response};const base64Data=response.data[0].b64_json;return"url"===outputType?`data:image/jpeg;base64,${base64Data}`:fetch(`data:image/jpeg;base64,${base64Data}`).then(res=>res.blob())}throw new InferenceClientProviderOutputError("Received malformed response from Together text-to-image API")}},conversational:new class extends BaseConversationalTask{constructor(){super("together","https://api.together.xyz")}preparePayload(params){const payload=super.preparePayload(params),response_format=payload.response_format;return"json_schema"===response_format?.type&&response_format?.json_schema?.schema&&(payload.response_format={type:"json_schema",schema:response_format.json_schema.schema}),payload}},"text-generation":new class extends BaseTextGenerationTask{constructor(){super("together","https://api.together.xyz")}preparePayload(params){return{model:params.model,...params.args,prompt:params.args.inputs}}async getResponse(response){if("object"==typeof response&&"choices"in response&&Array.isArray(response?.choices)&&"string"==typeof response?.model){return{generated_text:response.choices[0].text}}throw new InferenceClientProviderOutputError("Received malformed response from Together text generation API")}}},wavespeed:{"text-to-image":new class extends WavespeedAITask{constructor(){super("https://api.wavespeed.ai")}},"text-to-video":new class extends WavespeedAITask{constructor(){super("https://api.wavespeed.ai")}},"image-to-image":new class extends WavespeedAITask{constructor(){super("https://api.wavespeed.ai")}async preparePayloadAsync(args){const hasImages=args.images??args.parameters?.images,{base:base,images:images}=await buildImagesField(args.inputs,hasImages);return{...args,inputs:args.parameters?.prompt,image:base,images:images}}},"image-to-video":new class extends WavespeedAITask{constructor(){super("https://api.wavespeed.ai")}async preparePayloadAsync(args){const hasImages=args.images??args.parameters?.images,{base:base,images:images}=await buildImagesField(args.inputs,hasImages);return{...args,inputs:args.parameters?.prompt,image:base,images:images}}}},"zai-org":{conversational:new class extends BaseConversationalTask{constructor(){super("zai-org","https://api.z.ai")}prepareHeaders(params,binary){const headers=super.prepareHeaders(params,binary);return headers["x-source-channel"]="hugging_face",headers["accept-language"]="en-US,en",headers}makeRoute(){return"/api/paas/v4/chat/completions"}}}};function getProviderHelper(provider,task){if("hf-inference"===provider&&!task||"auto"===provider)return new HFInferenceTask;if(!task)throw new InferenceClientInputError("you need to provide a task name when using an external provider, e.g. 'text-to-image'");if(!(provider in PROVIDERS))throw new InferenceClientInputError(`Provider '${provider}' not supported. Available providers: ${Object.keys(PROVIDERS)}`);const providerTasks=PROVIDERS[provider];if(!providerTasks||!(task in providerTasks))throw new InferenceClientInputError(`Task '${task}' not supported for provider '${provider}'. Available tasks: ${Object.keys(providerTasks??{})}`);return providerTasks[task]}var PACKAGE_VERSION="4.13.4",PACKAGE_NAME="@huggingface/inference",tasks=null;async function makeRequestOptions(args,providerHelper,options){const{model:maybeModel}=args,provider=providerHelper.provider,{task:task}=options??{};if(args.endpointUrl&&"hf-inference"!==provider)throw new InferenceClientInputError("Cannot use endpointUrl with a third-party provider.");if(maybeModel&&isUrl(maybeModel))throw new InferenceClientInputError("Model URLs are no longer supported. Use endpointUrl instead.");if(args.endpointUrl)return makeRequestOptionsFromResolvedModel(maybeModel??args.endpointUrl,providerHelper,args,void 0,options);if(!maybeModel&&!task)throw new InferenceClientInputError("No model provided, and no task has been specified.");const hfModel=maybeModel??await async function(task){tasks||(tasks=await async function(){const url=`${HF_HUB_URL}/api/tasks`,res=await fetch(url);if(!res.ok)throw new InferenceClientHubApiError("Failed to load tasks definitions from Hugging Face Hub.",{url:url,method:"GET"},{requestId:res.headers.get("x-request-id")??"",status:res.status,body:await res.text()});return await res.json()}());const taskInfo=tasks[task];if((taskInfo?.models.length??0)<=0)throw new InferenceClientInputError(`No default model defined for task ${task}, please define the model explicitly.`);return taskInfo.models[0].id}(task);if(providerHelper.clientSideRoutingOnly&&!maybeModel)throw new InferenceClientInputError(`Provider ${provider} requires a model ID to be passed directly.`);const inferenceProviderMapping=providerHelper.clientSideRoutingOnly?{provider:provider,providerId:removeProviderPrefix(maybeModel,provider),hfModelId:maybeModel,status:"live",task:task}:await async function(params,options){const logger=getLogger();if("auto"===params.provider&&"conversational"===params.task)return{hfModelId:params.modelId,provider:"auto",providerId:params.modelId,status:"live",task:"conversational"};if(HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId])return HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId];const providerMapping=(await fetchInferenceProviderMappingForModel(params.modelId,params.accessToken,options)).find(mapping=>mapping.provider===params.provider);if(providerMapping){const equivalentTasks="hf-inference"===params.provider&&typedInclude(EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS,params.task)?EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS:[params.task];if(!typedInclude(equivalentTasks,providerMapping.task))throw new InferenceClientInputError(`Model ${params.modelId} is not supported for task ${params.task} and provider ${params.provider}. Supported task: ${providerMapping.task}.`);return"staging"===providerMapping.status&&logger.warn(`Model ${params.modelId} is in staging mode for provider ${params.provider}. Meant for test purposes only.`),providerMapping}return null}({modelId:hfModel,task:task,provider:provider,accessToken:args.accessToken},{fetch:options?.fetch});if(!inferenceProviderMapping)throw new InferenceClientInputError(`We have not been able to find inference provider information for model ${hfModel}.`);return makeRequestOptionsFromResolvedModel(inferenceProviderMapping.providerId,providerHelper,args,inferenceProviderMapping,options)}function makeRequestOptionsFromResolvedModel(resolvedModel,providerHelper,args,mapping,options){const{accessToken:accessToken,endpointUrl:endpointUrl,provider:maybeProvider,model:model,...remainingArgs}=args,provider=providerHelper.provider,{includeCredentials:includeCredentials,task:task,signal:signal,billTo:billTo}=options??{},authMethod=(()=>{if(providerHelper.clientSideRoutingOnly&&accessToken&&accessToken.startsWith("hf_"))throw new InferenceClientInputError(`Provider ${provider} is closed-source and does not support HF tokens.`);return accessToken?accessToken.startsWith("hf_")?"hf-token":"provider-key":"include"===includeCredentials?"credentials-include":"none"})(),modelId=endpointUrl??resolvedModel,url=providerHelper.makeUrl({authMethod:authMethod,model:modelId,task:task}),headers=providerHelper.prepareHeaders({accessToken:accessToken,authMethod:authMethod},"data"in args&&!!args.data);billTo&&(headers[HF_HEADER_X_BILL_TO]=billTo);const userAgent=[`${PACKAGE_NAME}/${PACKAGE_VERSION}`,"undefined"!=typeof navigator?navigator.userAgent:void 0].filter(x=>void 0!==x).join(" ");headers["User-Agent"]=userAgent;const body=providerHelper.makeBody({args:remainingArgs,model:resolvedModel,task:task,mapping:mapping});let credentials;"string"==typeof includeCredentials?credentials=includeCredentials:!0===includeCredentials&&(credentials="include");return{url:url,info:{headers:headers,method:"POST",body:body,...credentials?{credentials:credentials}:void 0,signal:signal}}}function removeProviderPrefix(model,provider){if(!model.startsWith(`${provider}/`))throw new InferenceClientInputError(`Models from ${provider} must be prefixed by "${provider}/". Got "${model}".`);return model.slice(provider.length+1)}function getLines(onLine){let buffer,position,fieldLength,discardTrailingNewline=!1;return function(arr){void 0===buffer?(buffer=arr,position=0,fieldLength=-1):buffer=function(a,b){const res=new Uint8Array(a.length+b.length);return res.set(a),res.set(b,a.length),res}(buffer,arr);const bufLength=buffer.length;let lineStart=0;for(;position<bufLength;){discardTrailingNewline&&(10===buffer[position]&&(lineStart=++position),discardTrailingNewline=!1);let lineEnd=-1;for(;position<bufLength&&-1===lineEnd;++position)switch(buffer[position]){case 58:-1===fieldLength&&(fieldLength=position-lineStart);break;case 13:discardTrailingNewline=!0;case 10:lineEnd=position}if(-1===lineEnd)break;onLine(buffer.subarray(lineStart,lineEnd),fieldLength),lineStart=position,fieldLength=-1}lineStart===bufLength?buffer=void 0:0!==lineStart&&(buffer=buffer.subarray(lineStart),position-=lineStart)}}function bodyToJson(body){let data=null;if(body instanceof Blob||body instanceof ArrayBuffer)data="[Blob or ArrayBuffer]";else if("string"==typeof body)try{data=JSON.parse(body)}catch{data=body}return data.accessToken&&(data.accessToken="[REDACTED]"),data}async function innerRequest(args,providerHelper,options){const{url:url,info:info}=await makeRequestOptions(args,providerHelper,options),response=await(options?.fetch??fetch)(url,info),requestContext={url:url,info:info};if(!1!==options?.retry_on_error&&503===response.status)return innerRequest(args,providerHelper,options);if(!response.ok){const contentType=response.headers.get("Content-Type");if(["application/json","application/problem+json"].some(ct=>contentType?.startsWith(ct))){const output=await response.json();if([400,422,404,500].includes(response.status)&&options?.chatCompletion)throw new InferenceClientProviderApiError(`Provider ${args.provider} does not seem to support chat completion for model ${args.model} . Error: ${JSON.stringify(output.error)}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output});throw"string"==typeof output.error||"string"==typeof output.detail||"string"==typeof output.message?new InferenceClientProviderApiError(`Failed to perform inference: ${output.error??output.detail??output.message}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output}):new InferenceClientProviderApiError("Failed to perform inference: an HTTP error occurred when requesting the provider.",{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output})}const message=contentType?.startsWith("text/plain;")?await response.text():void 0;throw new InferenceClientProviderApiError(`Failed to perform inference: ${message??"an HTTP error occurred when requesting the provider"}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:message??""})}if(response.headers.get("Content-Type")?.startsWith("application/json")){return{data:await response.json(),requestContext:requestContext}}return{data:await response.blob(),requestContext:requestContext}}async function*innerStreamingRequest(args,providerHelper,options){const{url:url,info:info}=await makeRequestOptions({...args,stream:!0},providerHelper,options),response=await(options?.fetch??fetch)(url,info);if(!1!==options?.retry_on_error&&503===response.status)return yield*innerStreamingRequest(args,providerHelper,options);if(!response.ok){if(response.headers.get("Content-Type")?.startsWith("application/json")){const output=await response.json();if([400,422,404,500].includes(response.status)&&options?.chatCompletion)throw new InferenceClientProviderApiError(`Provider ${args.provider} does not seem to support chat completion for model ${args.model} . Error: ${JSON.stringify(output.error)}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output});if("string"==typeof output.error)throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output});if(output.error&&"message"in output.error&&"string"==typeof output.error.message)throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error.message}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output});if("string"==typeof output.message)throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.message}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:output})}throw new InferenceClientProviderApiError("Failed to perform inference: an HTTP error occurred when requesting the provider.",{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:""})}if(!response.headers.get("content-type")?.startsWith("text/event-stream"))throw new InferenceClientProviderApiError("Failed to perform inference: server does not support event stream content type, it returned "+response.headers.get("content-type"),{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:""});if(!response.body)return;const reader=response.body.getReader();let events=[];const onChunk=getLines(function(onId,onRetry,onMessage){let message={data:"",event:"",id:"",retry:void 0};const decoder=new TextDecoder;return function(line,fieldLength){if(0===line.length)onMessage?.(message),message={data:"",event:"",id:"",retry:void 0};else if(fieldLength>0){const field=decoder.decode(line.subarray(0,fieldLength)),valueOffset=fieldLength+(32===line[fieldLength+1]?2:1),value=decoder.decode(line.subarray(valueOffset));switch(field){case"data":message.data=message.data?message.data+"\n"+value:value;break;case"event":message.event=value;break;case"id":onId(message.id=value);break;case"retry":{const retry=parseInt(value,10);isNaN(retry)||onRetry(message.retry=retry);break}}}}}(()=>{},()=>{},event=>{events.push(event)}));try{for(;;){const{done:done,value:value}=await reader.read();if(done)return;onChunk(value);for(const event of events)if(event.data.length>0){if("[DONE]"===event.data)return;const data=JSON.parse(event.data);if("object"==typeof data&&null!==data&&"error"in data){const errorStr="string"==typeof data.error?data.error:"object"==typeof data.error&&data.error&&"message"in data.error&&"string"==typeof data.error.message?data.error.message:JSON.stringify(data.error);throw new InferenceClientProviderApiError(`Failed to perform inference: an occurred while streaming the response: ${errorStr}`,{url:url,method:info.method??"GET",headers:info.headers,body:bodyToJson(info.body)},{requestId:response.headers.get("x-request-id")??"",status:response.status,body:data})}yield data}events=[]}}finally{reader.releaseLock()}}async function request(args,options){getLogger().warn("The request method is deprecated and will be removed in a future version of huggingface.js. Use specific task functions instead.");const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),options?.task);return(await innerRequest(args,providerHelper,options)).data}async function*streamingRequest(args,options){getLogger().warn("The streamingRequest method is deprecated and will be removed in a future version of huggingface.js. Use specific task functions instead.");const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),options?.task);yield*innerStreamingRequest(args,providerHelper,options)}function preparePayload(args){return"data"in args?args:{...omit(args,"inputs"),data:args.inputs}}async function audioClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"audio-classification"),payload=preparePayload(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"audio-classification"});return providerHelper.getResponse(res)}async function audioToAudio(args,options){const model="inputs"in args?args.model:void 0,providerHelper=getProviderHelper(await resolveProvider(args.provider,model),"audio-to-audio"),payload=preparePayload(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"audio-to-audio"});return providerHelper.getResponse(res)}async function automaticSpeechRecognition(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"automatic-speech-recognition"),payload=await providerHelper.preparePayloadAsync(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"automatic-speech-recognition"});return providerHelper.getResponse(res)}async function textToSpeech$1(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"text-to-speech"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"text-to-speech"});return providerHelper.getResponse(res)}function preparePayload2(args){return"data"in args?args:{...omit(args,"inputs"),data:args.inputs}}async function imageClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"image-classification"),payload=preparePayload2(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"image-classification"});return providerHelper.getResponse(res)}async function imageSegmentation(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"image-segmentation"),payload=await providerHelper.preparePayloadAsync(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"image-segmentation"}),{url:url,info:info}=await makeRequestOptions(args,providerHelper,{...options,task:"image-segmentation"});return providerHelper.getResponse(res,url,info.headers)}async function imageToImage$1(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"image-to-image"),payload=await providerHelper.preparePayloadAsync(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"image-to-image"}),{url:url,info:info}=await makeRequestOptions(args,providerHelper,{...options,task:"image-to-image"});return providerHelper.getResponse(res,url,info.headers)}async function imageToText$1(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"image-to-text"),payload=preparePayload2(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"image-to-text"});return providerHelper.getResponse(res[0])}async function imageToVideo(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"image-to-video"),payload=await providerHelper.preparePayloadAsync(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"image-to-video"}),{url:url,info:info}=await makeRequestOptions(args,providerHelper,{...options,task:"image-to-video"});return providerHelper.getResponse(res,url,info.headers)}async function objectDetection(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"object-detection"),payload=preparePayload2(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"object-detection"});return providerHelper.getResponse(res)}async function textToImage$1(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"text-to-image"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"text-to-image"}),{url:url,info:info}=await makeRequestOptions(args,providerHelper,{...options,task:"text-to-image"});return providerHelper.getResponse(res,url,info.headers,options?.outputType)}async function textToVideo$1(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"text-to-video"),{data:response}=await innerRequest(args,providerHelper,{...options,task:"text-to-video"}),{url:url,info:info}=await makeRequestOptions(args,providerHelper,{...options,task:"text-to-video"});return providerHelper.getResponse(response,url,info.headers)}async function zeroShotImageClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"zero-shot-image-classification"),payload=await async function(args){return args.inputs instanceof Blob?{...args,inputs:{image:base64FromBytes(new Uint8Array(await args.inputs.arrayBuffer()))}}:{...args,inputs:{image:base64FromBytes(new Uint8Array(args.inputs.image instanceof ArrayBuffer?args.inputs.image:await args.inputs.image.arrayBuffer()))}}}(args),{data:res}=await innerRequest(payload,providerHelper,{...options,task:"zero-shot-image-classification"});return providerHelper.getResponse(res)}async function chatCompletion(args,options){let providerHelper;if(args.provider&&"auto"!==args.provider){providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"conversational")}else providerHelper=new AutoRouterConversationalTask;const{data:response}=await innerRequest(args,providerHelper,{...options,task:"conversational"});return providerHelper.getResponse(response)}async function*chatCompletionStream(args,options){let providerHelper;if(args.provider&&"auto"!==args.provider){providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"conversational")}else providerHelper=new AutoRouterConversationalTask;yield*innerStreamingRequest(args,providerHelper,{...options,task:"conversational"})}async function featureExtraction(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"feature-extraction"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"feature-extraction"});return providerHelper.getResponse(res)}async function fillMask(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"fill-mask"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"fill-mask"});return providerHelper.getResponse(res)}async function questionAnswering(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"question-answering"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"question-answering"});return providerHelper.getResponse(res)}async function sentenceSimilarity(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"sentence-similarity"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"sentence-similarity"});return providerHelper.getResponse(res)}async function summarization(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"summarization"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"summarization"});return providerHelper.getResponse(res)}async function tableQuestionAnswering(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"table-question-answering"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"table-question-answering"});return providerHelper.getResponse(res)}async function textClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"text-classification"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"text-classification"});return providerHelper.getResponse(res)}async function textGeneration(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"text-generation"),{data:response}=await innerRequest(args,providerHelper,{...options,task:"text-generation"});return providerHelper.getResponse(response)}async function*textGenerationStream(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"text-generation");yield*innerStreamingRequest(args,providerHelper,{...options,task:"text-generation"})}async function tokenClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"token-classification"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"token-classification"});return providerHelper.getResponse(res)}async function translation(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"translation"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"translation"});return providerHelper.getResponse(res)}async function zeroShotClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"zero-shot-classification"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"zero-shot-classification"});return providerHelper.getResponse(res)}async function documentQuestionAnswering(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"document-question-answering"),reqArgs={...args,inputs:{question:args.inputs.question,image:base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))}},{data:res}=await innerRequest(reqArgs,providerHelper,{...options,task:"document-question-answering"});return providerHelper.getResponse(res)}async function visualQuestionAnswering(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"visual-question-answering"),reqArgs={...args,inputs:{question:args.inputs.question,image:base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))}},{data:res}=await innerRequest(reqArgs,providerHelper,{...options,task:"visual-question-answering"});return providerHelper.getResponse(res)}async function tabularClassification(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"tabular-classification"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"tabular-classification"});return providerHelper.getResponse(res)}async function tabularRegression(args,options){const providerHelper=getProviderHelper(await resolveProvider(args.provider,args.model,args.endpointUrl),"tabular-regression"),{data:res}=await innerRequest(args,providerHelper,{...options,task:"tabular-regression"});return providerHelper.getResponse(res)}var InferenceClient=class _InferenceClient{constructor(accessToken="",defaultOptions={}){__publicField(this,"accessToken"),__publicField(this,"defaultOptions"),this.accessToken=accessToken,this.defaultOptions=defaultOptions;for(const[name,fn]of(obj=tasks_exports,Object.entries(obj)))Object.defineProperty(this,name,{enumerable:!1,value:(params,options)=>fn({endpointUrl:defaultOptions.endpointUrl,accessToken:accessToken,...params},{...omit(defaultOptions,["endpointUrl"]),...options})});var obj}endpoint(endpointUrl){return new _InferenceClient(this.accessToken,{...this.defaultOptions,endpointUrl:endpointUrl})}};__export({},{getInferenceSnippets:()=>getInferenceSnippets});var TOKEN_TYPES=Object.freeze({Text:"Text",NumericLiteral:"NumericLiteral",StringLiteral:"StringLiteral",Identifier:"Identifier",Equals:"Equals",OpenParen:"OpenParen",CloseParen:"CloseParen",OpenStatement:"OpenStatement",CloseStatement:"CloseStatement",OpenExpression:"OpenExpression",CloseExpression:"CloseExpression",OpenSquareBracket:"OpenSquareBracket",CloseSquareBracket:"CloseSquareBracket",OpenCurlyBracket:"OpenCurlyBracket",CloseCurlyBracket:"CloseCurlyBracket",Comma:"Comma",Dot:"Dot",Colon:"Colon",Pipe:"Pipe",CallOperator:"CallOperator",AdditiveBinaryOperator:"AdditiveBinaryOperator",MultiplicativeBinaryOperator:"MultiplicativeBinaryOperator",ComparisonBinaryOperator:"ComparisonBinaryOperator",UnaryOperator:"UnaryOperator",Comment:"Comment"}),Token=class{constructor(value,type){this.value=value,this.type=type}};function isWord(char){return/\w/.test(char)}function isInteger(char){return/[0-9]/.test(char)}function isWhitespace(char){return/\s/.test(char)}var ORDERED_MAPPING_TABLE=[["{%",TOKEN_TYPES.OpenStatement],["%}",TOKEN_TYPES.CloseStatement],["{{",TOKEN_TYPES.OpenExpression],["}}",TOKEN_TYPES.CloseExpression],["(",TOKEN_TYPES.OpenParen],[")",TOKEN_TYPES.CloseParen],["{",TOKEN_TYPES.OpenCurlyBracket],["}",TOKEN_TYPES.CloseCurlyBracket],["[",TOKEN_TYPES.OpenSquareBracket],["]",TOKEN_TYPES.CloseSquareBracket],[",",TOKEN_TYPES.Comma],[".",TOKEN_TYPES.Dot],[":",TOKEN_TYPES.Colon],["|",TOKEN_TYPES.Pipe],["<=",TOKEN_TYPES.ComparisonBinaryOperator],[">=",TOKEN_TYPES.ComparisonBinaryOperator],["==",TOKEN_TYPES.ComparisonBinaryOperator],["!=",TOKEN_TYPES.ComparisonBinaryOperator],["<",TOKEN_TYPES.ComparisonBinaryOperator],[">",TOKEN_TYPES.ComparisonBinaryOperator],["+",TOKEN_TYPES.AdditiveBinaryOperator],["-",TOKEN_TYPES.AdditiveBinaryOperator],["~",TOKEN_TYPES.AdditiveBinaryOperator],["*",TOKEN_TYPES.MultiplicativeBinaryOperator],["/",TOKEN_TYPES.MultiplicativeBinaryOperator],["%",TOKEN_TYPES.MultiplicativeBinaryOperator],["=",TOKEN_TYPES.Equals]],ESCAPE_CHARACTERS=new Map([["n","\n"],["t","\t"],["r","\r"],["b","\b"],["f","\f"],["v","\v"],["'","'"],['"','"'],["\\","\\"]]);var Statement=class{constructor(){__publicField(this,"type","Statement")}},Program=class extends Statement{constructor(body){super(),__publicField(this,"type","Program"),this.body=body}},If=class extends Statement{constructor(test,body,alternate){super(),__publicField(this,"type","If"),this.test=test,this.body=body,this.alternate=alternate}},For=class extends Statement{constructor(loopvar,iterable,body,defaultBlock){super(),__publicField(this,"type","For"),this.loopvar=loopvar,this.iterable=iterable,this.body=body,this.defaultBlock=defaultBlock}},Break=class extends Statement{constructor(){super(...arguments),__publicField(this,"type","Break")}},Continue=class extends Statement{constructor(){super(...arguments),__publicField(this,"type","Continue")}},SetStatement=class extends Statement{constructor(assignee,value,body){super(),__publicField(this,"type","Set"),this.assignee=assignee,this.value=value,this.body=body}},Macro=class extends Statement{constructor(name,args,body){super(),__publicField(this,"type","Macro"),this.name=name,this.args=args,this.body=body}},Comment=class extends Statement{constructor(value){super(),__publicField(this,"type","Comment"),this.value=value}},Expression=class extends Statement{constructor(){super(...arguments),__publicField(this,"type","Expression")}},MemberExpression=class extends Expression{constructor(object,property,computed){super(),__publicField(this,"type","MemberExpression"),this.object=object,this.property=property,this.computed=computed}},CallExpression=class extends Expression{constructor(callee,args){super(),__publicField(this,"type","CallExpression"),this.callee=callee,this.args=args}},Identifier=class extends Expression{constructor(value){super(),__publicField(this,"type","Identifier"),this.value=value}},Literal=class extends Expression{constructor(value){super(),__publicField(this,"type","Literal"),this.value=value}},IntegerLiteral=class extends Literal{constructor(){super(...arguments),__publicField(this,"type","IntegerLiteral")}},FloatLiteral=class extends Literal{constructor(){super(...arguments),__publicField(this,"type","FloatLiteral")}},StringLiteral=class extends Literal{constructor(){super(...arguments),__publicField(this,"type","StringLiteral")}},ArrayLiteral=class extends Literal{constructor(){super(...arguments),__publicField(this,"type","ArrayLiteral")}},TupleLiteral=class extends Literal{constructor(){super(...arguments),__publicField(this,"type","TupleLiteral")}},ObjectLiteral=class extends Literal{constructor(){super(...arguments),__publicField(this,"type","ObjectLiteral")}},BinaryExpression=class extends Expression{constructor(operator,left,right){super(),__publicField(this,"type","BinaryExpression"),this.operator=operator,this.left=left,this.right=right}},FilterExpression=class extends Expression{constructor(operand,filter){super(),__publicField(this,"type","FilterExpression"),this.operand=operand,this.filter=filter}},FilterStatement=class extends Statement{constructor(filter,body){super(),__publicField(this,"type","FilterStatement"),this.filter=filter,this.body=body}},SelectExpression=class extends Expression{constructor(lhs,test){super(),__publicField(this,"type","SelectExpression"),this.lhs=lhs,this.test=test}},TestExpression=class extends Expression{constructor(operand,negate,test){super(),__publicField(this,"type","TestExpression"),this.operand=operand,this.negate=negate,this.test=test}},UnaryExpression=class extends Expression{constructor(operator,argument){super(),__publicField(this,"type","UnaryExpression"),this.operator=operator,this.argument=argument}},SliceExpression=class extends Expression{constructor(start=void 0,stop=void 0,step=void 0){super(),__publicField(this,"type","SliceExpression"),this.start=start,this.stop=stop,this.step=step}},KeywordArgumentExpression=class extends Expression{constructor(key,value){super(),__publicField(this,"type","KeywordArgumentExpression"),this.key=key,this.value=value}},SpreadExpression=class extends Expression{constructor(argument){super(),__publicField(this,"type","SpreadExpression"),this.argument=argument}},CallStatement=class extends Statement{constructor(call,callerArgs,body){super(),__publicField(this,"type","CallStatement"),this.call=call,this.callerArgs=callerArgs,this.body=body}},Ternary=class extends Expression{constructor(condition,trueExpr,falseExpr){super(),__publicField(this,"type","Ternary"),this.condition=condition,this.trueExpr=trueExpr,this.falseExpr=falseExpr}};function parse(tokens){const program=new Program([]);let current=0;function expect(type,error){const prev=tokens[current++];if(!prev||prev.type!==type)throw new Error(`Parser Error: ${error}. ${prev.type} !== ${type}.`);return prev}function expectIdentifier(name){if(!isIdentifier(name))throw new SyntaxError(`Expected ${name}`);++current}function parseAny(){switch(tokens[current].type){case TOKEN_TYPES.Comment:return new Comment(tokens[current++].value);case TOKEN_TYPES.Text:return new StringLiteral(expect(TOKEN_TYPES.Text,"Expected text token").value);case TOKEN_TYPES.OpenStatement:return function(){if(expect(TOKEN_TYPES.OpenStatement,"Expected opening statement token"),tokens[current].type!==TOKEN_TYPES.Identifier)throw new SyntaxError(`Unknown statement, got ${tokens[current].type}`);const name=tokens[current].value;let result;switch(name){case"set":++current,result=function(){const left=parseExpressionSequence();let value=null;const body=[];if(is(TOKEN_TYPES.Equals))++current,value=parseExpressionSequence();else{for(expect(TOKEN_TYPES.CloseStatement,"Expected %} token");!isStatement("endset");)body.push(parseAny());expect(TOKEN_TYPES.OpenStatement,"Expected {% token"),expectIdentifier("endset")}return expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token"),new SetStatement(left,value,body)}();break;case"if":++current,result=parseIfStatement(),expect(TOKEN_TYPES.OpenStatement,"Expected {% token"),expectIdentifier("endif"),expect(TOKEN_TYPES.CloseStatement,"Expected %} token");break;case"macro":++current,result=function(){const name=parsePrimaryExpression();if("Identifier"!==name.type)throw new SyntaxError("Expected identifier following macro statement");const args=parseArgs();expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");const body=[];for(;!isStatement("endmacro");)body.push(parseAny());return new Macro(name,args,body)}(),expect(TOKEN_TYPES.OpenStatement,"Expected {% token"),expectIdentifier("endmacro"),expect(TOKEN_TYPES.CloseStatement,"Expected %} token");break;case"for":++current,result=function(){const loopVariable=parseExpressionSequence(!0);if(!(loopVariable instanceof Identifier||loopVariable instanceof TupleLiteral))throw new SyntaxError(`Expected identifier/tuple for the loop variable, got ${loopVariable.type} instead`);if(!isIdentifier("in"))throw new SyntaxError("Expected `in` keyword following loop variable");++current;const iterable=parseExpression();expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");const body=[];for(;!isStatement("endfor","else");)body.push(parseAny());const alternative=[];if(isStatement("else"))for(++current,++current,expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");!isStatement("endfor");)alternative.push(parseAny());return new For(loopVariable,iterable,body,alternative)}(),expect(TOKEN_TYPES.OpenStatement,"Expected {% token"),expectIdentifier("endfor"),expect(TOKEN_TYPES.CloseStatement,"Expected %} token");break;case"call":{++current;let callerArgs=null;is(TOKEN_TYPES.OpenParen)&&(callerArgs=parseArgs());const callee=parsePrimaryExpression();if("Identifier"!==callee.type)throw new SyntaxError("Expected identifier following call statement");const callArgs=parseArgs();expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");const body=[];for(;!isStatement("endcall");)body.push(parseAny());expect(TOKEN_TYPES.OpenStatement,"Expected '{%'"),expectIdentifier("endcall"),expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");const callExpr=new CallExpression(callee,callArgs);result=new CallStatement(callExpr,callerArgs,body);break}case"break":++current,expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token"),result=new Break;break;case"continue":++current,expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token"),result=new Continue;break;case"filter":{++current;let filterNode=parsePrimaryExpression();filterNode instanceof Identifier&&is(TOKEN_TYPES.OpenParen)&&(filterNode=parseCallExpression(filterNode)),expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");const filterBody=[];for(;!isStatement("endfilter");)filterBody.push(parseAny());expect(TOKEN_TYPES.OpenStatement,"Expected '{%'"),expectIdentifier("endfilter"),expect(TOKEN_TYPES.CloseStatement,"Expected '%}'"),result=new FilterStatement(filterNode,filterBody);break}default:throw new SyntaxError(`Unknown statement type: ${name}`)}return result}();case TOKEN_TYPES.OpenExpression:return function(){expect(TOKEN_TYPES.OpenExpression,"Expected opening expression token");const result=parseExpression();return expect(TOKEN_TYPES.CloseExpression,"Expected closing expression token"),result}();default:throw new SyntaxError(`Unexpected token type: ${tokens[current].type}`)}}function is(...types){return current+types.length<=tokens.length&&types.every((type,i)=>type===tokens[current+i].type)}function isStatement(...names){return tokens[current]?.type===TOKEN_TYPES.OpenStatement&&tokens[current+1]?.type===TOKEN_TYPES.Identifier&&names.includes(tokens[current+1]?.value)}function isIdentifier(...names){return current+names.length<=tokens.length&&names.every((name,i)=>"Identifier"===tokens[current+i].type&&name===tokens[current+i].value)}function parseIfStatement(){const test=parseExpression();expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");const body=[],alternate=[];for(;!isStatement("elif","else","endif");)body.push(parseAny());if(isStatement("elif")){++current,++current;const result=parseIfStatement();alternate.push(result)}else if(isStatement("else"))for(++current,++current,expect(TOKEN_TYPES.CloseStatement,"Expected closing statement token");!isStatement("endif");)alternate.push(parseAny());return new If(test,body,alternate)}function parseExpressionSequence(primary=!1){const fn=primary?parsePrimaryExpression:parseExpression,expressions=[fn()],isTuple=is(TOKEN_TYPES.Comma);for(;isTuple&&(++current,expressions.push(fn()),is(TOKEN_TYPES.Comma)););return isTuple?new TupleLiteral(expressions):expressions[0]}function parseExpression(){return parseIfExpression()}function parseIfExpression(){const a=parseLogicalOrExpression();if(isIdentifier("if")){++current;const test=parseLogicalOrExpression();if(isIdentifier("else")){++current;const falseExpr=parseIfExpression();return new Ternary(test,a,falseExpr)}return new SelectExpression(a,test)}return a}function parseLogicalOrExpression(){let left=parseLogicalAndExpression();for(;isIdentifier("or");){const operator=tokens[current];++current;const right=parseLogicalAndExpression();left=new BinaryExpression(operator,left,right)}return left}function parseLogicalAndExpression(){let left=parseLogicalNegationExpression();for(;isIdentifier("and");){const operator=tokens[current];++current;const right=parseLogicalNegationExpression();left=new BinaryExpression(operator,left,right)}return left}function parseLogicalNegationExpression(){let right;for(;isIdentifier("not");){const operator=tokens[current];++current;const arg=parseLogicalNegationExpression();right=new UnaryExpression(operator,arg)}return right??function(){let left=parseAdditiveExpression();for(;;){let operator;if(isIdentifier("not","in"))operator=new Token("not in",TOKEN_TYPES.Identifier),current+=2;else if(isIdentifier("in"))operator=tokens[current++];else{if(!is(TOKEN_TYPES.ComparisonBinaryOperator))break;operator=tokens[current++]}const right=parseAdditiveExpression();left=new BinaryExpression(operator,left,right)}return left}()}function parseAdditiveExpression(){let left=parseMultiplicativeExpression();for(;is(TOKEN_TYPES.AdditiveBinaryOperator);){const operator=tokens[current];++current;const right=parseMultiplicativeExpression();left=new BinaryExpression(operator,left,right)}return left}function parseCallExpression(callee){let expression=new CallExpression(callee,parseArgs());return expression=parseMemberExpression(expression),is(TOKEN_TYPES.OpenParen)&&(expression=parseCallExpression(expression)),expression}function parseArgs(){expect(TOKEN_TYPES.OpenParen,"Expected opening parenthesis for arguments list");const args=function(){const args=[];for(;!is(TOKEN_TYPES.CloseParen);){let argument;if(tokens[current].type===TOKEN_TYPES.MultiplicativeBinaryOperator&&"*"===tokens[current].value){++current;const expr=parseExpression();argument=new SpreadExpression(expr)}else if(argument=parseExpression(),is(TOKEN_TYPES.Equals)){if(++current,!(argument instanceof Identifier))throw new SyntaxError("Expected identifier for keyword argument");const value=parseExpression();argument=new KeywordArgumentExpression(argument,value)}args.push(argument),is(TOKEN_TYPES.Comma)&&++current}return args}();return expect(TOKEN_TYPES.CloseParen,"Expected closing parenthesis for arguments list"),args}function parseMemberExpressionArgumentsList(){const slices=[];let isSlice=!1;for(;!is(TOKEN_TYPES.CloseSquareBracket);)is(TOKEN_TYPES.Colon)?(slices.push(void 0),++current,isSlice=!0):(slices.push(parseExpression()),is(TOKEN_TYPES.Colon)&&(++current,isSlice=!0));if(0===slices.length)throw new SyntaxError("Expected at least one argument for member/slice expression");if(isSlice){if(slices.length>3)throw new SyntaxError("Expected 0-3 arguments for slice expression");return new SliceExpression(...slices)}return slices[0]}function parseMemberExpression(object){for(;is(TOKEN_TYPES.Dot)||is(TOKEN_TYPES.OpenSquareBracket);){const operator=tokens[current];let property;++current;const computed=operator.type===TOKEN_TYPES.OpenSquareBracket;if(computed)property=parseMemberExpressionArgumentsList(),expect(TOKEN_TYPES.CloseSquareBracket,"Expected closing square bracket");else if(property=parsePrimaryExpression(),"Identifier"!==property.type)throw new SyntaxError("Expected identifier following dot operator");object=new MemberExpression(object,property,computed)}return object}function parseMultiplicativeExpression(){let left=parseTestExpression();for(;is(TOKEN_TYPES.MultiplicativeBinaryOperator);){const operator=tokens[current++],right=parseTestExpression();left=new BinaryExpression(operator,left,right)}return left}function parseTestExpression(){let operand=function(){let operand=function(){const member=parseMemberExpression(parsePrimaryExpression());return is(TOKEN_TYPES.OpenParen)?parseCallExpression(member):member}();for(;is(TOKEN_TYPES.Pipe);){++current;let filter=parsePrimaryExpression();if(!(filter instanceof Identifier))throw new SyntaxError("Expected identifier for the filter");is(TOKEN_TYPES.OpenParen)&&(filter=parseCallExpression(filter)),operand=new FilterExpression(operand,filter)}return operand}();for(;isIdentifier("is");){++current;const negate=isIdentifier("not");negate&&++current;const filter=parsePrimaryExpression();if(!(filter instanceof Identifier))throw new SyntaxError("Expected identifier for the test");operand=new TestExpression(operand,negate,filter)}return operand}function parsePrimaryExpression(){const token=tokens[current++];switch(token.type){case TOKEN_TYPES.NumericLiteral:{const num=token.value;return num.includes(".")?new FloatLiteral(Number(num)):new IntegerLiteral(Number(num))}case TOKEN_TYPES.StringLiteral:{let value=token.value;for(;is(TOKEN_TYPES.StringLiteral);)value+=tokens[current++].value;return new StringLiteral(value)}case TOKEN_TYPES.Identifier:return new Identifier(token.value);case TOKEN_TYPES.OpenParen:{const expression=parseExpressionSequence();return expect(TOKEN_TYPES.CloseParen,"Expected closing parenthesis, got ${tokens[current].type} instead."),expression}case TOKEN_TYPES.OpenSquareBracket:{const values=[];for(;!is(TOKEN_TYPES.CloseSquareBracket);)values.push(parseExpression()),is(TOKEN_TYPES.Comma)&&++current;return++current,new ArrayLiteral(values)}case TOKEN_TYPES.OpenCurlyBracket:{const values=new Map;for(;!is(TOKEN_TYPES.CloseCurlyBracket);){const key=parseExpression();expect(TOKEN_TYPES.Colon,"Expected colon between key and value in object literal");const value=parseExpression();values.set(key,value),is(TOKEN_TYPES.Comma)&&++current}return++current,new ObjectLiteral(values)}default:throw new SyntaxError(`Unexpected token: ${token.type}`)}}for(;current<tokens.length;)program.body.push(parseAny());return program}function range(start,stop,step=1){void 0===stop&&(stop=start,start=0);const result=[];for(let i=start;i<stop;i+=step)result.push(i);return result}function slice(array,start,stop,step=1){const direction=Math.sign(step);direction>=0?(start=(start??(start=0))<0?Math.max(array.length+start,0):Math.min(start,array.length),stop=(stop??(stop=array.length))<0?Math.max(array.length+stop,0):Math.min(stop,array.length)):(start=(start??(start=array.length-1))<0?Math.max(array.length+start,-1):Math.min(start,array.length-1),stop=(stop??(stop=-1))<-1?Math.max(array.length+stop,-1):Math.min(stop,array.length-1));const result=[];for(let i=start;direction*i<direction*stop;i+=step)result.push(array[i]);return result}function strftime_now(format2){return function(date,format2){const monthFormatterLong=new Intl.DateTimeFormat(void 0,{month:"long"}),monthFormatterShort=new Intl.DateTimeFormat(void 0,{month:"short"}),pad2=n=>n<10?"0"+n:n.toString();return format2.replace(/%[YmdbBHM%]/g,token=>{switch(token){case"%Y":return date.getFullYear().toString();case"%m":return pad2(date.getMonth()+1);case"%d":return pad2(date.getDate());case"%b":return monthFormatterShort.format(date);case"%B":return monthFormatterLong.format(date);case"%H":return pad2(date.getHours());case"%M":return pad2(date.getMinutes());case"%%":return"%";default:return token}})}(new Date,format2)}var BreakControl=class extends Error{},ContinueControl=class extends Error{},RuntimeValue=class{constructor(value=void 0){__publicField(this,"type","RuntimeValue"),__publicField(this,"value"),__publicField(this,"builtins",new Map),this.value=value}__bool__(){return new BooleanValue(!!this.value)}toString(){return String(this.value)}},IntegerValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","IntegerValue")}},FloatValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","FloatValue")}toString(){return this.value%1==0?this.value.toFixed(1):this.value.toString()}},StringValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","StringValue"),__publicField(this,"builtins",new Map([["upper",new FunctionValue(()=>new StringValue(this.value.toUpperCase()))],["lower",new FunctionValue(()=>new StringValue(this.value.toLowerCase()))],["strip",new FunctionValue(()=>new StringValue(this.value.trim()))],["title",new FunctionValue(()=>new StringValue(this.value.replace(/\b\w/g,c=>c.toUpperCase())))],["capitalize",new FunctionValue(()=>new StringValue(this.value.charAt(0).toUpperCase()+this.value.slice(1)))],["length",new IntegerValue(this.value.length)],["rstrip",new FunctionValue(()=>new StringValue(this.value.trimEnd()))],["lstrip",new FunctionValue(()=>new StringValue(this.value.trimStart()))],["startswith",new FunctionValue(args=>{if(0===args.length)throw new Error("startswith() requires at least one argument");const pattern=args[0];if(pattern instanceof StringValue)return new BooleanValue(this.value.startsWith(pattern.value));if(pattern instanceof ArrayValue){for(const item of pattern.value){if(!(item instanceof StringValue))throw new Error("startswith() tuple elements must be strings");if(this.value.startsWith(item.value))return new BooleanValue(!0)}return new BooleanValue(!1)}throw new Error("startswith() argument must be a string or tuple of strings")})],["endswith",new FunctionValue(args=>{if(0===args.length)throw new Error("endswith() requires at least one argument");const pattern=args[0];if(pattern instanceof StringValue)return new BooleanValue(this.value.endsWith(pattern.value));if(pattern instanceof ArrayValue){for(const item of pattern.value){if(!(item instanceof StringValue))throw new Error("endswith() tuple elements must be strings");if(this.value.endsWith(item.value))return new BooleanValue(!0)}return new BooleanValue(!1)}throw new Error("endswith() argument must be a string or tuple of strings")})],["split",new FunctionValue(args=>{const sep=args[0]??new NullValue;if(!(sep instanceof StringValue||sep instanceof NullValue))throw new Error("sep argument must be a string or null");const maxsplit=args[1]??new IntegerValue(-1);if(!(maxsplit instanceof IntegerValue))throw new Error("maxsplit argument must be a number");let result=[];if(sep instanceof NullValue){const text=this.value.trimStart();for(const{0:match,index:index}of text.matchAll(/\S+/g)){if(-1!==maxsplit.value&&result.length>=maxsplit.value&&void 0!==index){result.push(match+text.slice(index+match.length));break}result.push(match)}}else{if(""===sep.value)throw new Error("empty separator");result=this.value.split(sep.value),-1!==maxsplit.value&&result.length>maxsplit.value&&result.push(result.splice(maxsplit.value).join(sep.value))}return new ArrayValue(result.map(part=>new StringValue(part)))})],["replace",new FunctionValue(args=>{if(args.length<2)throw new Error("replace() requires at least two arguments");const oldValue=args[0],newValue=args[1];if(!(oldValue instanceof StringValue&&newValue instanceof StringValue))throw new Error("replace() arguments must be strings");let count;if(count=args.length>2?"KeywordArgumentsValue"===args[2].type?args[2].value.get("count")??new NullValue:args[2]:new NullValue,!(count instanceof IntegerValue||count instanceof NullValue))throw new Error("replace() count argument must be a number or null");return new StringValue(function(str,oldvalue,newvalue,count){if(0===count)return str;let remaining=null==count||count<0?1/0:count;const pattern=0===oldvalue.length?new RegExp("(?=)","gu"):new RegExp(oldvalue.replace(/[.*+?^${}()|[\]\\]/g,"\\$&"),"gu");return str.replaceAll(pattern,match=>remaining>0?(--remaining,newvalue):match)}(this.value,oldValue.value,newValue.value,count.value))})]]))}},BooleanValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","BooleanValue")}};function toJSON(input,indent,depth,convertUndefinedToNull=!0){const currentDepth=depth??0;switch(input.type){case"NullValue":return"null";case"UndefinedValue":return convertUndefinedToNull?"null":"undefined";case"IntegerValue":case"FloatValue":case"StringValue":case"BooleanValue":return JSON.stringify(input.value);case"ArrayValue":case"ObjectValue":{const indentValue=indent?" ".repeat(indent):"",basePadding="\n"+indentValue.repeat(currentDepth),childrenPadding=basePadding+indentValue;if("ArrayValue"===input.type){const core=input.value.map(x=>toJSON(x,indent,currentDepth+1,convertUndefinedToNull));return indent?`[${childrenPadding}${core.join(`,${childrenPadding}`)}${basePadding}]`:`[${core.join(", ")}]`}{const core=Array.from(input.value.entries()).map(([key,value])=>{const v=`"${key}": ${toJSON(value,indent,currentDepth+1,convertUndefinedToNull)}`;return indent?`${childrenPadding}${v}`:v});return indent?`{${core.join(",")}${basePadding}}`:`{${core.join(", ")}}`}}default:throw new Error(`Cannot convert to JSON: ${input.type}`)}}var ObjectValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","ObjectValue"),__publicField(this,"builtins",new Map([["get",new FunctionValue(([key,defaultValue])=>{if(!(key instanceof StringValue))throw new Error(`Object key must be a string: got ${key.type}`);return this.value.get(key.value)??defaultValue??new NullValue})],["items",new FunctionValue(()=>this.items())],["keys",new FunctionValue(()=>this.keys())],["values",new FunctionValue(()=>this.values())],["dictsort",new FunctionValue(args=>{let kwargs=new Map;const positionalArgs=args.filter(arg=>!(arg instanceof KeywordArgumentsValue)||(kwargs=arg.value,!1)),caseSensitive=positionalArgs.at(0)??kwargs.get("case_sensitive")??new BooleanValue(!1);if(!(caseSensitive instanceof BooleanValue))throw new Error("case_sensitive must be a boolean");const by=positionalArgs.at(1)??kwargs.get("by")??new StringValue("key");if(!(by instanceof StringValue))throw new Error("by must be a string");if(!["key","value"].includes(by.value))throw new Error("by must be either 'key' or 'value'");const reverse=positionalArgs.at(2)??kwargs.get("reverse")??new BooleanValue(!1);if(!(reverse instanceof BooleanValue))throw new Error("reverse must be a boolean");const items=Array.from(this.value.entries()).map(([key,value])=>new ArrayValue([new StringValue(key),value])).sort((a,b)=>{const index="key"===by.value?0:1,result=compareRuntimeValues(a.value[index],b.value[index],caseSensitive.value);return reverse.value?-result:result});return new ArrayValue(items)})]]))}__bool__(){return new BooleanValue(this.value.size>0)}items(){return new ArrayValue(Array.from(this.value.entries()).map(([key,value])=>new ArrayValue([new StringValue(key),value])))}keys(){return new ArrayValue(Array.from(this.value.keys()).map(key=>new StringValue(key)))}values(){return new ArrayValue(Array.from(this.value.values()))}toString(){return toJSON(this,null,0,!1)}},KeywordArgumentsValue=class extends ObjectValue{constructor(){super(...arguments),__publicField(this,"type","KeywordArgumentsValue")}},ArrayValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","ArrayValue"),__publicField(this,"builtins",new Map([["length",new IntegerValue(this.value.length)]]))}__bool__(){return new BooleanValue(this.value.length>0)}toString(){return toJSON(this,null,0,!1)}},TupleValue=class extends ArrayValue{constructor(){super(...arguments),__publicField(this,"type","TupleValue")}},FunctionValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","FunctionValue")}},NullValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","NullValue")}},UndefinedValue=class extends RuntimeValue{constructor(){super(...arguments),__publicField(this,"type","UndefinedValue")}},Environment=class{constructor(parent){__publicField(this,"variables",new Map([["namespace",new FunctionValue(args=>{if(0===args.length)return new ObjectValue(new Map);if(1!==args.length||!(args[0]instanceof ObjectValue))throw new Error("`namespace` expects either zero arguments or a single object argument");return args[0]})]])),__publicField(this,"tests",new Map([["boolean",operand=>"BooleanValue"===operand.type],["callable",operand=>operand instanceof FunctionValue],["odd",operand=>{if(!(operand instanceof IntegerValue))throw new Error(`cannot odd on ${operand.type}`);return operand.value%2!=0}],["even",operand=>{if(!(operand instanceof IntegerValue))throw new Error(`cannot even on ${operand.type}`);return operand.value%2==0}],["false",operand=>"BooleanValue"===operand.type&&!operand.value],["true",operand=>"BooleanValue"===operand.type&&operand.value],["none",operand=>"NullValue"===operand.type],["string",operand=>"StringValue"===operand.type],["number",operand=>operand instanceof IntegerValue||operand instanceof FloatValue],["integer",operand=>operand instanceof IntegerValue],["iterable",operand=>"ArrayValue"===operand.type||"StringValue"===operand.type],["mapping",operand=>"ObjectValue"===operand.type],["lower",operand=>{const str=operand.value;return"StringValue"===operand.type&&str===str.toLowerCase()}],["upper",operand=>{const str=operand.value;return"StringValue"===operand.type&&str===str.toUpperCase()}],["none",operand=>"NullValue"===operand.type],["defined",operand=>"UndefinedValue"!==operand.type],["undefined",operand=>"UndefinedValue"===operand.type],["equalto",(a,b)=>a.value===b.value],["eq",(a,b)=>a.value===b.value]])),this.parent=parent}set(name,value){return this.declareVariable(name,convertToRuntimeValues(value))}declareVariable(name,value){if(this.variables.has(name))throw new SyntaxError(`Variable already declared: ${name}`);return this.variables.set(name,value),value}setVariable(name,value){return this.variables.set(name,value),value}resolve(name){if(this.variables.has(name))return this;if(this.parent)return this.parent.resolve(name);throw new Error(`Unknown variable: ${name}`)}lookupVariable(name){try{return this.resolve(name).variables.get(name)??new UndefinedValue}catch{return new UndefinedValue}}};function getAttributeValue(item,attributePath){const parts=attributePath.split(".");let value=item;for(const part of parts)if(value instanceof ObjectValue)value=value.value.get(part)??new UndefinedValue;else{if(!(value instanceof ArrayValue))return new UndefinedValue;{const index=parseInt(part,10);if(!(!isNaN(index)&&index>=0&&index<value.value.length))return new UndefinedValue;value=value.value[index]}}return value}function compareRuntimeValues(a,b,caseSensitive=!1){if(a instanceof NullValue&&b instanceof NullValue)return 0;if(a instanceof NullValue||b instanceof NullValue)throw new Error(`Cannot compare ${a.type} with ${b.type}`);if(a instanceof UndefinedValue&&b instanceof UndefinedValue)return 0;if(a instanceof UndefinedValue||b instanceof UndefinedValue)throw new Error(`Cannot compare ${a.type} with ${b.type}`);const isNumericLike=v=>v instanceof IntegerValue||v instanceof FloatValue||v instanceof BooleanValue,getNumericValue=v=>v instanceof BooleanValue?v.value?1:0:v.value;if(isNumericLike(a)&&isNumericLike(b)){const aNum=getNumericValue(a),bNum=getNumericValue(b);return aNum<bNum?-1:aNum>bNum?1:0}if(a.type!==b.type)throw new Error(`Cannot compare different types: ${a.type} and ${b.type}`);if("StringValue"===a.type){let aStr=a.value,bStr=b.value;return caseSensitive||(aStr=aStr.toLowerCase(),bStr=bStr.toLowerCase()),aStr<bStr?-1:aStr>bStr?1:0}throw new Error(`Cannot compare type: ${a.type}`)}var Interpreter=class{constructor(env){__publicField(this,"global"),this.global=env??new Environment}run(program){return this.evaluate(program,this.global)}evaluateBinaryExpression(node,environment){const left=this.evaluate(node.left,environment);switch(node.operator.value){case"and":return left.__bool__().value?this.evaluate(node.right,environment):left;case"or":return left.__bool__().value?left:this.evaluate(node.right,environment)}const right=this.evaluate(node.right,environment);switch(node.operator.value){case"==":return new BooleanValue(left.value==right.value);case"!=":return new BooleanValue(left.value!=right.value)}if(left instanceof UndefinedValue||right instanceof UndefinedValue){if(right instanceof UndefinedValue&&["in","not in"].includes(node.operator.value))return new BooleanValue("not in"===node.operator.value);throw new Error(`Cannot perform operation ${node.operator.value} on undefined values`)}if(left instanceof NullValue||right instanceof NullValue)throw new Error("Cannot perform operation on null values");if("~"===node.operator.value)return new StringValue(left.value.toString()+right.value.toString());if((left instanceof IntegerValue||left instanceof FloatValue)&&(right instanceof IntegerValue||right instanceof FloatValue)){const a=left.value,b=right.value;switch(node.operator.value){case"+":case"-":case"*":{const res="+"===node.operator.value?a+b:"-"===node.operator.value?a-b:a*b;return left instanceof FloatValue||right instanceof FloatValue?new FloatValue(res):new IntegerValue(res)}case"/":return new FloatValue(a/b);case"%":{const rem=a%b;return left instanceof FloatValue||right instanceof FloatValue?new FloatValue(rem):new IntegerValue(rem)}case"<":return new BooleanValue(a<b);case">":return new BooleanValue(a>b);case">=":return new BooleanValue(a>=b);case"<=":return new BooleanValue(a<=b)}}else if(left instanceof ArrayValue&&right instanceof ArrayValue){if("+"===node.operator.value)return new ArrayValue(left.value.concat(right.value))}else if(right instanceof ArrayValue){const member=void 0!==right.value.find(x=>x.value===left.value);switch(node.operator.value){case"in":return new BooleanValue(member);case"not in":return new BooleanValue(!member)}}if((left instanceof StringValue||right instanceof StringValue)&&"+"===node.operator.value)return new StringValue(left.value.toString()+right.value.toString());if(left instanceof StringValue&&right instanceof StringValue)switch(node.operator.value){case"in":return new BooleanValue(right.value.includes(left.value));case"not in":return new BooleanValue(!right.value.includes(left.value))}if(left instanceof StringValue&&right instanceof ObjectValue)switch(node.operator.value){case"in":return new BooleanValue(right.value.has(left.value));case"not in":return new BooleanValue(!right.value.has(left.value))}throw new SyntaxError(`Unknown operator "${node.operator.value}" between ${left.type} and ${right.type}`)}evaluateArguments(args,environment){const positionalArguments=[],keywordArguments=new Map;for(const argument of args)if("SpreadExpression"===argument.type){const spreadNode=argument,val=this.evaluate(spreadNode.argument,environment);if(!(val instanceof ArrayValue))throw new Error(`Cannot unpack non-iterable type: ${val.type}`);for(const item of val.value)positionalArguments.push(item)}else if("KeywordArgumentExpression"===argument.type){const kwarg=argument;keywordArguments.set(kwarg.key.value,this.evaluate(kwarg.value,environment))}else{if(keywordArguments.size>0)throw new Error("Positional arguments must come before keyword arguments");positionalArguments.push(this.evaluate(argument,environment))}return[positionalArguments,keywordArguments]}applyFilter(operand,filterNode,environment){if("Identifier"===filterNode.type){const filter=filterNode;if("tojson"===filter.value)return new StringValue(toJSON(operand));if(operand instanceof ArrayValue)switch(filter.value){case"list":return operand;case"first":return operand.value[0];case"last":return operand.value[operand.value.length-1];case"length":return new IntegerValue(operand.value.length);case"reverse":return new ArrayValue(operand.value.slice().reverse());case"sort":return new ArrayValue(operand.value.slice().sort((a,b)=>compareRuntimeValues(a,b,!1)));case"join":return new StringValue(operand.value.map(x=>x.value).join(""));case"string":return new StringValue(toJSON(operand,null,0,!1));case"unique":{const seen=new Set,output=[];for(const item of operand.value)seen.has(item.value)||(seen.add(item.value),output.push(item));return new ArrayValue(output)}default:throw new Error(`Unknown ArrayValue filter: ${filter.value}`)}else if(operand instanceof StringValue)switch(filter.value){case"length":case"upper":case"lower":case"title":case"capitalize":{const builtin=operand.builtins.get(filter.value);if(builtin instanceof FunctionValue)return builtin.value([],environment);if(builtin instanceof IntegerValue)return builtin;throw new Error(`Unknown StringValue filter: ${filter.value}`)}case"trim":return new StringValue(operand.value.trim());case"indent":return new StringValue(operand.value.split("\n").map((x,i)=>0===i||0===x.length?x:"    "+x).join("\n"));case"join":case"string":return operand;case"int":{const val=parseInt(operand.value,10);return new IntegerValue(isNaN(val)?0:val)}case"float":{const val=parseFloat(operand.value);return new FloatValue(isNaN(val)?0:val)}default:throw new Error(`Unknown StringValue filter: ${filter.value}`)}else if(operand instanceof IntegerValue||operand instanceof FloatValue)switch(filter.value){case"abs":return operand instanceof IntegerValue?new IntegerValue(Math.abs(operand.value)):new FloatValue(Math.abs(operand.value));case"int":return new IntegerValue(Math.floor(operand.value));case"float":return new FloatValue(operand.value);default:throw new Error(`Unknown NumericValue filter: ${filter.value}`)}else if(operand instanceof ObjectValue)switch(filter.value){case"items":return new ArrayValue(Array.from(operand.value.entries()).map(([key,value])=>new ArrayValue([new StringValue(key),value])));case"length":return new IntegerValue(operand.value.size);default:{const builtin=operand.builtins.get(filter.value);if(builtin)return builtin instanceof FunctionValue?builtin.value([],environment):builtin;throw new Error(`Unknown ObjectValue filter: ${filter.value}`)}}else if(operand instanceof BooleanValue)switch(filter.value){case"bool":return new BooleanValue(operand.value);case"int":return new IntegerValue(operand.value?1:0);case"float":return new FloatValue(operand.value?1:0);case"string":return new StringValue(operand.value?"true":"false");default:throw new Error(`Unknown BooleanValue filter: ${filter.value}`)}throw new Error(`Cannot apply filter "${filter.value}" to type: ${operand.type}`)}if("CallExpression"===filterNode.type){const filter=filterNode;if("Identifier"!==filter.callee.type)throw new Error(`Unknown filter: ${filter.callee.type}`);const filterName=filter.callee.value;if("tojson"===filterName){const[,kwargs]=this.evaluateArguments(filter.args,environment),indent=kwargs.get("indent")??new NullValue;if(!(indent instanceof IntegerValue||indent instanceof NullValue))throw new Error("If set, indent must be a number");return new StringValue(toJSON(operand,indent.value))}if("join"===filterName){let value;if(operand instanceof StringValue)value=Array.from(operand.value);else{if(!(operand instanceof ArrayValue))throw new Error(`Cannot apply filter "${filterName}" to type: ${operand.type}`);value=operand.value.map(x=>x.value)}const[args,kwargs]=this.evaluateArguments(filter.args,environment),separator=args.at(0)??kwargs.get("separator")??new StringValue("");if(!(separator instanceof StringValue))throw new Error("separator must be a string");return new StringValue(value.join(separator.value))}if("int"===filterName||"float"===filterName){const[args,kwargs]=this.evaluateArguments(filter.args,environment),defaultValue=args.at(0)??kwargs.get("default")??("int"===filterName?new IntegerValue(0):new FloatValue(0));if(operand instanceof StringValue){const val="int"===filterName?parseInt(operand.value,10):parseFloat(operand.value);return isNaN(val)?defaultValue:"int"===filterName?new IntegerValue(val):new FloatValue(val)}if(operand instanceof IntegerValue||operand instanceof FloatValue)return operand;if(operand instanceof BooleanValue)return"int"===filterName?new IntegerValue(operand.value?1:0):new FloatValue(operand.value?1:0);throw new Error(`Cannot apply filter "${filterName}" to type: ${operand.type}`)}if("default"===filterName){const[args,kwargs]=this.evaluateArguments(filter.args,environment),defaultValue=args[0]??new StringValue(""),booleanValue=args[1]??kwargs.get("boolean")??new BooleanValue(!1);if(!(booleanValue instanceof BooleanValue))throw new Error("`default` filter flag must be a boolean");return operand instanceof UndefinedValue||booleanValue.value&&!operand.__bool__().value?defaultValue:operand}if(operand instanceof ArrayValue){switch(filterName){case"sort":{const[args,kwargs]=this.evaluateArguments(filter.args,environment),reverse=args.at(0)??kwargs.get("reverse")??new BooleanValue(!1);if(!(reverse instanceof BooleanValue))throw new Error("reverse must be a boolean");const caseSensitive=args.at(1)??kwargs.get("case_sensitive")??new BooleanValue(!1);if(!(caseSensitive instanceof BooleanValue))throw new Error("case_sensitive must be a boolean");const attribute=args.at(2)??kwargs.get("attribute")??new NullValue;if(!(attribute instanceof StringValue||attribute instanceof IntegerValue||attribute instanceof NullValue))throw new Error("attribute must be a string, integer, or null");const getSortValue=item=>{if(attribute instanceof NullValue)return item;return getAttributeValue(item,attribute instanceof IntegerValue?String(attribute.value):attribute.value)};return new ArrayValue(operand.value.slice().sort((a,b)=>{const result=compareRuntimeValues(getSortValue(a),getSortValue(b),caseSensitive.value);return reverse.value?-result:result}))}case"selectattr":case"rejectattr":{const select="selectattr"===filterName;if(operand.value.some(x=>!(x instanceof ObjectValue)))throw new Error(`\`${filterName}\` can only be applied to array of objects`);if(filter.args.some(x=>"StringLiteral"!==x.type))throw new Error(`arguments of \`${filterName}\` must be strings`);const[attr,testName,value]=filter.args.map(x=>this.evaluate(x,environment));let testFunction;if(testName){const test=environment.tests.get(testName.value);if(!test)throw new Error(`Unknown test: ${testName.value}`);testFunction=test}else testFunction=(...x)=>x[0].__bool__().value;const filtered=operand.value.filter(item=>{const a=item.value.get(attr.value),result=!!a&&testFunction(a,value);return select?result:!result});return new ArrayValue(filtered)}case"map":{const[,kwargs]=this.evaluateArguments(filter.args,environment);if(kwargs.has("attribute")){const attr=kwargs.get("attribute");if(!(attr instanceof StringValue))throw new Error("attribute must be a string");const defaultValue=kwargs.get("default"),mapped=operand.value.map(item=>{if(!(item instanceof ObjectValue))throw new Error("items in map must be an object");const value=getAttributeValue(item,attr.value);return value instanceof UndefinedValue?defaultValue??new UndefinedValue:value});return new ArrayValue(mapped)}throw new Error("`map` expressions without `attribute` set are not currently supported.")}}throw new Error(`Unknown ArrayValue filter: ${filterName}`)}if(operand instanceof StringValue){switch(filterName){case"indent":{const[args,kwargs]=this.evaluateArguments(filter.args,environment),width=args.at(0)??kwargs.get("width")??new IntegerValue(4);if(!(width instanceof IntegerValue))throw new Error("width must be a number");const first=args.at(1)??kwargs.get("first")??new BooleanValue(!1),blank=args.at(2)??kwargs.get("blank")??new BooleanValue(!1),lines=operand.value.split("\n"),indent=" ".repeat(width.value),indented=lines.map((x,i)=>!first.value&&0===i||!blank.value&&0===x.length?x:indent+x);return new StringValue(indented.join("\n"))}case"replace":{const replaceFn=operand.builtins.get("replace");if(!(replaceFn instanceof FunctionValue))throw new Error("replace filter not available");const[args,kwargs]=this.evaluateArguments(filter.args,environment);return replaceFn.value([...args,new KeywordArgumentsValue(kwargs)],environment)}}throw new Error(`Unknown StringValue filter: ${filterName}`)}if(operand instanceof ObjectValue){const builtin=operand.builtins.get(filterName);if(builtin&&builtin instanceof FunctionValue){const[args,kwargs]=this.evaluateArguments(filter.args,environment);return kwargs.size>0&&args.push(new KeywordArgumentsValue(kwargs)),builtin.value(args,environment)}throw new Error(`Unknown ObjectValue filter: ${filterName}`)}throw new Error(`Cannot apply filter "${filterName}" to type: ${operand.type}`)}throw new Error(`Unknown filter: ${filterNode.type}`)}evaluateFilterExpression(node,environment){const operand=this.evaluate(node.operand,environment);return this.applyFilter(operand,node.filter,environment)}evaluateTestExpression(node,environment){const operand=this.evaluate(node.operand,environment),test=environment.tests.get(node.test.value);if(!test)throw new Error(`Unknown test: ${node.test.value}`);const result=test(operand);return new BooleanValue(node.negate?!result:result)}evaluateSelectExpression(node,environment){return this.evaluate(node.test,environment).__bool__().value?this.evaluate(node.lhs,environment):new UndefinedValue}evaluateUnaryExpression(node,environment){const argument=this.evaluate(node.argument,environment);if("not"===node.operator.value)return new BooleanValue(!argument.value);throw new SyntaxError(`Unknown operator: ${node.operator.value}`)}evaluateTernaryExpression(node,environment){return this.evaluate(node.condition,environment).__bool__().value?this.evaluate(node.trueExpr,environment):this.evaluate(node.falseExpr,environment)}evalProgram(program,environment){return this.evaluateBlock(program.body,environment)}evaluateBlock(statements,environment){let result="";for(const statement of statements){const lastEvaluated=this.evaluate(statement,environment);"NullValue"!==lastEvaluated.type&&"UndefinedValue"!==lastEvaluated.type&&(result+=lastEvaluated.toString())}return new StringValue(result)}evaluateIdentifier(node,environment){return environment.lookupVariable(node.value)}evaluateCallExpression(expr,environment){const[args,kwargs]=this.evaluateArguments(expr.args,environment);kwargs.size>0&&args.push(new KeywordArgumentsValue(kwargs));const fn=this.evaluate(expr.callee,environment);if("FunctionValue"!==fn.type)throw new Error(`Cannot call something that is not a function: got ${fn.type}`);return fn.value(args,environment)}evaluateSliceExpression(object,expr,environment){if(!(object instanceof ArrayValue||object instanceof StringValue))throw new Error("Slice object must be an array or string");const start=this.evaluate(expr.start,environment),stop=this.evaluate(expr.stop,environment),step=this.evaluate(expr.step,environment);if(!(start instanceof IntegerValue||start instanceof UndefinedValue))throw new Error("Slice start must be numeric or undefined");if(!(stop instanceof IntegerValue||stop instanceof UndefinedValue))throw new Error("Slice stop must be numeric or undefined");if(!(step instanceof IntegerValue||step instanceof UndefinedValue))throw new Error("Slice step must be numeric or undefined");return object instanceof ArrayValue?new ArrayValue(slice(object.value,start.value,stop.value,step.value)):new StringValue(slice(Array.from(object.value),start.value,stop.value,step.value).join(""))}evaluateMemberExpression(expr,environment){const object=this.evaluate(expr.object,environment);let property,value;if(expr.computed){if("SliceExpression"===expr.property.type)return this.evaluateSliceExpression(object,expr.property,environment);property=this.evaluate(expr.property,environment)}else property=new StringValue(expr.property.value);if(object instanceof ObjectValue){if(!(property instanceof StringValue))throw new Error(`Cannot access property with non-string: got ${property.type}`);value=object.value.get(property.value)??object.builtins.get(property.value)}else if(object instanceof ArrayValue||object instanceof StringValue)if(property instanceof IntegerValue)value=object.value.at(property.value),object instanceof StringValue&&(value=new StringValue(object.value.at(property.value)));else{if(!(property instanceof StringValue))throw new Error(`Cannot access property with non-string/non-number: got ${property.type}`);value=object.builtins.get(property.value)}else{if(!(property instanceof StringValue))throw new Error(`Cannot access property with non-string: got ${property.type}`);value=object.builtins.get(property.value)}return value instanceof RuntimeValue?value:new UndefinedValue}evaluateSet(node,environment){const rhs=node.value?this.evaluate(node.value,environment):this.evaluateBlock(node.body,environment);if("Identifier"===node.assignee.type){const variableName=node.assignee.value;environment.setVariable(variableName,rhs)}else if("TupleLiteral"===node.assignee.type){const tuple=node.assignee;if(!(rhs instanceof ArrayValue))throw new Error(`Cannot unpack non-iterable type in set: ${rhs.type}`);const arr=rhs.value;if(arr.length!==tuple.value.length)throw new Error(`Too ${tuple.value.length>arr.length?"few":"many"} items to unpack in set`);for(let i=0;i<tuple.value.length;++i){const elem=tuple.value[i];if("Identifier"!==elem.type)throw new Error(`Cannot unpack to non-identifier in set: ${elem.type}`);environment.setVariable(elem.value,arr[i])}}else{if("MemberExpression"!==node.assignee.type)throw new Error(`Invalid LHS inside assignment expression: ${JSON.stringify(node.assignee)}`);{const member=node.assignee,object=this.evaluate(member.object,environment);if(!(object instanceof ObjectValue))throw new Error("Cannot assign to member of non-object");if("Identifier"!==member.property.type)throw new Error("Cannot assign to member with non-identifier property");object.value.set(member.property.value,rhs)}}return new NullValue}evaluateIf(node,environment){const test=this.evaluate(node.test,environment);return this.evaluateBlock(test.__bool__().value?node.body:node.alternate,environment)}evaluateFor(node,environment){const scope=new Environment(environment);let test,iterable;if("SelectExpression"===node.iterable.type){const select=node.iterable;iterable=this.evaluate(select.lhs,scope),test=select.test}else iterable=this.evaluate(node.iterable,scope);if(!(iterable instanceof ArrayValue||iterable instanceof ObjectValue))throw new Error(`Expected iterable or object type in for loop: got ${iterable.type}`);iterable instanceof ObjectValue&&(iterable=iterable.keys());const items=[],scopeUpdateFunctions=[];for(let i=0;i<iterable.value.length;++i){const loopScope=new Environment(scope),current=iterable.value[i];let scopeUpdateFunction;if("Identifier"===node.loopvar.type)scopeUpdateFunction=scope2=>scope2.setVariable(node.loopvar.value,current);else{if("TupleLiteral"!==node.loopvar.type)throw new Error(`Invalid loop variable(s): ${node.loopvar.type}`);{const loopvar=node.loopvar;if("ArrayValue"!==current.type)throw new Error(`Cannot unpack non-iterable type: ${current.type}`);const c=current;if(loopvar.value.length!==c.value.length)throw new Error(`Too ${loopvar.value.length>c.value.length?"few":"many"} items to unpack`);scopeUpdateFunction=scope2=>{for(let j=0;j<loopvar.value.length;++j){if("Identifier"!==loopvar.value[j].type)throw new Error(`Cannot unpack non-identifier type: ${loopvar.value[j].type}`);scope2.setVariable(loopvar.value[j].value,c.value[j])}}}}if(test){scopeUpdateFunction(loopScope);if(!this.evaluate(test,loopScope).__bool__().value)continue}items.push(current),scopeUpdateFunctions.push(scopeUpdateFunction)}let result="",noIteration=!0;for(let i=0;i<items.length;++i){const loop=new Map([["index",new IntegerValue(i+1)],["index0",new IntegerValue(i)],["revindex",new IntegerValue(items.length-i)],["revindex0",new IntegerValue(items.length-i-1)],["first",new BooleanValue(0===i)],["last",new BooleanValue(i===items.length-1)],["length",new IntegerValue(items.length)],["previtem",i>0?items[i-1]:new UndefinedValue],["nextitem",i<items.length-1?items[i+1]:new UndefinedValue]]);scope.setVariable("loop",new ObjectValue(loop)),scopeUpdateFunctions[i](scope);try{result+=this.evaluateBlock(node.body,scope).value}catch(err){if(err instanceof ContinueControl)continue;if(err instanceof BreakControl)break;throw err}noIteration=!1}if(noIteration){result+=this.evaluateBlock(node.defaultBlock,scope).value}return new StringValue(result)}evaluateMacro(node,environment){return environment.setVariable(node.name.value,new FunctionValue((args,scope)=>{const macroScope=new Environment(scope);let kwargs;args=args.slice(),"KeywordArgumentsValue"===args.at(-1)?.type&&(kwargs=args.pop());for(let i=0;i<node.args.length;++i){const nodeArg=node.args[i],passedArg=args[i];if("Identifier"===nodeArg.type){const identifier=nodeArg;if(!passedArg)throw new Error(`Missing positional argument: ${identifier.value}`);macroScope.setVariable(identifier.value,passedArg)}else{if("KeywordArgumentExpression"!==nodeArg.type)throw new Error(`Unknown argument type: ${nodeArg.type}`);{const kwarg=nodeArg,value=passedArg??kwargs?.value.get(kwarg.key.value)??this.evaluate(kwarg.value,macroScope);macroScope.setVariable(kwarg.key.value,value)}}}return this.evaluateBlock(node.body,macroScope)})),new NullValue}evaluateCallStatement(node,environment){const callerFn=new FunctionValue((callerArgs,callerEnv)=>{const callBlockEnv=new Environment(callerEnv);if(node.callerArgs)for(let i=0;i<node.callerArgs.length;++i){const param=node.callerArgs[i];if("Identifier"!==param.type)throw new Error(`Caller parameter must be an identifier, got ${param.type}`);callBlockEnv.setVariable(param.value,callerArgs[i]??new UndefinedValue)}return this.evaluateBlock(node.body,callBlockEnv)}),[macroArgs,macroKwargs]=this.evaluateArguments(node.call.args,environment);macroArgs.push(new KeywordArgumentsValue(macroKwargs));const fn=this.evaluate(node.call.callee,environment);if("FunctionValue"!==fn.type)throw new Error(`Cannot call something that is not a function: got ${fn.type}`);const newEnv=new Environment(environment);return newEnv.setVariable("caller",callerFn),fn.value(macroArgs,newEnv)}evaluateFilterStatement(node,environment){const rendered=this.evaluateBlock(node.body,environment);return this.applyFilter(rendered,node.filter,environment)}evaluate(statement,environment){if(!statement)return new UndefinedValue;switch(statement.type){case"Program":return this.evalProgram(statement,environment);case"Set":return this.evaluateSet(statement,environment);case"If":return this.evaluateIf(statement,environment);case"For":return this.evaluateFor(statement,environment);case"Macro":return this.evaluateMacro(statement,environment);case"CallStatement":return this.evaluateCallStatement(statement,environment);case"Break":throw new BreakControl;case"Continue":throw new ContinueControl;case"IntegerLiteral":return new IntegerValue(statement.value);case"FloatLiteral":return new FloatValue(statement.value);case"StringLiteral":return new StringValue(statement.value);case"ArrayLiteral":return new ArrayValue(statement.value.map(x=>this.evaluate(x,environment)));case"TupleLiteral":return new TupleValue(statement.value.map(x=>this.evaluate(x,environment)));case"ObjectLiteral":{const mapping=new Map;for(const[key,value]of statement.value){const evaluatedKey=this.evaluate(key,environment);if(!(evaluatedKey instanceof StringValue))throw new Error(`Object keys must be strings: got ${evaluatedKey.type}`);mapping.set(evaluatedKey.value,this.evaluate(value,environment))}return new ObjectValue(mapping)}case"Identifier":return this.evaluateIdentifier(statement,environment);case"CallExpression":return this.evaluateCallExpression(statement,environment);case"MemberExpression":return this.evaluateMemberExpression(statement,environment);case"UnaryExpression":return this.evaluateUnaryExpression(statement,environment);case"BinaryExpression":return this.evaluateBinaryExpression(statement,environment);case"FilterExpression":return this.evaluateFilterExpression(statement,environment);case"FilterStatement":return this.evaluateFilterStatement(statement,environment);case"TestExpression":return this.evaluateTestExpression(statement,environment);case"SelectExpression":return this.evaluateSelectExpression(statement,environment);case"Ternary":return this.evaluateTernaryExpression(statement,environment);case"Comment":return new NullValue;default:throw new SyntaxError(`Unknown node type: ${statement.type}`)}}};function convertToRuntimeValues(input){switch(typeof input){case"number":return Number.isInteger(input)?new IntegerValue(input):new FloatValue(input);case"string":return new StringValue(input);case"boolean":return new BooleanValue(input);case"undefined":return new UndefinedValue;case"object":return null===input?new NullValue:Array.isArray(input)?new ArrayValue(input.map(convertToRuntimeValues)):new ObjectValue(new Map(Object.entries(input).map(([key,value])=>[key,convertToRuntimeValues(value)])));case"function":return new FunctionValue((args,_scope)=>convertToRuntimeValues(input(...args.map(x=>x.value))??null));default:throw new Error(`Cannot convert to runtime value: ${input}`)}}function createStatement(...text){return"{%- "+text.join(" ")+" -%}"}function formatStatements(stmts,depth,indentStr){return stmts.map(stmt=>function(node,depth,indentStr){const pad=indentStr.repeat(depth);switch(node.type){case"Program":return formatStatements(node.body,depth,indentStr);case"If":return function(node,depth,indentStr){const pad=indentStr.repeat(depth),clauses=[];let current=node;for(;current&&(clauses.push({test:current.test,body:current.body}),1===current.alternate.length&&"If"===current.alternate[0].type);)current=current.alternate[0];let out=pad+createStatement("if",formatExpression(clauses[0].test))+"\n"+formatStatements(clauses[0].body,depth+1,indentStr);for(let i=1;i<clauses.length;++i)out+="\n"+pad+createStatement("elif",formatExpression(clauses[i].test))+"\n"+formatStatements(clauses[i].body,depth+1,indentStr);current&&current.alternate.length>0&&(out+="\n"+pad+createStatement("else")+"\n"+formatStatements(current.alternate,depth+1,indentStr));return out+="\n"+pad+createStatement("endif"),out}(node,depth,indentStr);case"For":return function(node,depth,indentStr){const pad=indentStr.repeat(depth);let formattedIterable="";if("SelectExpression"===node.iterable.type){const n=node.iterable;formattedIterable=`${formatExpression(n.lhs)} if ${formatExpression(n.test)}`}else formattedIterable=formatExpression(node.iterable);let out=pad+createStatement("for",formatExpression(node.loopvar),"in",formattedIterable)+"\n"+formatStatements(node.body,depth+1,indentStr);node.defaultBlock.length>0&&(out+="\n"+pad+createStatement("else")+"\n"+formatStatements(node.defaultBlock,depth+1,indentStr));return out+="\n"+pad+createStatement("endfor"),out}(node,depth,indentStr);case"Set":return function(node,depth,indentStr){const pad=indentStr.repeat(depth),left=formatExpression(node.assignee),right=node.value?formatExpression(node.value):"",value=pad+createStatement("set",`${left}${node.value?" = "+right:""}`);if(0===node.body.length)return value;return value+"\n"+formatStatements(node.body,depth+1,indentStr)+"\n"+pad+createStatement("endset")}(node,depth,indentStr);case"Macro":return function(node,depth,indentStr){const pad=indentStr.repeat(depth),args=node.args.map(formatExpression).join(", ");return pad+createStatement("macro",`${node.name.value}(${args})`)+"\n"+formatStatements(node.body,depth+1,indentStr)+"\n"+pad+createStatement("endmacro")}(node,depth,indentStr);case"Break":return pad+createStatement("break");case"Continue":return pad+createStatement("continue");case"CallStatement":return function(node,depth,indentStr){const pad=indentStr.repeat(depth),params=node.callerArgs&&node.callerArgs.length>0?`(${node.callerArgs.map(formatExpression).join(", ")})`:"",callExpr=formatExpression(node.call);let out=pad+createStatement(`call${params}`,callExpr)+"\n";return out+=formatStatements(node.body,depth+1,indentStr)+"\n",out+=pad+createStatement("endcall"),out}(node,depth,indentStr);case"FilterStatement":return function(node,depth,indentStr){const pad=indentStr.repeat(depth),spec="Identifier"===node.filter.type?node.filter.value:formatExpression(node.filter);let out=pad+createStatement("filter",spec)+"\n";return out+=formatStatements(node.body,depth+1,indentStr)+"\n",out+=pad+createStatement("endfilter"),out}(node,depth,indentStr);case"Comment":return pad+"{# "+node.value+" #}";default:return pad+"{{- "+formatExpression(node)+" -}}"}}(stmt,depth,indentStr)).join("\n")}function formatExpression(node,parentPrec=-1){switch(node.type){case"SpreadExpression":return`*${formatExpression(node.argument)}`;case"Identifier":return node.value;case"IntegerLiteral":case"FloatLiteral":return`${node.value}`;case"StringLiteral":return JSON.stringify(node.value);case"BinaryExpression":{const n=node,thisPrecedence=function(expr){switch(expr.operator.type){case"MultiplicativeBinaryOperator":return 4;case"AdditiveBinaryOperator":return 3;case"ComparisonBinaryOperator":return 2;case"Identifier":return"and"===expr.operator.value?1:"in"===expr.operator.value||"not in"===expr.operator.value?2:0}return 0}(n),left=formatExpression(n.left,thisPrecedence),right=formatExpression(n.right,thisPrecedence+1),expr=`${left} ${n.operator.value} ${right}`;return thisPrecedence<parentPrec?`(${expr})`:expr}case"UnaryExpression":{const n=node;return n.operator.value+("not"===n.operator.value?" ":"")+formatExpression(n.argument,1/0)}case"CallExpression":{const n=node,args=n.args.map(formatExpression).join(", ");return`${formatExpression(n.callee)}(${args})`}case"MemberExpression":{const n=node;let obj=formatExpression(n.object);["Identifier","MemberExpression","CallExpression","StringLiteral","IntegerLiteral","FloatLiteral","ArrayLiteral","TupleLiteral","ObjectLiteral"].includes(n.object.type)||(obj=`(${obj})`);let prop=formatExpression(n.property);return n.computed||"Identifier"===n.property.type||(prop=`(${prop})`),n.computed?`${obj}[${prop}]`:`${obj}.${prop}`}case"FilterExpression":{const n=node,operand=formatExpression(n.operand,1/0);return"CallExpression"===n.filter.type?`${operand} | ${formatExpression(n.filter)}`:`${operand} | ${n.filter.value}`}case"SelectExpression":{const n=node;return`${formatExpression(n.lhs)} if ${formatExpression(n.test)}`}case"TestExpression":{const n=node;return`${formatExpression(n.operand)} is${n.negate?" not":""} ${n.test.value}`}case"ArrayLiteral":case"TupleLiteral":{const elems=node.value.map(formatExpression),brackets="ArrayLiteral"===node.type?"[]":"()";return`${brackets[0]}${elems.join(", ")}${brackets[1]}`}case"ObjectLiteral":return`{${Array.from(node.value.entries()).map(([k,v])=>`${formatExpression(k)}: ${formatExpression(v)}`).join(", ")}}`;case"SliceExpression":{const n=node;return`${n.start?formatExpression(n.start):""}:${n.stop?formatExpression(n.stop):""}${n.step?`:${formatExpression(n.step)}`:""}`}case"KeywordArgumentExpression":{const n=node;return`${n.key.value}=${formatExpression(n.value)}`}case"Ternary":{const n=node,expr=`${formatExpression(n.trueExpr)} if ${formatExpression(n.condition,0)} else ${formatExpression(n.falseExpr)}`;return parentPrec>-1?`(${expr})`:expr}default:throw new Error(`Unknown expression type: ${node.type}`)}}var Template=class{constructor(template){__publicField(this,"parsed");const tokens=function(source,options={}){const tokens=[],src=function(template,options={}){return template.endsWith("\n")&&(template=template.slice(0,-1)),options.lstrip_blocks&&(template=template.replace(/^[ \t]*({[#%-])/gm,"$1")),options.trim_blocks&&(template=template.replace(/([#%-]})\n/g,"$1")),template.replace(/{%\s*(end)?generation\s*%}/gs,"")}(source,options);let cursorPosition=0,curlyBracketDepth=0;const consumeWhile=predicate=>{let str="";for(;predicate(src[cursorPosition]);){if("\\"===src[cursorPosition]){if(++cursorPosition,cursorPosition>=src.length)throw new SyntaxError("Unexpected end of input");const escaped=src[cursorPosition++],unescaped=ESCAPE_CHARACTERS.get(escaped);if(void 0===unescaped)throw new SyntaxError(`Unexpected escaped character: ${escaped}`);str+=unescaped;continue}if(str+=src[cursorPosition++],cursorPosition>=src.length)throw new SyntaxError("Unexpected end of input")}return str},stripTrailingWhitespace=()=>{const lastToken=tokens.at(-1);lastToken&&lastToken.type===TOKEN_TYPES.Text&&(lastToken.value=lastToken.value.trimEnd(),""===lastToken.value&&tokens.pop())},skipLeadingWhitespace=()=>{for(;cursorPosition<src.length&&isWhitespace(src[cursorPosition]);)++cursorPosition};main:for(;cursorPosition<src.length;){const lastTokenType=tokens.at(-1)?.type;if(void 0===lastTokenType||lastTokenType===TOKEN_TYPES.CloseStatement||lastTokenType===TOKEN_TYPES.CloseExpression||lastTokenType===TOKEN_TYPES.Comment){let text="";for(;cursorPosition<src.length&&("{"!==src[cursorPosition]||"%"!==src[cursorPosition+1]&&"{"!==src[cursorPosition+1]&&"#"!==src[cursorPosition+1]);)text+=src[cursorPosition++];if(text.length>0){tokens.push(new Token(text,TOKEN_TYPES.Text));continue}}if("{"===src[cursorPosition]&&"#"===src[cursorPosition+1]){cursorPosition+=2;const stripBefore="-"===src[cursorPosition];stripBefore&&++cursorPosition;let comment="";for(;"#"!==src[cursorPosition]||"}"!==src[cursorPosition+1];){if(cursorPosition+2>=src.length)throw new SyntaxError("Missing end of comment tag");comment+=src[cursorPosition++]}const stripAfter=comment.endsWith("-");stripAfter&&(comment=comment.slice(0,-1)),stripBefore&&stripTrailingWhitespace(),tokens.push(new Token(comment,TOKEN_TYPES.Comment)),cursorPosition+=2,stripAfter&&skipLeadingWhitespace();continue}if("{%-"===src.slice(cursorPosition,cursorPosition+3)){stripTrailingWhitespace(),tokens.push(new Token("{%",TOKEN_TYPES.OpenStatement)),cursorPosition+=3;continue}if("{{-"===src.slice(cursorPosition,cursorPosition+3)){stripTrailingWhitespace(),tokens.push(new Token("{{",TOKEN_TYPES.OpenExpression)),curlyBracketDepth=0,cursorPosition+=3;continue}if(consumeWhile(isWhitespace),"-%}"===src.slice(cursorPosition,cursorPosition+3)){tokens.push(new Token("%}",TOKEN_TYPES.CloseStatement)),cursorPosition+=3,skipLeadingWhitespace();continue}if("-}}"===src.slice(cursorPosition,cursorPosition+3)){tokens.push(new Token("}}",TOKEN_TYPES.CloseExpression)),cursorPosition+=3,skipLeadingWhitespace();continue}const char=src[cursorPosition];if("-"===char||"+"===char){const lastTokenType2=tokens.at(-1)?.type;if(lastTokenType2===TOKEN_TYPES.Text||void 0===lastTokenType2)throw new SyntaxError(`Unexpected character: ${char}`);switch(lastTokenType2){case TOKEN_TYPES.Identifier:case TOKEN_TYPES.NumericLiteral:case TOKEN_TYPES.StringLiteral:case TOKEN_TYPES.CloseParen:case TOKEN_TYPES.CloseSquareBracket:break;default:{++cursorPosition;const num=consumeWhile(isInteger);tokens.push(new Token(`${char}${num}`,num.length>0?TOKEN_TYPES.NumericLiteral:TOKEN_TYPES.UnaryOperator));continue}}}for(const[seq,type]of ORDERED_MAPPING_TABLE)if(!("}}"===seq&&curlyBracketDepth>0)&&src.slice(cursorPosition,cursorPosition+seq.length)===seq){tokens.push(new Token(seq,type)),type===TOKEN_TYPES.OpenExpression?curlyBracketDepth=0:type===TOKEN_TYPES.OpenCurlyBracket?++curlyBracketDepth:type===TOKEN_TYPES.CloseCurlyBracket&&--curlyBracketDepth,cursorPosition+=seq.length;continue main}if("'"===char||'"'===char){++cursorPosition;const str=consumeWhile(c=>c!==char);tokens.push(new Token(str,TOKEN_TYPES.StringLiteral)),++cursorPosition;continue}if(isInteger(char)){let num=consumeWhile(isInteger);"."===src[cursorPosition]&&isInteger(src[cursorPosition+1])&&(++cursorPosition,num=`${num}.${consumeWhile(isInteger)}`),tokens.push(new Token(num,TOKEN_TYPES.NumericLiteral));continue}if(isWord(char)){const word=consumeWhile(isWord);tokens.push(new Token(word,TOKEN_TYPES.Identifier));continue}throw new SyntaxError(`Unexpected character: ${char}`)}return tokens}(template,{lstrip_blocks:!0,trim_blocks:!0});this.parsed=parse(tokens)}render(items){const env=new Environment;if(function(env){env.set("false",!1),env.set("true",!0),env.set("none",null),env.set("raise_exception",args=>{throw new Error(args)}),env.set("range",range),env.set("strftime_now",strftime_now),env.set("True",!0),env.set("False",!1),env.set("None",null)}(env),items)for(const[key,value]of Object.entries(items))env.set(key,value);return new Interpreter(env).run(this.parsed).value}format(options){return function(program,indent="\t"){const indentStr="number"==typeof indent?" ".repeat(indent):indent;return formatStatements(program.body,0,indentStr).replace(/\n$/,"")}(this.parsed,options?.indent||"\t")}},LIBRARY_TASK_MAPPING={transformers:["audio-classification","automatic-speech-recognition","depth-estimation","document-question-answering","feature-extraction","fill-mask","image-classification","image-feature-extraction","image-segmentation","image-to-image","image-to-text","image-text-to-text","mask-generation","object-detection","question-answering","summarization","table-question-answering","text-classification","text-generation","text-to-audio","text-to-speech","token-classification","translation","video-classification","visual-question-answering","zero-shot-classification","zero-shot-image-classification","zero-shot-object-detection"]},PIPELINE_DATA={"text-classification":{name:"Text Classification",subtasks:[{type:"acceptability-classification",name:"Acceptability Classification"},{type:"entity-linking-classification",name:"Entity Linking Classification"},{type:"fact-checking",name:"Fact Checking"},{type:"intent-classification",name:"Intent Classification"},{type:"language-identification",name:"Language Identification"},{type:"multi-class-classification",name:"Multi Class Classification"},{type:"multi-label-classification",name:"Multi Label Classification"},{type:"multi-input-text-classification",name:"Multi-input Text Classification"},{type:"natural-language-inference",name:"Natural Language Inference"},{type:"semantic-similarity-classification",name:"Semantic Similarity Classification"},{type:"sentiment-classification",name:"Sentiment Classification"},{type:"topic-classification",name:"Topic Classification"},{type:"semantic-similarity-scoring",name:"Semantic Similarity Scoring"},{type:"sentiment-scoring",name:"Sentiment Scoring"},{type:"sentiment-analysis",name:"Sentiment Analysis"},{type:"hate-speech-detection",name:"Hate Speech Detection"},{type:"text-scoring",name:"Text Scoring"}],modality:"nlp"},"token-classification":{name:"Token Classification",subtasks:[{type:"named-entity-recognition",name:"Named Entity Recognition"},{type:"part-of-speech",name:"Part of Speech"},{type:"parsing",name:"Parsing"},{type:"lemmatization",name:"Lemmatization"},{type:"word-sense-disambiguation",name:"Word Sense Disambiguation"},{type:"coreference-resolution",name:"Coreference-resolution"}],modality:"nlp"},"table-question-answering":{name:"Table Question Answering",modality:"nlp"},"question-answering":{name:"Question Answering",subtasks:[{type:"extractive-qa",name:"Extractive QA"},{type:"open-domain-qa",name:"Open Domain QA"},{type:"closed-domain-qa",name:"Closed Domain QA"}],modality:"nlp"},"zero-shot-classification":{name:"Zero-Shot Classification",modality:"nlp"},translation:{name:"Translation",modality:"nlp"},summarization:{name:"Summarization",subtasks:[{type:"news-articles-summarization",name:"News Articles Summarization"},{type:"news-articles-headline-generation",name:"News Articles Headline Generation"}],modality:"nlp"},"feature-extraction":{name:"Feature Extraction",modality:"nlp"},"text-generation":{name:"Text Generation",subtasks:[{type:"dialogue-modeling",name:"Dialogue Modeling"},{type:"dialogue-generation",name:"Dialogue Generation"},{type:"conversational",name:"Conversational"},{type:"language-modeling",name:"Language Modeling"},{type:"text-simplification",name:"Text simplification"},{type:"explanation-generation",name:"Explanation Generation"},{type:"abstractive-qa",name:"Abstractive QA"},{type:"open-domain-abstractive-qa",name:"Open Domain Abstractive QA"},{type:"closed-domain-qa",name:"Closed Domain QA"},{type:"open-book-qa",name:"Open Book QA"},{type:"closed-book-qa",name:"Closed Book QA"},{type:"text2text-generation",name:"Text2Text Generation"}],modality:"nlp"},"fill-mask":{name:"Fill-Mask",subtasks:[{type:"slot-filling",name:"Slot Filling"},{type:"masked-language-modeling",name:"Masked Language Modeling"}],modality:"nlp"},"sentence-similarity":{name:"Sentence Similarity",modality:"nlp"},"text-to-speech":{name:"Text-to-Speech",modality:"audio"},"text-to-audio":{name:"Text-to-Audio",modality:"audio"},"automatic-speech-recognition":{name:"Automatic Speech Recognition",modality:"audio"},"audio-to-audio":{name:"Audio-to-Audio",modality:"audio"},"audio-classification":{name:"Audio Classification",subtasks:[{type:"keyword-spotting",name:"Keyword Spotting"},{type:"speaker-identification",name:"Speaker Identification"},{type:"audio-intent-classification",name:"Audio Intent Classification"},{type:"audio-emotion-recognition",name:"Audio Emotion Recognition"},{type:"audio-language-identification",name:"Audio Language Identification"}],modality:"audio"},"audio-text-to-text":{name:"Audio-Text-to-Text",modality:"multimodal",hideInDatasets:!0},"voice-activity-detection":{name:"Voice Activity Detection",modality:"audio"},"depth-estimation":{name:"Depth Estimation",modality:"cv"},"image-classification":{name:"Image Classification",subtasks:[{type:"multi-label-image-classification",name:"Multi Label Image Classification"},{type:"multi-class-image-classification",name:"Multi Class Image Classification"}],modality:"cv"},"object-detection":{name:"Object Detection",subtasks:[{type:"face-detection",name:"Face Detection"},{type:"vehicle-detection",name:"Vehicle Detection"}],modality:"cv"},"image-segmentation":{name:"Image Segmentation",subtasks:[{type:"instance-segmentation",name:"Instance Segmentation"},{type:"semantic-segmentation",name:"Semantic Segmentation"},{type:"panoptic-segmentation",name:"Panoptic Segmentation"}],modality:"cv"},"text-to-image":{name:"Text-to-Image",modality:"cv"},"image-to-text":{name:"Image-to-Text",subtasks:[{type:"image-captioning",name:"Image Captioning"}],modality:"cv"},"image-to-image":{name:"Image-to-Image",subtasks:[{type:"image-inpainting",name:"Image Inpainting"},{type:"image-colorization",name:"Image Colorization"},{type:"super-resolution",name:"Super Resolution"}],modality:"cv"},"image-to-video":{name:"Image-to-Video",modality:"cv"},"unconditional-image-generation":{name:"Unconditional Image Generation",modality:"cv"},"video-classification":{name:"Video Classification",modality:"cv"},"reinforcement-learning":{name:"Reinforcement Learning",modality:"rl"},robotics:{name:"Robotics",modality:"rl",subtasks:[{type:"grasping",name:"Grasping"},{type:"task-planning",name:"Task Planning"}]},"tabular-classification":{name:"Tabular Classification",modality:"tabular",subtasks:[{type:"tabular-multi-class-classification",name:"Tabular Multi Class Classification"},{type:"tabular-multi-label-classification",name:"Tabular Multi Label Classification"}]},"tabular-regression":{name:"Tabular Regression",modality:"tabular",subtasks:[{type:"tabular-single-column-regression",name:"Tabular Single Column Regression"}]},"tabular-to-text":{name:"Tabular to Text",modality:"tabular",subtasks:[{type:"rdf-to-text",name:"RDF to text"}],hideInModels:!0},"table-to-text":{name:"Table to Text",modality:"nlp",hideInModels:!0},"multiple-choice":{name:"Multiple Choice",subtasks:[{type:"multiple-choice-qa",name:"Multiple Choice QA"},{type:"multiple-choice-coreference-resolution",name:"Multiple Choice Coreference Resolution"}],modality:"nlp",hideInModels:!0},"text-ranking":{name:"Text Ranking",modality:"nlp"},"text-retrieval":{name:"Text Retrieval",subtasks:[{type:"document-retrieval",name:"Document Retrieval"},{type:"utterance-retrieval",name:"Utterance Retrieval"},{type:"entity-linking-retrieval",name:"Entity Linking Retrieval"},{type:"fact-checking-retrieval",name:"Fact Checking Retrieval"}],modality:"nlp",hideInModels:!0},"time-series-forecasting":{name:"Time Series Forecasting",modality:"tabular",subtasks:[{type:"univariate-time-series-forecasting",name:"Univariate Time Series Forecasting"},{type:"multivariate-time-series-forecasting",name:"Multivariate Time Series Forecasting"}]},"text-to-video":{name:"Text-to-Video",modality:"cv"},"image-text-to-text":{name:"Image-Text-to-Text",modality:"multimodal"},"visual-question-answering":{name:"Visual Question Answering",subtasks:[{type:"visual-question-answering",name:"Visual Question Answering"}],modality:"multimodal"},"document-question-answering":{name:"Document Question Answering",subtasks:[{type:"document-question-answering",name:"Document Question Answering"}],modality:"multimodal",hideInDatasets:!0},"zero-shot-image-classification":{name:"Zero-Shot Image Classification",modality:"cv"},"graph-ml":{name:"Graph Machine Learning",modality:"other"},"mask-generation":{name:"Mask Generation",modality:"cv"},"zero-shot-object-detection":{name:"Zero-Shot Object Detection",modality:"cv"},"text-to-3d":{name:"Text-to-3D",modality:"cv"},"image-to-3d":{name:"Image-to-3D",modality:"cv"},"image-feature-extraction":{name:"Image Feature Extraction",modality:"cv"},"video-text-to-text":{name:"Video-Text-to-Text",modality:"multimodal",hideInDatasets:!1},"keypoint-detection":{name:"Keypoint Detection",subtasks:[{type:"pose-estimation",name:"Pose Estimation"}],modality:"cv",hideInDatasets:!0},"visual-document-retrieval":{name:"Visual Document Retrieval",modality:"multimodal"},"any-to-any":{name:"Any-to-Any",modality:"multimodal"},"video-to-video":{name:"Video-to-Video",modality:"cv",hideInDatasets:!0},other:{name:"Other",modality:"other",hideInModels:!0,hideInDatasets:!0}},PIPELINE_TYPES=Object.keys(PIPELINE_DATA);Object.values(PIPELINE_DATA).flatMap(data=>"subtasks"in data?data.subtasks:[]).map(s=>s.type),new Set(PIPELINE_TYPES);var data_default2={datasets:[{description:"A benchmark of 10 different audio tasks.",id:"s3prl/superb"},{description:"A dataset of YouTube clips and their sound categories.",id:"agkphysics/AudioSet"}],demo:{inputs:[{filename:"audio.wav",type:"audio"}],outputs:[{data:[{label:"Up",score:.2},{label:"Down",score:.8}],type:"chart"}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"An easy-to-use model for command recognition.",id:"speechbrain/google_speech_command_xvector"},{description:"An emotion recognition model.",id:"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"},{description:"A language identification model.",id:"facebook/mms-lid-126"}],spaces:[{description:"An application that can classify music into different genre.",id:"kurianbenoy/audioclassification"}],summary:"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.",widgetModels:["MIT/ast-finetuned-audioset-10-10-0.4593"],youtubeId:"KWwzcmG98Ds"},data_default3={datasets:[{description:"A dataset containing audio conversations with question‚Äìanswer pairs.",id:"nvidia/AF-Think"},{description:"A more advanced and comprehensive dataset that contains characteristics of the audio as well",id:"tsinghua-ee/QualiSpeech"}],demo:{inputs:[{filename:"audio.wav",type:"audio"},{label:"Text Prompt",content:"What is the gender of the speaker?",type:"text"}],outputs:[{label:"Generated Text",content:"The gender of the speaker is female.",type:"text"}]},metrics:[],models:[{description:"A lightweight model that has capabilities of taking both audio and text as inputs and generating responses.",id:"fixie-ai/ultravox-v0_5-llama-3_2-1b"},{description:"A multimodal model that supports voice chat and audio analysis.",id:"Qwen/Qwen2-Audio-7B-Instruct"},{description:"A model for audio understanding, speech translation, and transcription.",id:"mistralai/Voxtral-Small-24B-2507"},{description:"A new model capable of audio question answering and reasoning.",id:"nvidia/audio-flamingo-3"}],spaces:[{description:"A space that takes input as both audio and text and generates answers.",id:"iamomtiwari/ATTT"},{description:"A web application that demonstrates chatting with the Qwen2Audio Model.",id:"freddyaboulton/talk-to-qwen-webrtc"}],summary:"Audio-text-to-text models take both an audio clip and a text prompt as input, and generate natural language text as output. These models can answer questions about spoken content, summarize meetings, analyze music, or interpret speech beyond simple transcription. They are useful for applications that combine speech understanding with reasoning or conversation.",widgetModels:[],youtubeId:""},data_default4={datasets:[{description:"512-element X-vector embeddings of speakers from CMU ARCTIC dataset.",id:"Matthijs/cmu-arctic-xvectors"}],demo:{inputs:[{filename:"input.wav",type:"audio"}],outputs:[{filename:"label-0.wav",type:"audio"},{filename:"label-1.wav",type:"audio"}]},metrics:[{description:"The Signal-to-Noise ratio is the relationship between the target signal level and the background noise level. It is calculated as the logarithm of the target signal divided by the background noise, in decibels.",id:"snri"},{description:"The Signal-to-Distortion ratio is the relationship between the target signal and the sum of noise, interference, and artifact errors",id:"sdri"}],models:[{description:"A speech enhancement model.",id:"ResembleAI/resemble-enhance"},{description:"A model that can change the voice in a speech recording.",id:"microsoft/speecht5_vc"}],spaces:[{description:"An application for speech separation.",id:"younver/speechbrain-speech-separation"},{description:"An application for audio style transfer.",id:"nakas/audio-diffusion_style_transfer"}],summary:"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.",widgetModels:["speechbrain/sepformer-wham"],youtubeId:"iohj7nCCYoM"},data_default5={datasets:[{description:"31,175 hours of multilingual audio-text dataset in 108 languages.",id:"mozilla-foundation/common_voice_17_0"},{description:"Multilingual and diverse audio dataset with 101k hours of audio.",id:"amphion/Emilia-Dataset"},{description:"A dataset with 44.6k hours of English speaker data and 6k hours of other language speakers.",id:"parler-tts/mls_eng"},{description:"A multilingual audio dataset with 370K hours of audio.",id:"espnet/yodas"}],demo:{inputs:[{filename:"input.flac",type:"audio"}],outputs:[{label:"Transcript",content:"Going along slushy country roads and speaking to damp audiences in...",type:"text"}]},metrics:[{description:"",id:"wer"},{description:"",id:"cer"}],models:[{description:"A powerful ASR model by OpenAI.",id:"openai/whisper-large-v3"},{description:"A good generic speech model by MetaAI for fine-tuning.",id:"facebook/w2v-bert-2.0"},{description:"An end-to-end model that performs ASR and Speech Translation by MetaAI.",id:"facebook/seamless-m4t-v2-large"},{description:"A powerful multilingual ASR and Speech Translation model by Nvidia.",id:"nvidia/canary-1b"},{description:"Powerful speaker diarization model.",id:"pyannote/speaker-diarization-3.1"}],spaces:[{description:"A powerful general-purpose speech recognition application.",id:"hf-audio/whisper-large-v3"},{description:"Latest ASR model from Useful Sensors.",id:"mrfakename/Moonshinex"},{description:"A high quality speech and text translation model by Meta.",id:"facebook/seamless_m4t"},{description:"A powerful multilingual ASR and Speech Translation model by Nvidia",id:"nvidia/canary-1b"}],summary:"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.",widgetModels:["openai/whisper-large-v3"],youtubeId:"TksaY_FDgnk"},data_default6={datasets:[{description:"Largest document understanding dataset.",id:"HuggingFaceM4/Docmatix"},{description:"Dataset from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry Documents Library.",id:"eliolio/docvqa"}],demo:{inputs:[{label:"Question",content:"What is the idea behind the consumer relations efficiency team?",type:"text"},{filename:"document-question-answering-input.png",type:"img"}],outputs:[{label:"Answer",content:"Balance cost efficiency with quality customer service",type:"text"}]},metrics:[{description:"The evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein Similarity (ANLS). This metric is flexible to character regognition errors and compares the predicted answer with the ground truth answer.",id:"anls"},{description:"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",id:"exact-match"}],models:[{description:"A robust document question answering model.",id:"impira/layoutlm-document-qa"},{description:"A document question answering model specialized in invoices.",id:"impira/layoutlm-invoices"},{description:"A special model for OCR-free document question answering.",id:"microsoft/udop-large"},{description:"A powerful model for document question answering.",id:"google/pix2struct-docvqa-large"}],spaces:[{description:"A robust document question answering application.",id:"impira/docquery"},{description:"An application that can answer questions from invoices.",id:"impira/invoices"},{description:"An application to compare different document question answering models.",id:"merve/compare_docvqa_models"}],summary:"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.",widgetModels:["impira/layoutlm-invoices"],youtubeId:""},data_default7={datasets:[{description:"Wikipedia dataset containing cleaned articles of all languages. Can be used to train `feature-extraction` models.",id:"wikipedia"}],demo:{inputs:[{label:"Input",content:"India, officially the Republic of India, is a country in South Asia.",type:"text"}],outputs:[{table:[["Dimension 1","Dimension 2","Dimension 3"],["2.583383083343506","2.757075071334839","0.9023529887199402"],["8.29393482208252","1.1071064472198486","2.03399395942688"],["-0.7754912972450256","-1.647324562072754","-0.6113331913948059"],["0.07087723910808563","1.5942802429199219","1.4610432386398315"]],type:"tabular"}]},metrics:[],models:[{description:"A powerful feature extraction model for natural language processing tasks.",id:"thenlper/gte-large"},{description:"A strong feature extraction model for retrieval.",id:"Alibaba-NLP/gte-Qwen1.5-7B-instruct"}],spaces:[{description:"A leaderboard to rank text feature extraction models based on a benchmark.",id:"mteb/leaderboard"},{description:"A leaderboard to rank best feature extraction models based on human feedback.",id:"mteb/arena"}],summary:"Feature extraction is the task of extracting features learnt in a model.",widgetModels:["facebook/bart-base"]},data_default8={datasets:[{description:"A common dataset that is used to train models for many languages.",id:"wikipedia"},{description:"A large English dataset with text crawled from the web.",id:"c4"}],demo:{inputs:[{label:"Input",content:"The <mask> barked at me",type:"text"}],outputs:[{type:"chart",data:[{label:"wolf",score:.487},{label:"dog",score:.061},{label:"cat",score:.058},{label:"fox",score:.047},{label:"squirrel",score:.025}]}]},metrics:[{description:"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",id:"cross_entropy"},{description:"Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",id:"perplexity"}],models:[{description:"State-of-the-art masked language model.",id:"answerdotai/ModernBERT-large"},{description:"A multilingual model trained on 100 languages.",id:"FacebookAI/xlm-roberta-base"}],spaces:[],summary:"Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.",widgetModels:["distilroberta-base"],youtubeId:"mqElG5QJWUg"},data_default9={datasets:[{description:"Benchmark dataset used for image classification with images that belong to 100 classes.",id:"cifar100"},{description:"Dataset consisting of images of garments.",id:"fashion_mnist"}],demo:{inputs:[{filename:"image-classification-input.jpeg",type:"img"}],outputs:[{type:"chart",data:[{label:"Egyptian cat",score:.514},{label:"Tabby cat",score:.193},{label:"Tiger cat",score:.068}]}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"A strong image classification model.",id:"google/vit-base-patch16-224"},{description:"A robust image classification model.",id:"facebook/deit-base-distilled-patch16-224"},{description:"A strong image classification model.",id:"facebook/convnext-large-224"}],spaces:[{description:"A leaderboard to evaluate different image classification models.",id:"timm/leaderboard"}],summary:"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.",widgetModels:["google/vit-base-patch16-224"],youtubeId:"tjAIM7BOYhw"},data_default10={datasets:[{description:"ImageNet-1K is a image classification dataset in which images are used to train image-feature-extraction models.",id:"imagenet-1k"}],demo:{inputs:[{filename:"mask-generation-input.png",type:"img"}],outputs:[{table:[["Dimension 1","Dimension 2","Dimension 3"],["0.21236686408519745","1.0919708013534546","0.8512550592422485"],["0.809657871723175","-0.18544459342956543","-0.7851548194885254"],["1.3103108406066895","-0.2479034662246704","-0.9107287526130676"],["1.8536205291748047","-0.36419737339019775","0.09717650711536407"]],type:"tabular"}]},metrics:[],models:[{description:"A powerful image feature extraction model.",id:"timm/vit_large_patch14_dinov2.lvd142m"},{description:"A strong image feature extraction model.",id:"nvidia/MambaVision-T-1K"},{description:"A robust image feature extraction model.",id:"facebook/dino-vitb16"},{description:"Cutting-edge image feature extraction model.",id:"apple/aimv2-large-patch14-336-distilled"},{description:"Strong image feature extraction model that can be used on images and documents.",id:"OpenGVLab/InternViT-6B-448px-V1-2"}],spaces:[{description:"A leaderboard to evaluate different image-feature-extraction models on classification performances",id:"timm/leaderboard"}],summary:"Image feature extraction is the task of extracting features learnt in a computer vision model.",widgetModels:[]},data_default11={datasets:[{description:"Synthetic dataset, for image relighting",id:"VIDIT"},{description:"Multiple images of celebrities, used for facial expression translation",id:"huggan/CelebA-faces"},{description:"12M image-caption pairs.",id:"Spawning/PD12M"}],demo:{inputs:[{filename:"image-to-image-input.jpeg",type:"img"}],outputs:[{filename:"image-to-image-output.png",type:"img"}]},isPlaceholder:!1,metrics:[{description:"Peak Signal to Noise Ratio (PSNR) is an approximation of the human perception, considering the ratio of the absolute intensity with respect to the variations. Measured in dB, a high value indicates a high fidelity.",id:"PSNR"},{description:"Structural Similarity Index (SSIM) is a perceptual metric which compares the luminance, contrast and structure of two images. The values of SSIM range between -1 and 1, and higher values indicate closer resemblance to the original image.",id:"SSIM"},{description:"Inception Score (IS) is an analysis of the labels predicted by an image classification model when presented with a sample of the generated images.",id:"IS"}],models:[{description:"An image-to-image model to improve image resolution.",id:"fal/AuraSR-v2"},{description:"Powerful image editing model.",id:"black-forest-labs/FLUX.1-Kontext-dev"},{description:"Virtual try-on model.",id:"yisol/IDM-VTON"},{description:"Image re-lighting model.",id:"kontext-community/relighting-kontext-dev-lora-v3"},{description:"Strong model for inpainting and outpainting.",id:"black-forest-labs/FLUX.1-Fill-dev"},{description:"Strong model for image editing using depth maps.",id:"black-forest-labs/FLUX.1-Depth-dev-lora"}],spaces:[{description:"Image editing application.",id:"black-forest-labs/FLUX.1-Kontext-Dev"},{description:"Image relighting application.",id:"lllyasviel/iclight-v2-vary"},{description:"An application for image upscaling.",id:"jasperai/Flux.1-dev-Controlnet-Upscaler"}],summary:"Image-to-image is the task of transforming an input image through a variety of possible manipulations and enhancements, such as super-resolution, image inpainting, colorization, and more.",widgetModels:["Qwen/Qwen-Image"],youtubeId:""},data_default12={datasets:[{description:"Dataset from 12M image-text of Reddit",id:"red_caps"},{description:"Dataset from 3.3M images of Google",id:"datasets/conceptual_captions"}],demo:{inputs:[{filename:"savanna.jpg",type:"img"}],outputs:[{label:"Detailed description",content:"a herd of giraffes and zebras grazing in a field",type:"text"}]},metrics:[],models:[{description:"Strong OCR model.",id:"allenai/olmOCR-7B-0725"},{description:"Powerful image captioning model.",id:"fancyfeast/llama-joycaption-beta-one-hf-llava"}],spaces:[{description:"SVG generator app from images.",id:"multimodalart/OmniSVG-3B"},{description:"An application that converts documents to markdown.",id:"numind/NuMarkdown-8B-Thinking"},{description:"An application that can caption images.",id:"fancyfeast/joy-caption-beta-one"}],summary:"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.",widgetModels:["Salesforce/blip-image-captioning-large"],youtubeId:""},data_default13={datasets:[{description:"Instructions composed of image and text.",id:"liuhaotian/LLaVA-Instruct-150K"},{description:"Collection of image-text pairs on scientific topics.",id:"DAMO-NLP-SG/multimodal_textbook"},{description:"A collection of datasets made for model fine-tuning.",id:"HuggingFaceM4/the_cauldron"},{description:"Screenshots of websites with their HTML/CSS codes.",id:"HuggingFaceM4/WebSight"}],demo:{inputs:[{filename:"image-text-to-text-input.png",type:"img"},{label:"Text Prompt",content:"Describe the position of the bee in detail.",type:"text"}],outputs:[{label:"Answer",content:"The bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned in the center of the flower, with its head and front legs sticking out.",type:"text"}]},metrics:[],models:[{description:"Small and efficient yet powerful vision language model.",id:"HuggingFaceTB/SmolVLM-Instruct"},{description:"Cutting-edge reasoning vision language model.",id:"zai-org/GLM-4.5V"},{description:"Cutting-edge small vision language model to convert documents to text.",id:"rednote-hilab/dots.ocr"},{description:"Small yet powerful model.",id:"Qwen/Qwen2.5-VL-3B-Instruct"},{description:"Image-text-to-text model with agentic capabilities.",id:"microsoft/Magma-8B"}],spaces:[{description:"Leaderboard to evaluate vision language models.",id:"opencompass/open_vlm_leaderboard"},{description:"An application that compares object detection capabilities of different vision language models.",id:"sergiopaniego/vlm_object_understanding"},{description:"An application to compare different OCR models.",id:"prithivMLmods/Multimodal-OCR"}],summary:"Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input.",widgetModels:["zai-org/GLM-4.5V"],youtubeId:"IoGaGfU1CIg"},data_default14={datasets:[{description:"Scene segmentation dataset.",id:"scene_parse_150"}],demo:{inputs:[{filename:"image-segmentation-input.jpeg",type:"img"}],outputs:[{filename:"image-segmentation-output.png",type:"img"}]},metrics:[{description:"Average Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for each semantic class separately",id:"Average Precision"},{description:"Mean Average Precision (mAP) is the overall average of the AP values",id:"Mean Average Precision"},{description:"Intersection over Union (IoU) is the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes",id:"Mean Intersection over Union"},{description:"APŒ± is the Average Precision at the IoU threshold of a Œ± value, for example, AP50 and AP75",id:"APŒ±"}],models:[{description:"Solid panoptic segmentation model trained on COCO.",id:"tue-mps/coco_panoptic_eomt_large_640"},{description:"Background removal model.",id:"briaai/RMBG-1.4"},{description:"A multipurpose image segmentation model for high resolution images.",id:"ZhengPeng7/BiRefNet"},{description:"Powerful human-centric image segmentation model.",id:"facebook/sapiens-seg-1b"},{description:"Panoptic segmentation model trained on the COCO (common objects) dataset.",id:"facebook/mask2former-swin-large-coco-panoptic"}],spaces:[{description:"A semantic segmentation application that can predict unseen instances out of the box.",id:"facebook/ov-seg"},{description:"One of the strongest segmentation applications.",id:"jbrinkma/segment-anything"},{description:"A human-centric segmentation model.",id:"facebook/sapiens-pose"},{description:"An instance segmentation application to predict neuronal cell types from microscopy images.",id:"rashmi/sartorius-cell-instance-segmentation"},{description:"An application that segments videos.",id:"ArtGAN/Segment-Anything-Video"},{description:"An panoptic segmentation application built for outdoor environments.",id:"segments/panoptic-segment-anything"}],summary:"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.",widgetModels:["nvidia/segformer-b0-finetuned-ade-512-512"],youtubeId:"dKE8SIt9C-w"},data_default15={datasets:[{description:"A benchmark dataset for reference image controlled video generation.",id:"ali-vilab/VACE-Benchmark"},{description:"A dataset of video generation style preferences.",id:"Rapidata/sora-video-generation-style-likert-scoring"},{description:"A dataset with videos and captions throughout the videos.",id:"BestWishYsh/ChronoMagic"}],demo:{inputs:[{filename:"image-to-video-input.jpg",type:"img"},{label:"Optional Text Prompt",content:"This penguin is dancing",type:"text"}],outputs:[{filename:"image-to-video-output.gif",type:"img"}]},metrics:[{description:"Fr√©chet Video Distance (FVD) measures the perceptual similarity between the distributions of generated videos and a set of real videos, assessing overall visual quality and temporal coherence of the video generated from an input image.",id:"fvd"},{description:"CLIP Score measures the semantic similarity between a textual prompt (if provided alongside the input image) and the generated video frames. It evaluates how well the video's generated content and motion align with the textual description, conditioned on the initial image.",id:"clip_score"},{description:"First Frame Fidelity, often measured using LPIPS (Learned Perceptual Image Patch Similarity), PSNR, or SSIM, quantifies how closely the first frame of the generated video matches the input conditioning image.",id:"lpips"},{description:"Identity Preservation Score measures the consistency of identity (e.g., a person's face or a specific object's characteristics) between the input image and throughout the generated video frames, often calculated using features from specialized models like face recognition (e.g., ArcFace) or re-identification models.",id:"identity_preservation"},{description:"Motion Score evaluates the quality, realism, and temporal consistency of motion in the video generated from a static image. This can be based on optical flow analysis (e.g., smoothness, magnitude), consistency of object trajectories, or specific motion plausibility assessments.",id:"motion_score"}],models:[{description:"LTX-Video, a 13B parameter model for high quality video generation",id:"Lightricks/LTX-Video-0.9.7-dev"},{description:"A 14B parameter model for reference image controlled video generation",id:"Wan-AI/Wan2.1-VACE-14B"},{description:"An image-to-video generation model using FramePack F1 methodology with Hunyuan-DiT architecture",id:"lllyasviel/FramePack_F1_I2V_HY_20250503"},{description:"A distilled version of the LTX-Video-0.9.7-dev model for faster inference",id:"Lightricks/LTX-Video-0.9.7-distilled"},{description:"An image-to-video generation model by Skywork AI, 14B parameters, producing 720p videos.",id:"Skywork/SkyReels-V2-I2V-14B-720P"},{description:"Image-to-video variant of Tencent's HunyuanVideo.",id:"tencent/HunyuanVideo-I2V"},{description:"A 14B parameter model for 720p image-to-video generation by Wan-AI.",id:"Wan-AI/Wan2.1-I2V-14B-720P"},{description:"A Diffusers version of the Wan2.1-I2V-14B-720P model for 720p image-to-video generation.",id:"Wan-AI/Wan2.1-I2V-14B-720P-Diffusers"}],spaces:[{description:"An application to generate videos fast.",id:"Lightricks/ltx-video-distilled"},{description:"Generate videos with the FramePack-F1",id:"linoyts/FramePack-F1"},{description:"Generate videos with the FramePack",id:"lisonallen/framepack-i2v"},{description:"Wan2.1 with CausVid LoRA",id:"multimodalart/wan2-1-fast"},{description:"A demo for Stable Video Diffusion",id:"multimodalart/stable-video-diffusion"}],summary:"Image-to-video models take a still image as input and generate a video. These models can be guided by text prompts to influence the content and style of the output video.",widgetModels:[],youtubeId:void 0},data_default16={datasets:[{description:"Widely used benchmark dataset for multiple Vision tasks.",id:"merve/coco2017"},{description:"Medical Imaging dataset of the Human Brain for segmentation and mask generating tasks",id:"rocky93/BraTS_segmentation"}],demo:{inputs:[{filename:"mask-generation-input.png",type:"img"}],outputs:[{filename:"mask-generation-output.png",type:"img"}]},metrics:[{description:"IoU is used to measure the overlap between predicted mask and the ground truth mask.",id:"Intersection over Union (IoU)"}],models:[{description:"Small yet powerful mask generation model.",id:"Zigeng/SlimSAM-uniform-50"},{description:"Very strong mask generation model.",id:"facebook/sam2-hiera-large"}],spaces:[{description:"An application that combines a mask generation model with a zero-shot object detection model for text-guided image segmentation.",id:"merve/OWLSAM2"},{description:"An application that compares the performance of a large and a small mask generation model.",id:"merve/slimsam"},{description:"An application based on an improved mask generation model.",id:"SkalskiP/segment-anything-model-2"},{description:"An application to remove objects from videos using mask generation models.",id:"SkalskiP/SAM_and_ProPainter"}],summary:"Mask generation is the task of generating masks that identify a specific object or region of interest in a given image. Masks are often used in segmentation tasks, where they provide a precise way to isolate the object of interest for further processing or analysis.",widgetModels:[],youtubeId:""},data_default17={datasets:[{description:"Widely used benchmark dataset for multiple vision tasks.",id:"merve/coco2017"},{description:"Multi-task computer vision benchmark.",id:"merve/pascal-voc"}],demo:{inputs:[{filename:"object-detection-input.jpg",type:"img"}],outputs:[{filename:"object-detection-output.jpg",type:"img"}]},metrics:[{description:"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",id:"Average Precision"},{description:"The Mean Average Precision (mAP) metric is the overall average of the AP values",id:"Mean Average Precision"},{description:"The APŒ± metric is the Average Precision at the IoU threshold of a Œ± value, for example, AP50 and AP75",id:"APŒ±"}],models:[{description:"Solid object detection model pre-trained on the COCO 2017 dataset.",id:"facebook/detr-resnet-50"},{description:"Accurate object detection model.",id:"IDEA-Research/dab-detr-resnet-50"},{description:"Fast and accurate object detection model.",id:"PekingU/rtdetr_v2_r50vd"},{description:"Object detection model for low-lying objects.",id:"StephanST/WALDO30"}],spaces:[{description:"Real-time object detection demo.",id:"Roboflow/RF-DETR"},{description:"An application that contains various object detection models to try from.",id:"Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS"},{description:"A cutting-edge object detection application.",id:"sunsmarterjieleaf/yolov12"},{description:"An object tracking, segmentation and inpainting application.",id:"VIPLab/Track-Anything"},{description:"Very fast object tracking application based on object detection.",id:"merve/RT-DETR-tracking-coco"}],summary:"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.",widgetModels:["facebook/detr-resnet-50"],youtubeId:"WdAeKSOpxhw"},data_default18={datasets:[{description:"NYU Depth V2 Dataset: Video dataset containing both RGB and depth sensor data.",id:"sayakpaul/nyu_depth_v2"},{description:"Monocular depth estimation benchmark based without noise and errors.",id:"depth-anything/DA-2K"}],demo:{inputs:[{filename:"depth-estimation-input.jpg",type:"img"}],outputs:[{filename:"depth-estimation-output.png",type:"img"}]},metrics:[],models:[{description:"Cutting-edge depth estimation model.",id:"depth-anything/Depth-Anything-V2-Large"},{description:"A strong monocular depth estimation model.",id:"jingheya/lotus-depth-g-v1-0"},{description:"A depth estimation model that predicts depth in videos.",id:"tencent/DepthCrafter"},{description:"A robust depth estimation model.",id:"apple/DepthPro-hf"}],spaces:[{description:"An application that predicts the depth of an image and then reconstruct the 3D model as voxels.",id:"radames/dpt-depth-estimation-3d-voxels"},{description:"An application for bleeding-edge depth estimation.",id:"akhaliq/depth-pro"},{description:"An application on cutting-edge depth estimation in videos.",id:"tencent/DepthCrafter"},{description:"A human-centric depth estimation application.",id:"facebook/sapiens-depth"}],summary:"Depth estimation is the task of predicting depth of the objects present in an image.",widgetModels:[""],youtubeId:""},data_default19={datasets:[],demo:{inputs:[],outputs:[]},isPlaceholder:!0,metrics:[],models:[],spaces:[],summary:"",widgetModels:[],youtubeId:void 0,canonicalId:void 0},data_default20={datasets:[{description:"A curation of widely used datasets for Data Driven Deep Reinforcement Learning (D4RL)",id:"edbeeching/decision_transformer_gym_replay"}],demo:{inputs:[{label:"State",content:"Red traffic light, pedestrians are about to pass.",type:"text"}],outputs:[{label:"Action",content:"Stop the car.",type:"text"},{label:"Next State",content:"Yellow light, pedestrians have crossed.",type:"text"}]},metrics:[{description:"Accumulated reward across all time steps discounted by a factor that ranges between 0 and 1 and determines how much the agent optimizes for future relative to immediate rewards. Measures how good is the policy ultimately found by a given algorithm considering uncertainty over the future.",id:"Discounted Total Reward"},{description:"Average return obtained after running the policy for a certain number of evaluation episodes. As opposed to total reward, mean reward considers how much reward a given algorithm receives while learning.",id:"Mean Reward"},{description:"Measures how good a given algorithm is after a predefined time. Some algorithms may be guaranteed to converge to optimal behavior across many time steps. However, an agent that reaches an acceptable level of optimality after a given time horizon may be preferable to one that ultimately reaches optimality but takes a long time.",id:"Level of Performance After Some Time"}],models:[{description:"A Reinforcement Learning model trained on expert data from the Gym Hopper environment",id:"edbeeching/decision-transformer-gym-hopper-expert"},{description:"A PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo.",id:"HumanCompatibleAI/ppo-seals-CartPole-v0"}],spaces:[{description:"An application for a cute puppy agent learning to catch a stick.",id:"ThomasSimonini/Huggy"},{description:"An application to play Snowball Fight with a reinforcement learning agent.",id:"ThomasSimonini/SnowballFight"}],summary:"Reinforcement learning is the computational approach of learning from action by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback",widgetModels:[],youtubeId:"q0BiUn5LiBc"},data_default21={datasets:[{description:"A famous question answering dataset based on English articles from Wikipedia.",id:"squad_v2"},{description:"A dataset of aggregated anonymized actual queries issued to the Google search engine.",id:"natural_questions"}],demo:{inputs:[{label:"Question",content:"Which name is also used to describe the Amazon rainforest in English?",type:"text"},{label:"Context",content:"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle",type:"text"}],outputs:[{label:"Answer",content:"Amazonia",type:"text"}]},metrics:[{description:"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",id:"exact-match"},{description:" The F1-Score metric is useful if we value both false positives and false negatives equally. The F1-Score is calculated on each word in the predicted sequence against the correct answer",id:"f1"}],models:[{description:"A robust baseline model for most question answering domains.",id:"deepset/roberta-base-squad2"},{description:"Small yet robust model that can answer questions.",id:"distilbert/distilbert-base-cased-distilled-squad"},{description:"A special model that can answer questions from tables.",id:"google/tapas-base-finetuned-wtq"}],spaces:[{description:"An application that can answer a long question from Wikipedia.",id:"deepset/wikipedia-assistant"}],summary:"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!",widgetModels:["deepset/roberta-base-squad2"],youtubeId:"ajPx5LwJD-I"},data_default22={datasets:[{description:"Bing queries with relevant passages from various web sources.",id:"microsoft/ms_marco"}],demo:{inputs:[{label:"Source sentence",content:"Machine learning is so easy.",type:"text"},{label:"Sentences to compare to",content:"Deep learning is so straightforward.",type:"text"},{label:"",content:"This is so difficult, like rocket science.",type:"text"},{label:"",content:"I can't believe how much I struggled with this.",type:"text"}],outputs:[{type:"chart",data:[{label:"Deep learning is so straightforward.",score:.623},{label:"This is so difficult, like rocket science.",score:.413},{label:"I can't believe how much I struggled with this.",score:.256}]}]},metrics:[{description:"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",id:"Mean Reciprocal Rank"},{description:"The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length",id:"Cosine Similarity"}],models:[{description:"This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.",id:"sentence-transformers/all-mpnet-base-v2"},{description:"A multilingual robust sentence similarity model.",id:"BAAI/bge-m3"},{description:"A robust sentence similarity model.",id:"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5"}],spaces:[{description:"An application that leverages sentence similarity to answer questions from YouTube videos.",id:"Gradio-Blocks/Ask_Questions_To_YouTube_Videos"},{description:"An application that retrieves relevant PubMed abstracts for a given online article which can be used as further references.",id:"Gradio-Blocks/pubmed-abstract-retriever"},{description:"An application that leverages sentence similarity to summarize text.",id:"nickmuchi/article-text-summarizer"},{description:"A guide that explains how Sentence Transformers can be used for semantic search.",id:"sentence-transformers/Sentence_Transformers_for_semantic_search"}],summary:"Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.",widgetModels:["sentence-transformers/all-MiniLM-L6-v2"],youtubeId:"VCZq5AkbNEU"},data_default23={canonicalId:"text-generation",datasets:[{description:"News articles in five different languages along with their summaries. Widely used for benchmarking multilingual summarization models.",id:"mlsum"},{description:"English conversations and their summaries. Useful for benchmarking conversational agents.",id:"samsum"}],demo:{inputs:[{label:"Input",content:"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",type:"text"}],outputs:[{label:"Output",content:"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. It was the first structure to reach a height of 300 metres.",type:"text"}]},metrics:[{description:"The generated sequence is compared against its summary, and the overlap of tokens are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.",id:"rouge"}],models:[{description:"A strong summarization model trained on English news articles. Excels at generating factual summaries.",id:"facebook/bart-large-cnn"},{description:"A summarization model trained on medical articles.",id:"Falconsai/medical_summarization"}],spaces:[{description:"An application that can summarize long paragraphs.",id:"pszemraj/summarize-long-text"},{description:"A much needed summarization application for terms and conditions.",id:"ml6team/distilbart-tos-summarizer-tosdr"},{description:"An application that summarizes long documents.",id:"pszemraj/document-summarization"},{description:"An application that can detect errors in abstractive summarization.",id:"ml6team/post-processing-summarization"}],summary:"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.",widgetModels:["facebook/bart-large-cnn"],youtubeId:"yHnr5Dk2zCI"},data_default24={datasets:[{description:"The WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.",id:"wikitablequestions"},{description:"WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia.",id:"wikisql"}],demo:{inputs:[{table:[["Rank","Name","No.of reigns","Combined days"],["1","lou Thesz","3","3749"],["2","Ric Flair","8","3103"],["3","Harley Race","7","1799"]],type:"tabular"},{label:"Question",content:"What is the number of reigns for Harley Race?",type:"text"}],outputs:[{label:"Result",content:"7",type:"text"}]},metrics:[{description:"Checks whether the predicted answer(s) is the same as the ground-truth answer(s).",id:"Denotation Accuracy"}],models:[{description:"A table question answering model that is capable of neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table.",id:"microsoft/tapex-base"},{description:"A robust table question answering model.",id:"google/tapas-base-finetuned-wtq"}],spaces:[{description:"An application that answers questions based on table CSV files.",id:"katanaml/table-query"}],summary:"Table Question Answering (Table QA) is the answering a question about an information on a given table.",widgetModels:["google/tapas-base-finetuned-wtq"]},data_default25={datasets:[{description:"A comprehensive curation of datasets covering all benchmarks.",id:"inria-soda/tabular-benchmark"}],demo:{inputs:[{table:[["Glucose","Blood Pressure ","Skin Thickness","Insulin","BMI"],["148","72","35","0","33.6"],["150","50","30","0","35.1"],["141","60","29","1","39.2"]],type:"tabular"}],outputs:[{table:[["Diabetes"],["1"],["1"],["0"]],type:"tabular"}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"Breast cancer prediction model based on decision trees.",id:"scikit-learn/cancer-prediction-trees"}],spaces:[{description:"An application that can predict defective products on a production line.",id:"scikit-learn/tabular-playground"},{description:"An application that compares various tabular classification techniques on different datasets.",id:"scikit-learn/classification"}],summary:"Tabular classification is the task of classifying a target category (a group) based on set of attributes.",widgetModels:["scikit-learn/tabular-playground"],youtubeId:""},data_default26={datasets:[{description:"A comprehensive curation of datasets covering all benchmarks.",id:"inria-soda/tabular-benchmark"}],demo:{inputs:[{table:[["Car Name","Horsepower","Weight"],["ford torino","140","3,449"],["amc hornet","97","2,774"],["toyota corolla","65","1,773"]],type:"tabular"}],outputs:[{table:[["MPG (miles per gallon)"],["17"],["18"],["31"]],type:"tabular"}]},metrics:[{description:"",id:"mse"},{description:"Coefficient of determination (or R-squared) is a measure of how well the model fits the data. Higher R-squared is considered a better fit.",id:"r-squared"}],models:[{description:"Fish weight prediction based on length measurements and species.",id:"scikit-learn/Fish-Weight"}],spaces:[{description:"An application that can predict weight of a fish based on set of attributes.",id:"scikit-learn/fish-weight-prediction"}],summary:"Tabular regression is the task of predicting a numerical value given a set of attributes.",widgetModels:["scikit-learn/Fish-Weight"],youtubeId:""},data_default27={datasets:[{description:"RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.",id:"red_caps"},{description:"Conceptual Captions is a dataset consisting of ~3.3M images annotated with captions.",id:"conceptual_captions"},{description:"12M image-caption pairs.",id:"Spawning/PD12M"}],demo:{inputs:[{label:"Input",content:"A city above clouds, pastel colors, Victorian style",type:"text"}],outputs:[{filename:"image.jpeg",type:"img"}]},metrics:[{description:"The Inception Score (IS) measure assesses diversity and meaningfulness. It uses a generated image sample to predict its label. A higher score signifies more diverse and meaningful images.",id:"IS"},{description:"The Fr√©chet Inception Distance (FID) calculates the distance between distributions between synthetic and real samples. A lower FID score indicates better similarity between the distributions of real and generated images.",id:"FID"},{description:"R-precision assesses how the generated image aligns with the provided text description. It uses the generated images as queries to retrieve relevant text descriptions. The top 'r' relevant descriptions are selected and used to calculate R-precision as r/R, where 'R' is the number of ground truth descriptions associated with the generated images. A higher R-precision value indicates a better model.",id:"R-Precision"}],models:[{description:"One of the most powerful image generation models that can generate realistic outputs.",id:"black-forest-labs/FLUX.1-Krea-dev"},{description:"A powerful image generation model.",id:"Qwen/Qwen-Image"},{description:"Powerful and fast image generation model.",id:"ByteDance/SDXL-Lightning"},{description:"A powerful text-to-image model.",id:"ByteDance/Hyper-SD"}],spaces:[{description:"A powerful text-to-image application.",id:"stabilityai/stable-diffusion-3-medium"},{description:"A text-to-image application to generate comics.",id:"jbilcke-hf/ai-comic-factory"},{description:"An application to match multiple custom image generation models.",id:"multimodalart/flux-lora-lab"},{description:"A powerful yet very fast image generation application.",id:"latent-consistency/lcm-lora-for-sdxl"},{description:"A gallery to explore various text-to-image models.",id:"multimodalart/LoraTheExplorer"},{description:"An application for `text-to-image`, `image-to-image` and image inpainting.",id:"ArtGAN/Stable-Diffusion-ControlNet-WebUI"},{description:"An application to generate realistic images given photos of a person and a prompt.",id:"InstantX/InstantID"}],summary:"Text-to-image is the task of generating images from input text. These pipelines can also be used to modify and edit images based on text prompts.",widgetModels:["black-forest-labs/FLUX.1-dev"],youtubeId:""},data_default28={canonicalId:"text-to-audio",datasets:[{description:"10K hours of multi-speaker English dataset.",id:"parler-tts/mls_eng_10k"},{description:"Multi-speaker English dataset.",id:"mythicinfinity/libritts_r"},{description:"Multi-lingual dataset.",id:"facebook/multilingual_librispeech"}],demo:{inputs:[{label:"Input",content:"I love audio models on the Hub!",type:"text"}],outputs:[{filename:"audio.wav",type:"audio"}]},metrics:[{description:"The Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated speech.",id:"mel cepstral distortion"}],models:[{description:"Small yet powerful TTS model.",id:"KittenML/kitten-tts-nano-0.1"},{description:"Bleeding edge TTS model.",id:"ResembleAI/chatterbox"},{description:"A massively multi-lingual TTS model.",id:"fishaudio/fish-speech-1.5"},{description:"A text-to-dialogue model.",id:"nari-labs/Dia-1.6B-0626"}],spaces:[{description:"An application for generate high quality speech in different languages.",id:"hexgrad/Kokoro-TTS"},{description:"A multilingual text-to-speech application.",id:"fishaudio/fish-speech-1"},{description:"Performant TTS application.",id:"ResembleAI/Chatterbox"},{description:"An application to compare different TTS models.",id:"TTS-AGI/TTS-Arena-V2"},{description:"An application that generates podcast episodes.",id:"ngxson/kokoro-podcast-generator"}],summary:"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.",widgetModels:["suno/bark"],youtubeId:"NW62DpzJ274"},data_default29={datasets:[{description:"A widely used dataset useful to benchmark named entity recognition models.",id:"eriktks/conll2003"},{description:"A multilingual dataset of Wikipedia articles annotated for named entity recognition in over 150 different languages.",id:"unimelb-nlp/wikiann"}],demo:{inputs:[{label:"Input",content:"My name is Omar and I live in Z√ºrich.",type:"text"}],outputs:[{text:"My name is Omar and I live in Z√ºrich.",tokens:[{type:"PERSON",start:11,end:15},{type:"GPE",start:30,end:36}],type:"text-with-tokens"}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"A robust performance model to identify people, locations, organizations and names of miscellaneous entities.",id:"dslim/bert-base-NER"},{description:"A strong model to identify people, locations, organizations and names in multiple languages.",id:"FacebookAI/xlm-roberta-large-finetuned-conll03-english"},{description:"A token classification model specialized on medical entity recognition.",id:"blaze999/Medical-NER"},{description:"Flair models are typically the state of the art in named entity recognition tasks.",id:"flair/ner-english"}],spaces:[{description:"An application that can recognizes entities, extracts noun chunks and recognizes various linguistic features of each token.",id:"spacy/gradio_pipeline_visualizer"}],summary:"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.",widgetModels:["FacebookAI/xlm-roberta-large-finetuned-conll03-english"],youtubeId:"wVHdVlPScxA"},data_default30={canonicalId:"text-generation",datasets:[{description:"A dataset of copyright-free books translated into 16 different languages.",id:"Helsinki-NLP/opus_books"},{description:"An example of translation between programming languages. This dataset consists of functions in Java and C#.",id:"google/code_x_glue_cc_code_to_code_trans"}],demo:{inputs:[{label:"Input",content:"My name is Omar and I live in Z√ºrich.",type:"text"}],outputs:[{label:"Output",content:"Mein Name ist Omar und ich wohne in Z√ºrich.",type:"text"}]},metrics:[{description:"BLEU score is calculated by counting the number of shared single or subsequent tokens between the generated sequence and the reference. Subsequent n tokens are called ‚Äún-grams‚Äù. Unigram refers to a single token while bi-gram refers to token pairs and n-grams refer to n subsequent tokens. The score ranges from 0 to 1, where 1 means the translation perfectly matched and 0 did not match at all",id:"bleu"},{description:"",id:"sacrebleu"}],models:[{description:"Very powerful model that can translate many languages between each other, especially low-resource languages.",id:"facebook/nllb-200-1.3B"},{description:"A general-purpose Transformer that can be used to translate from English to German, French, or Romanian.",id:"google-t5/t5-base"}],spaces:[{description:"An application that can translate between 100 languages.",id:"Iker/Translate-100-languages"},{description:"An application that can translate between many languages.",id:"Geonmo/nllb-translation-demo"}],summary:"Translation is the task of converting text from one language to another.",widgetModels:["facebook/mbart-large-50-many-to-many-mmt"],youtubeId:"1JvfrvZgi6c"},data_default31={datasets:[{description:"A widely used dataset used to benchmark multiple variants of text classification.",id:"nyu-mll/glue"},{description:"A text classification dataset used to benchmark natural language inference models",id:"stanfordnlp/snli"}],demo:{inputs:[{label:"Input",content:"I love Hugging Face!",type:"text"}],outputs:[{type:"chart",data:[{label:"POSITIVE",score:.9},{label:"NEUTRAL",score:.1},{label:"NEGATIVE",score:0}]}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"The F1 metric is the harmonic mean of the precision and recall. It can be calculated as: F1 = 2 * (precision * recall) / (precision + recall)",id:"f1"}],models:[{description:"A robust model trained for sentiment analysis.",id:"distilbert/distilbert-base-uncased-finetuned-sst-2-english"},{description:"A sentiment analysis model specialized in financial sentiment.",id:"ProsusAI/finbert"},{description:"A sentiment analysis model specialized in analyzing tweets.",id:"cardiffnlp/twitter-roberta-base-sentiment-latest"},{description:"A model that can classify languages.",id:"papluca/xlm-roberta-base-language-detection"},{description:"A model that can classify text generation attacks.",id:"meta-llama/Prompt-Guard-86M"}],spaces:[{description:"An application that can classify financial sentiment.",id:"IoannisTr/Tech_Stocks_Trading_Assistant"},{description:"A dashboard that contains various text classification tasks.",id:"miesnerjacob/Multi-task-NLP"},{description:"An application that analyzes user reviews in healthcare.",id:"spacy/healthsea-demo"}],summary:"Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.",widgetModels:["distilbert/distilbert-base-uncased-finetuned-sst-2-english"],youtubeId:"leNG9fN9FQU"},data_default32={datasets:[{description:"Multilingual dataset used to evaluate text generation models.",id:"CohereForAI/Global-MMLU"},{description:"High quality multilingual data used to train text-generation models.",id:"HuggingFaceFW/fineweb-2"},{description:"Truly open-source, curated and cleaned dialogue dataset.",id:"HuggingFaceH4/ultrachat_200k"},{description:"A reasoning dataset.",id:"open-r1/OpenThoughts-114k-math"},{description:"A multilingual instruction dataset with preference ratings on responses.",id:"allenai/tulu-3-sft-mixture"},{description:"A large synthetic dataset for alignment of text generation models.",id:"HuggingFaceTB/smoltalk"},{description:"A dataset made for training text generation models solving math questions.",id:"HuggingFaceTB/finemath"}],demo:{inputs:[{label:"Input",content:"Once upon a time,",type:"text"}],outputs:[{label:"Output",content:"Once upon a time, we knew that our ancestors were on the verge of extinction. The great explorers and poets of the Old World, from Alexander the Great to Chaucer, are dead and gone. A good many of our ancient explorers and poets have",type:"text"}]},metrics:[{description:"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",id:"Cross Entropy"},{description:"The Perplexity metric is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",id:"Perplexity"}],models:[{description:"A text-generation model trained to follow instructions.",id:"google/gemma-2-2b-it"},{description:"Powerful text generation model for coding.",id:"Qwen/Qwen3-Coder-480B-A35B-Instruct"},{description:"Great text generation model with top-notch tool calling capabilities.",id:"openai/gpt-oss-120b"},{description:"Powerful text generation model.",id:"zai-org/GLM-4.5"},{description:"A powerful small model with reasoning capabilities.",id:"Qwen/Qwen3-4B-Thinking-2507"},{description:"Strong conversational model that supports very long instructions.",id:"Qwen/Qwen2.5-7B-Instruct-1M"},{description:"Text generation model used to write code.",id:"Qwen/Qwen2.5-Coder-32B-Instruct"},{description:"Powerful reasoning based open large language model.",id:"deepseek-ai/DeepSeek-R1"}],spaces:[{description:"An application that writes and executes code from text instructions and supports many models.",id:"akhaliq/anycoder"},{description:"An application that builds websites from natural language prompts.",id:"enzostvs/deepsite"},{description:"A leaderboard for comparing chain-of-thought performance of models.",id:"logikon/open_cot_leaderboard"},{description:"An text generation based application based on a very powerful LLaMA2 model.",id:"ysharma/Explore_llamav2_with_TGI"},{description:"An text generation based application to converse with Zephyr model.",id:"HuggingFaceH4/zephyr-chat"},{description:"A leaderboard that ranks text generation models based on blind votes from people.",id:"lmsys/chatbot-arena-leaderboard"},{description:"An chatbot to converse with a very powerful text generation model.",id:"mlabonne/phixtral-chat"}],summary:"Generating text is the task of generating new text given another text. These models can, for example, fill in incomplete text or paraphrase.",widgetModels:["mistralai/Mistral-Nemo-Instruct-2407"],youtubeId:"e9gNEAlsOvU"},data_default33={datasets:[{description:"Bing queries with relevant passages from various web sources.",id:"microsoft/ms_marco"}],demo:{inputs:[{label:"Source sentence",content:"Machine learning is so easy.",type:"text"},{label:"Sentences to compare to",content:"Deep learning is so straightforward.",type:"text"},{label:"",content:"This is so difficult, like rocket science.",type:"text"},{label:"",content:"I can't believe how much I struggled with this.",type:"text"}],outputs:[{type:"chart",data:[{label:"Deep learning is so straightforward.",score:2.2006407},{label:"This is so difficult, like rocket science.",score:-6.2634873},{label:"I can't believe how much I struggled with this.",score:-10.251488}]}]},metrics:[{description:"Discounted Cumulative Gain (DCG) measures the gain, or usefulness, of search results discounted by their position. The normalization is done by dividing the DCG by the ideal DCG, which is the DCG of the perfect ranking.",id:"Normalized Discounted Cumulative Gain"},{description:"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",id:"Mean Reciprocal Rank"},{description:"Mean Average Precision (mAP) is the overall average of the Average Precision (AP) values, where AP is the Area Under the PR Curve (AUC-PR)",id:"Mean Average Precision"}],models:[{description:"An extremely efficient text ranking model trained on a web search dataset.",id:"cross-encoder/ms-marco-MiniLM-L6-v2"},{description:"A strong multilingual text reranker model.",id:"Alibaba-NLP/gte-multilingual-reranker-base"},{description:"An efficient text ranking model that punches above its weight.",id:"Alibaba-NLP/gte-reranker-modernbert-base"}],spaces:[],summary:"Text Ranking is the task of ranking a set of texts based on their relevance to a query. Text ranking models are trained on large datasets of queries and relevant documents to learn how to rank documents based on their relevance to the query. This task is particularly useful for search engines and information retrieval systems.",widgetModels:["cross-encoder/ms-marco-MiniLM-L6-v2"],youtubeId:""},data_default34={datasets:[{description:"Microsoft Research Video to Text is a large-scale dataset for open domain video captioning",id:"iejMac/CLIP-MSR-VTT"},{description:"UCF101 Human Actions dataset consists of 13,320 video clips from YouTube, with 101 classes.",id:"quchenyuan/UCF101-ZIP"},{description:"A high-quality dataset for human action recognition in YouTube videos.",id:"nateraw/kinetics"},{description:"A dataset of video clips of humans performing pre-defined basic actions with everyday objects.",id:"HuggingFaceM4/something_something_v2"},{description:"This dataset consists of text-video pairs and contains noisy samples with irrelevant video descriptions",id:"HuggingFaceM4/webvid"},{description:"A dataset of short Flickr videos for the temporal localization of events with descriptions.",id:"iejMac/CLIP-DiDeMo"}],demo:{inputs:[{label:"Input",content:"Darth Vader is surfing on the waves.",type:"text"}],outputs:[{filename:"text-to-video-output.gif",type:"img"}]},metrics:[{description:"Inception Score uses an image classification model that predicts class labels and evaluates how distinct and diverse the images are. A higher score indicates better video generation.",id:"is"},{description:"Frechet Inception Distance uses an image classification model to obtain image embeddings. The metric compares mean and standard deviation of the embeddings of real and generated images. A smaller score indicates better video generation.",id:"fid"},{description:"Frechet Video Distance uses a model that captures coherence for changes in frames and the quality of each frame. A smaller score indicates better video generation.",id:"fvd"},{description:"CLIPSIM measures similarity between video frames and text using an image-text similarity model. A higher score indicates better video generation.",id:"clipsim"}],models:[{description:"A strong model for consistent video generation.",id:"tencent/HunyuanVideo"},{description:"A text-to-video model with high fidelity motion and strong prompt adherence.",id:"Lightricks/LTX-Video"},{description:"A text-to-video model focusing on physics-aware applications like robotics.",id:"nvidia/Cosmos-1.0-Diffusion-7B-Text2World"},{description:"Very fast model for video generation.",id:"Lightricks/LTX-Video-0.9.8-13B-distilled"}],spaces:[{description:"An application that generates video from text.",id:"VideoCrafter/VideoCrafter"},{description:"Consistent video generation application.",id:"Wan-AI/Wan2.1"},{description:"A cutting edge video generation application.",id:"Pyramid-Flow/pyramid-flow"}],summary:"Text-to-video models can be used in any application that requires generating consistent sequence of images from text. ",widgetModels:["Wan-AI/Wan2.2-TI2V-5B"],youtubeId:void 0},data_default35={datasets:[{description:"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class.",id:"cifar100"},{description:"Multiple images of celebrities, used for facial expression translation.",id:"CelebA"}],demo:{inputs:[{label:"Seed",content:"42",type:"text"},{label:"Number of images to generate:",content:"4",type:"text"}],outputs:[{filename:"unconditional-image-generation-output.jpeg",type:"img"}]},metrics:[{description:"The inception score (IS) evaluates the quality of generated images. It measures the diversity of the generated images (the model predictions are evenly distributed across all possible labels) and their 'distinction' or 'sharpness' (the model confidently predicts a single label for each image).",id:"Inception score (IS)"},{description:"The Fr√©chet Inception Distance (FID) evaluates the quality of images created by a generative model by calculating the distance between feature vectors for real and generated images.",id:"Freƒáhet Inception Distance (FID)"}],models:[{description:"High-quality image generation model trained on the CIFAR-10 dataset. It synthesizes images of the ten classes presented in the dataset using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",id:"google/ddpm-cifar10-32"},{description:"High-quality image generation model trained on the 256x256 CelebA-HQ dataset. It synthesizes images of faces using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",id:"google/ddpm-celebahq-256"}],spaces:[{description:"An application that can generate realistic faces.",id:"CompVis/celeba-latent-diffusion"}],summary:"Unconditional image generation is the task of generating images with no condition in any context (like a prompt text or another image). Once trained, the model will create images that resemble its training data distribution.",widgetModels:[""],youtubeId:""},data_default36={datasets:[{description:"Benchmark dataset used for video classification with videos that belong to 400 classes.",id:"kinetics400"}],demo:{inputs:[{filename:"video-classification-input.gif",type:"img"}],outputs:[{type:"chart",data:[{label:"Playing Guitar",score:.514},{label:"Playing Tennis",score:.193},{label:"Cooking",score:.068}]}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"Strong Video Classification model trained on the Kinetics 400 dataset.",id:"google/vivit-b-16x2-kinetics400"},{description:"Strong Video Classification model trained on the Kinetics 400 dataset.",id:"microsoft/xclip-base-patch32"}],spaces:[{description:"An application that classifies video at different timestamps.",id:"nateraw/lavila"},{description:"An application that classifies video.",id:"fcakyon/video-classification"}],summary:"Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to.",widgetModels:[],youtubeId:""},data_default37={datasets:[{description:"A large dataset used to train visual document retrieval models.",id:"vidore/colpali_train_set"}],demo:{inputs:[{filename:"input.png",type:"img"},{label:"Question",content:"Is the model in this paper the fastest for inference?",type:"text"}],outputs:[{type:"chart",data:[{label:"Page 10",score:.7},{label:"Page 11",score:.06},{label:"Page 9",score:.003}]}]},isPlaceholder:!1,metrics:[{description:"NDCG@k scores ranked recommendation lists for top-k results. 0 is the worst, 1 is the best.",id:"Normalized Discounted Cumulative Gain at K"}],models:[{description:"Very accurate visual document retrieval model for multilingual queries and documents.",id:"vidore/colqwen2-v1.0"},{description:"Very fast and efficient visual document retrieval model that can also take in other modalities like audio.",id:"Tevatron/OmniEmbed-v0.1"}],spaces:[{description:"A leaderboard of visual document retrieval models.",id:"vidore/vidore-leaderboard"},{description:"Visual retrieval augmented generation demo based on ColQwen2 model.",id:"vidore/visual-rag-tool"}],summary:"Visual document retrieval is the task of searching for relevant image-based documents, such as PDFs. These models take a text query and multiple documents as input and return the top-most relevant documents and relevancy scores as output.",widgetModels:[""],youtubeId:""},data_default38={datasets:[{description:"A widely used dataset containing questions (with answers) about images.",id:"Graphcore/vqa"},{description:"A dataset to benchmark visual reasoning based on text in images.",id:"facebook/textvqa"}],demo:{inputs:[{filename:"elephant.jpeg",type:"img"},{label:"Question",content:"What is in this image?",type:"text"}],outputs:[{type:"chart",data:[{label:"elephant",score:.97},{label:"elephants",score:.06},{label:"animal",score:.003}]}]},isPlaceholder:!1,metrics:[{description:"",id:"accuracy"},{description:"Measures how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.",id:"wu-palmer similarity"}],models:[{description:"A visual question answering model trained to convert charts and plots to text.",id:"google/deplot"},{description:"A visual question answering model trained for mathematical reasoning and chart derendering from images.",id:"google/matcha-base"},{description:"A strong visual question answering that answers questions from book covers.",id:"google/pix2struct-ocrvqa-large"}],spaces:[{description:"An application that compares visual question answering models across different tasks.",id:"merve/pix2struct"},{description:"An application that can answer questions based on images.",id:"nielsr/vilt-vqa"},{description:"An application that can caption images and answer questions about a given image. ",id:"Salesforce/BLIP"},{description:"An application that can caption images and answer questions about a given image. ",id:"vumichien/Img2Prompt"}],summary:"Visual Question Answering is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.",widgetModels:["dandelin/vilt-b32-finetuned-vqa"],youtubeId:""},data_default39={datasets:[{description:"A widely used dataset used to benchmark multiple variants of text classification.",id:"nyu-mll/glue"},{description:"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information.",id:"nyu-mll/multi_nli"},{description:"FEVER is a publicly available dataset for fact extraction and verification against textual sources.",id:"fever/fever"}],demo:{inputs:[{label:"Text Input",content:"Dune is the best movie ever.",type:"text"},{label:"Candidate Labels",content:"CINEMA, ART, MUSIC",type:"text"}],outputs:[{type:"chart",data:[{label:"CINEMA",score:.9},{label:"ART",score:.1},{label:"MUSIC",score:0}]}]},metrics:[],models:[{description:"Powerful zero-shot text classification model.",id:"facebook/bart-large-mnli"},{description:"Cutting-edge zero-shot multilingual text classification model.",id:"MoritzLaurer/ModernBERT-large-zeroshot-v2.0"},{description:"Zero-shot text classification model that can be used for topic and sentiment classification.",id:"knowledgator/gliclass-modern-base-v2.0-init"}],spaces:[],summary:"Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.",widgetModels:["facebook/bart-large-mnli"]},data_default40={datasets:[{description:"",id:""}],demo:{inputs:[{filename:"image-classification-input.jpeg",type:"img"},{label:"Classes",content:"cat, dog, bird",type:"text"}],outputs:[{type:"chart",data:[{label:"Cat",score:.664},{label:"Dog",score:.329},{label:"Bird",score:.008}]}]},metrics:[{description:"Computes the number of times the correct label appears in top K labels predicted",id:"top-K accuracy"}],models:[{description:"Multilingual image classification model for 80 languages.",id:"visheratin/mexma-siglip"},{description:"Strong zero-shot image classification model.",id:"google/siglip2-base-patch16-224"},{description:"Robust zero-shot image classification model.",id:"intfloat/mmE5-mllama-11b-instruct"},{description:"Powerful zero-shot image classification model supporting 94 languages.",id:"jinaai/jina-clip-v2"},{description:"Strong image classification model for biomedical domain.",id:"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"}],spaces:[{description:"An application that leverages zero-shot image classification to find best captions to generate an image. ",id:"pharma/CLIP-Interrogator"},{description:"An application to compare different zero-shot image classification models. ",id:"merve/compare_clip_siglip"}],summary:"Zero-shot image classification is the task of classifying previously unseen classes during training of a model.",widgetModels:["google/siglip-so400m-patch14-224"],youtubeId:""},data_default41={datasets:[],demo:{inputs:[{filename:"zero-shot-object-detection-input.jpg",type:"img"},{label:"Classes",content:"cat, dog, bird",type:"text"}],outputs:[{filename:"zero-shot-object-detection-output.jpg",type:"img"}]},metrics:[{description:"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",id:"Average Precision"},{description:"The Mean Average Precision (mAP) metric is the overall average of the AP values",id:"Mean Average Precision"},{description:"The APŒ± metric is the Average Precision at the IoU threshold of a Œ± value, for example, AP50 and AP75",id:"APŒ±"}],models:[{description:"Solid zero-shot object detection model.",id:"openmmlab-community/mm_grounding_dino_large_all"},{description:"Cutting-edge zero-shot object detection model.",id:"fushh7/LLMDet"}],spaces:[{description:"A demo to compare different zero-shot object detection models per output and latency.",id:"ariG23498/zero-shot-od"},{description:"A demo that combines a zero-shot object detection and mask generation model for zero-shot segmentation.",id:"merve/OWLSAM"}],summary:"Zero-shot object detection is a computer vision task to detect objects and their classes in images, without any prior training or knowledge of the classes. Zero-shot object detection models receive an image as input, as well as a list of candidate classes, and output the bounding boxes and labels where the objects have been detected.",widgetModels:[],youtubeId:""},data_default42={datasets:[{description:"A large dataset of over 10 million 3D objects.",id:"allenai/objaverse-xl"},{description:"A dataset of isolated object images for evaluating image-to-3D models.",id:"dylanebert/iso3d"}],demo:{inputs:[{filename:"image-to-3d-image-input.png",type:"img"}],outputs:[{label:"Result",content:"image-to-3d-3d-output-filename.glb",type:"text"}]},metrics:[],models:[{description:"Fast image-to-3D mesh model by Tencent.",id:"TencentARC/InstantMesh"},{description:"3D world generation model.",id:"tencent/HunyuanWorld-1"},{description:"A scaled up image-to-3D mesh model derived from TripoSR.",id:"hwjiang/Real3D"},{description:"Consistent image-to-3d generation model.",id:"stabilityai/stable-point-aware-3d"}],spaces:[{description:"Leaderboard to evaluate image-to-3D models.",id:"dylanebert/3d-arena"},{description:"Image-to-3D demo with mesh outputs.",id:"TencentARC/InstantMesh"},{description:"Image-to-3D demo.",id:"stabilityai/stable-point-aware-3d"},{description:"Image-to-3D demo with mesh outputs.",id:"hwjiang/Real3D"},{description:"Image-to-3D demo with splat outputs.",id:"dylanebert/LGM-mini"}],summary:"Image-to-3D models take in image input and produce 3D output.",widgetModels:[],youtubeId:""},data_default43={datasets:[{description:"A large dataset of over 10 million 3D objects.",id:"allenai/objaverse-xl"},{description:"Descriptive captions for 3D objects in Objaverse.",id:"tiange/Cap3D"}],demo:{inputs:[{label:"Prompt",content:"a cat statue",type:"text"}],outputs:[{label:"Result",content:"text-to-3d-3d-output-filename.glb",type:"text"}]},metrics:[],models:[{description:"Text-to-3D mesh model by OpenAI",id:"openai/shap-e"},{description:"Generative 3D gaussian splatting model.",id:"ashawkey/LGM"}],spaces:[{description:"Text-to-3D demo with mesh outputs.",id:"hysts/Shap-E"},{description:"Text/image-to-3D demo with splat outputs.",id:"ashawkey/LGM"}],summary:"Text-to-3D models take in text input and produce 3D output.",widgetModels:[],youtubeId:""},data_default44={datasets:[{description:"A dataset of hand keypoints of over 500k examples.",id:"Vincent-luo/hagrid-mediapipe-hands"}],demo:{inputs:[{filename:"keypoint-detection-input.png",type:"img"}],outputs:[{filename:"keypoint-detection-output.png",type:"img"}]},metrics:[],models:[{description:"A robust keypoint detection model.",id:"magic-leap-community/superpoint"},{description:"A robust keypoint matching model.",id:"magic-leap-community/superglue_outdoor"},{description:"Strong keypoint detection model used to detect human pose.",id:"qualcomm/RTMPose-Body2d"},{description:"Powerful keypoint matching model.",id:"ETH-CVG/lightglue_disk"}],spaces:[{description:"An application that detects hand keypoints in real-time.",id:"datasciencedojo/Hand-Keypoint-Detection-Realtime"},{description:"An application for keypoint detection and matching.",id:"ETH-CVG/LightGlue"}],summary:"Keypoint detection is the task of identifying meaningful distinctive points or features in an image.",widgetModels:[],youtubeId:""},data_default45={datasets:[{description:"Multiple-choice questions and answers about videos.",id:"lmms-lab/Video-MME"},{description:"A dataset of instructions and question-answer pairs about videos.",id:"lmms-lab/VideoChatGPT"},{description:"Large video understanding dataset.",id:"HuggingFaceFV/finevideo"}],demo:{inputs:[{filename:"video-text-to-text-input.gif",type:"img"},{label:"Text Prompt",content:"What is happening in this video?",type:"text"}],outputs:[{label:"Answer",content:"The video shows a series of images showing a fountain with water jets and a variety of colorful flowers and butterflies in the background.",type:"text"}]},metrics:[],models:[{description:"A robust video-text-to-text model.",id:"Vision-CAIR/LongVU_Qwen2_7B"},{description:"Strong video-text-to-text model with reasoning capabilities.",id:"GoodiesHere/Apollo-LMMs-Apollo-7B-t32"},{description:"Strong video-text-to-text model.",id:"HuggingFaceTB/SmolVLM2-2.2B-Instruct"}],spaces:[{description:"An application to chat with a video-text-to-text model.",id:"llava-hf/video-llava"},{description:"A leaderboard for various video-text-to-text models.",id:"opencompass/openvlm_video_leaderboard"},{description:"An application to generate highlights from a video.",id:"HuggingFaceTB/SmolVLM2-HighlightGenerator"}],summary:"Video-text-to-text models take in a video and a text prompt and output text. These models are also called video-language models.",widgetModels:[""],youtubeId:""},data_default46={datasets:[{description:"Dataset with detailed annotations for training and benchmarking video instance editing.",id:"suimu/VIRESET"},{description:"Dataset to evaluate models on long video generation and understanding.",id:"zhangsh2001/LongV-EVAL"},{description:"Collection of 104 demo videos from the SeedVR/SeedVR2 series showcasing model outputs.",id:"Iceclear/SeedVR_VideoDemos"}],demo:{inputs:[{filename:"input.gif",type:"img"}],outputs:[{filename:"output.gif",type:"img"}]},metrics:[],models:[{description:"Model for editing outfits, character, and scenery in videos.",id:"decart-ai/Lucy-Edit-Dev"},{description:"Framework that uses 3D mesh proxies for precise, consistent video editing.",id:"LeoLau/Shape-for-Motion"},{description:"Model for generating physics-aware videos from input videos and control conditions.",id:"nvidia/Cosmos-Transfer2.5-2B"},{description:"A model to upscale videos at input, designed for seamless use with ComfyUI.",id:"numz/SeedVR2_comfyUI"}],spaces:[{description:"Interactive demo space for Lucy-Edit-Dev video editing.",id:"decart-ai/lucy-edit-dev"},{description:"Demo space for SeedVR2-3B showcasing video upscaling and restoration.",id:"ByteDance-Seed/SeedVR2-3B"}],summary:"Video-to-video models take one or more videos as input and generate new videos as output. They can enhance quality, interpolate frames, modify styles, or create new motion dynamics, enabling creative applications, video production, and research.",widgetModels:[],youtubeId:""},TASKS_MODEL_LIBRARIES={"audio-classification":["speechbrain","transformers","transformers.js"],"audio-to-audio":["asteroid","fairseq","speechbrain"],"automatic-speech-recognition":["espnet","nemo","speechbrain","transformers","transformers.js"],"audio-text-to-text":["transformers"],"depth-estimation":["transformers","transformers.js"],"document-question-answering":["transformers","transformers.js"],"feature-extraction":["sentence-transformers","transformers","transformers.js"],"fill-mask":["transformers","transformers.js"],"graph-ml":["transformers"],"image-classification":["keras","timm","transformers","transformers.js"],"image-feature-extraction":["timm","transformers"],"image-segmentation":["transformers","transformers.js"],"image-text-to-text":["transformers"],"image-to-image":["diffusers","transformers","transformers.js"],"image-to-text":["transformers","transformers.js"],"image-to-video":["diffusers"],"keypoint-detection":["transformers"],"video-classification":["transformers"],"mask-generation":["transformers"],"multiple-choice":["transformers"],"object-detection":["transformers","transformers.js","ultralytics"],other:[],"question-answering":["adapter-transformers","allennlp","transformers","transformers.js"],robotics:[],"reinforcement-learning":["transformers","stable-baselines3","ml-agents","sample-factory"],"sentence-similarity":["sentence-transformers","spacy","transformers.js"],summarization:["transformers","transformers.js"],"table-question-answering":["transformers"],"table-to-text":["transformers"],"tabular-classification":["sklearn"],"tabular-regression":["sklearn"],"tabular-to-text":["transformers"],"text-classification":["adapter-transformers","setfit","spacy","transformers","transformers.js"],"text-generation":["transformers","transformers.js"],"text-ranking":["sentence-transformers","transformers"],"text-retrieval":[],"text-to-image":["diffusers"],"text-to-speech":["espnet","tensorflowtts","transformers","transformers.js"],"text-to-audio":["transformers","transformers.js"],"text-to-video":["diffusers"],"time-series-forecasting":[],"token-classification":["adapter-transformers","flair","spacy","span-marker","stanza","transformers","transformers.js"],translation:["transformers","transformers.js"],"unconditional-image-generation":["diffusers"],"video-text-to-text":["transformers"],"visual-question-answering":["transformers","transformers.js"],"voice-activity-detection":[],"zero-shot-classification":["transformers","transformers.js"],"zero-shot-image-classification":["transformers","transformers.js"],"zero-shot-object-detection":["transformers","transformers.js"],"text-to-3d":["diffusers"],"image-to-3d":["diffusers"],"any-to-any":["transformers"],"visual-document-retrieval":["transformers"],"video-to-video":["diffusers"]};function getData(type,partialTaskData=data_default19){return{...partialTaskData,id:type,label:PIPELINE_DATA[type].name,libraries:TASKS_MODEL_LIBRARIES[type]}}getData("any-to-any",{datasets:[{description:"A dataset with multiple modality input and output pairs.",id:"PKU-Alignment/align-anything"}],demo:{inputs:[{filename:"any-to-any-input.jpg",type:"img"},{label:"Text Prompt",content:"What is the significance of this place?",type:"text"}],outputs:[{label:"Generated Text",content:"The place in the picture is Osaka Castle, located in Osaka, Japan. Osaka Castle is a historic castle that was originally built in the 16th century by Toyotomi Hideyoshi, a powerful warlord of the time. It is one of the most famous landmarks in Osaka and is known for its distinctive white walls and black roof tiles. The castle has been rebuilt several times over the centuries and is now a popular tourist attraction, offering visitors a glimpse into Japan's rich history and culture.",type:"text"},{filename:"any-to-any-output.wav",type:"audio"}]},metrics:[],models:[{description:"Strong model that can take in video, audio, image, text and output text and natural speech.",id:"Qwen/Qwen2.5-Omni-7B"},{description:"Robust model that can take in image and text and generate image and text.",id:"OmniGen2/OmniGen2"},{description:"Any-to-any model with speech, video, audio, image and text understanding capabilities.",id:"openbmb/MiniCPM-o-2_6"},{description:"A model that can understand image and text and generate image and text.",id:"ByteDance-Seed/BAGEL-7B-MoT"}],spaces:[{description:"An application to chat with an any-to-any (image & text) model.",id:"OmniGen2/OmniGen2"}],summary:"Any-to-any models can understand two or more modalities and output two or more modalities.",widgetModels:[],youtubeId:""}),getData("audio-classification",data_default2),getData("audio-to-audio",data_default4),getData("audio-text-to-text",data_default3),getData("automatic-speech-recognition",data_default5),getData("depth-estimation",data_default18),getData("document-question-answering",data_default6),getData("visual-document-retrieval",data_default37),getData("feature-extraction",data_default7),getData("fill-mask",data_default8),getData("image-classification",data_default9),getData("image-feature-extraction",data_default10),getData("image-segmentation",data_default14),getData("image-to-image",data_default11),getData("image-text-to-text",data_default13),getData("image-to-text",data_default12),getData("image-to-video",data_default15),getData("keypoint-detection",data_default44),getData("mask-generation",data_default16),getData("object-detection",data_default17),getData("video-classification",data_default36),getData("question-answering",data_default21),getData("reinforcement-learning",data_default20),getData("sentence-similarity",data_default22),getData("summarization",data_default23),getData("table-question-answering",data_default24),getData("tabular-classification",data_default25),getData("tabular-regression",data_default26),getData("text-classification",data_default31),getData("text-generation",data_default32),getData("text-ranking",data_default33),getData("text-to-image",data_default27),getData("text-to-speech",data_default28),getData("text-to-video",data_default34),getData("token-classification",data_default29),getData("translation",data_default30),getData("unconditional-image-generation",data_default35),getData("video-text-to-text",data_default45),getData("video-to-video",data_default46),getData("visual-question-answering",data_default38),getData("zero-shot-classification",data_default39),getData("zero-shot-image-classification",data_default40),getData("zero-shot-object-detection",data_default41),getData("text-to-3d",data_default43),getData("image-to-3d",data_default42);var inputsTextGeneration=model=>model.tags.includes("conversational")?"text-generation"===model.pipeline_tag?[{role:"user",content:"What is the capital of France?"}]:[{role:"user",content:[{type:"text",text:"Describe this image in one sentence."},{type:"image_url",image_url:{url:"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"}}]}]:'"Can you please let us know more details about your "',inputsTabularPrediction=()=>'\'{"Height":[11.52,12.48],"Length1":[23.2,24.0],"Length2":[25.4,26.3],"Species": ["Bream","Bream"]}\'',modelInputSnippets={"audio-to-audio":()=>'"sample1.flac"',"audio-classification":()=>'"sample1.flac"',"automatic-speech-recognition":()=>'"sample1.flac"',"document-question-answering":()=>'{\n        "image": "cat.png",\n        "question": "What is in this image?"\n    }',"feature-extraction":()=>'"Today is a sunny day and I will get some ice cream."',"fill-mask":model=>`"The answer to the universe is ${model.mask_token}."`,"image-classification":()=>'"cats.jpg"',"image-to-text":()=>'"cats.jpg"',"image-to-image":()=>'{\n    "image": "cat.png",\n    "prompt": "Turn the cat into a tiger."\n}',"image-to-video":()=>'{\n    "image": "cat.png",\n    "prompt": "The cat starts to dance"\n}',"image-segmentation":()=>'"cats.jpg"',"object-detection":()=>'"cats.jpg"',"question-answering":()=>'{\n    "question": "What is my name?",\n    "context": "My name is Clara and I live in Berkeley."\n}',"sentence-similarity":()=>'{\n    "source_sentence": "That is a happy person",\n    "sentences": [\n        "That is a happy dog",\n        "That is a very happy person",\n        "Today is a sunny day"\n    ]\n}',summarization:()=>'"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."',"table-question-answering":()=>'{\n    "query": "How many stars does the transformers repository have?",\n    "table": {\n        "Repository": ["Transformers", "Datasets", "Tokenizers"],\n        "Stars": ["36542", "4512", "3934"],\n        "Contributors": ["651", "77", "34"],\n        "Programming language": [\n            "Python",\n            "Python",\n            "Rust, Python and NodeJS"\n        ]\n    }\n}',"tabular-regression":inputsTabularPrediction,"tabular-classification":inputsTabularPrediction,"text-classification":()=>'"I like you. I love you"',"text-generation":inputsTextGeneration,"image-text-to-text":inputsTextGeneration,"text-to-image":()=>'"Astronaut riding a horse"',"text-to-video":()=>'"A young man walking on the street"',"text-to-speech":()=>'"The answer to the universe is 42"',"text-to-audio":()=>'"liquid drum and bass, atmospheric synths, airy sounds"',"token-classification":()=>'"My name is Sarah Jessica Parker but you can call me Jessica"',translation:()=>'"–ú–µ–Ω—è –∑–æ–≤—É—Ç –í–æ–ª—å—Ñ–≥–∞–Ω–≥ –∏ —è –∂–∏–≤—É –≤ –ë–µ—Ä–ª–∏–Ω–µ"',"zero-shot-classification":()=>'"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"',"zero-shot-image-classification":()=>'"cats.jpg"'};function getModelInputSnippet(model,noWrap=!1,noQuotes=!1){if(model.pipeline_tag){const inputs=modelInputSnippets[model.pipeline_tag];if(inputs){let result=inputs(model);if("string"==typeof result&&(noWrap&&(result=result.replace(/(?:(?:\r?\n|\r)\t*)|\t+/g," ")),noQuotes)){const REGEX_QUOTES=/^"(.+)"$/s,match=result.match(REGEX_QUOTES);result=match?match[1]:result}return result}}return"No input example has been defined for this model task."}function nameWithoutNamespace(modelId){const splitted=modelId.split("/");return 1===splitted.length?splitted[0]:splitted[1]}function get_base_diffusers_model(model){return model.cardData?.base_model?.toString()??"fill-in-base-model"}function get_prompt_from_diffusers_model(model){const prompt=model.widgetData?.[0]?.text??model.cardData?.instance_prompt;if(prompt)return str=prompt,JSON.stringify(str).slice(1,-1);var str}var diffusersDefaultPrompt="Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",diffusersVideoDefaultPrompt="A man with short gray hair plays a red electric guitar.",diffusers=model=>{let codeSnippets;return codeSnippets=model.tags.includes("StableDiffusionInpaintPipeline")||model.tags.includes("StableDiffusionXLInpaintPipeline")?(model=>[`import torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\n\n# switch to "mps" for apple devices\npipe = AutoPipelineForInpainting.from_pretrained("${model.id}", dtype=torch.float16, variant="fp16", device_map="cuda")\n\nimg_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"\nmask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"\n\nimage = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\n\nprompt = "a tiger sitting on a park bench"\ngenerator = torch.Generator(device="cuda").manual_seed(0)\n\nimage = pipe(\n  prompt=prompt,\n  image=image,\n  mask_image=mask_image,\n  guidance_scale=8.0,\n  num_inference_steps=20,  # steps between 15 and 30 work well for us\n  strength=0.99,  # make sure to use \`strength\` below 1.0\n  generator=generator,\n).images[0]`])(model):model.tags.includes("controlnet")?(model=>[`from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n\ncontrolnet = ControlNetModel.from_pretrained("${model.id}")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\t"${get_base_diffusers_model(model)}", controlnet=controlnet\n)`])(model):model.tags.includes("lora")?"image-to-image"===model.pipeline_tag?(model=>[`import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}", dtype=torch.bfloat16, device_map="cuda")\npipe.load_lora_weights("${model.id}")\n\nprompt = "${get_prompt_from_diffusers_model(model)??"Turn this cat into a dog"}"\ninput_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`])(model):"image-to-video"===model.pipeline_tag?(model=>[`import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}", dtype=torch.bfloat16, device_map="cuda")\npipe.load_lora_weights("${model.id}")\n\nprompt = "${get_prompt_from_diffusers_model(model)??diffusersVideoDefaultPrompt}"\ninput_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png")\n\nimage = pipe(image=input_image, prompt=prompt).frames[0]\nexport_to_video(output, "output.mp4")`])(model):"text-to-video"===model.pipeline_tag?(model=>[`import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}", dtype=torch.bfloat16, device_map="cuda")\npipe.load_lora_weights("${model.id}")\n\nprompt = "${get_prompt_from_diffusers_model(model)??diffusersVideoDefaultPrompt}"\n\noutput = pipe(prompt=prompt).frames[0]\nexport_to_video(output, "output.mp4")`])(model):(model=>[`import torch\nfrom diffusers import DiffusionPipeline\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}", dtype=torch.bfloat16, device_map="cuda")\npipe.load_lora_weights("${model.id}")\n\nprompt = "${get_prompt_from_diffusers_model(model)??diffusersDefaultPrompt}"\nimage = pipe(prompt).images[0]`])(model):model.tags.includes("textual_inversion")?(model=>[`import torch\nfrom diffusers import DiffusionPipeline\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}", dtype=torch.bfloat16, device_map="cuda")\npipe.load_textual_inversion("${model.id}")`])(model):model.tags.includes("FluxFillPipeline")?(model=>[`import torch\nfrom diffusers import FluxFillPipeline\nfrom diffusers.utils import load_image\n\nimage = load_image("https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup.png")\nmask = load_image("https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup_mask.png")\n\n# switch to "mps" for apple devices\npipe = FluxFillPipeline.from_pretrained("${model.id}", dtype=torch.bfloat16, device_map="cuda")\nimage = pipe(\n    prompt="a white paper cup",\n    image=image,\n    mask_image=mask,\n    height=1632,\n    width=1232,\n    guidance_scale=30,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=torch.Generator("cpu").manual_seed(0)\n).images[0]\nimage.save(f"flux-fill-dev.png")`])(model):"image-to-video"===model.pipeline_tag?(model=>[`import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${model.id}", dtype=torch.bfloat16, device_map="cuda")\npipe.to("cuda")\n\nprompt = "${get_prompt_from_diffusers_model(model)??diffusersVideoDefaultPrompt}"\nimage = load_image(\n    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png"\n)\n\noutput = pipe(image=image, prompt=prompt).frames[0]\nexport_to_video(output, "output.mp4")`])(model):"image-to-image"===model.pipeline_tag?(model=>[`import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${model.id}", dtype=torch.bfloat16, device_map="cuda")\n\nprompt = "${get_prompt_from_diffusers_model(model)??"Turn this cat into a dog"}"\ninput_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`])(model):(model=>[`import torch\nfrom diffusers import DiffusionPipeline\n\n# switch to "mps" for apple devices\npipe = DiffusionPipeline.from_pretrained("${model.id}", dtype=torch.bfloat16, device_map="cuda")\n\nprompt = "${get_prompt_from_diffusers_model(model)??diffusersDefaultPrompt}"\nimage = pipe(prompt).images[0]`])(model),["pip install -U diffusers transformers accelerate",...codeSnippets]},_keras_hub_tasks_with_example={CausalLM:modelId=>`\nimport keras_hub\n\n# Load CausalLM model (optional: use half precision for inference)\ncausal_lm = keras_hub.models.CausalLM.from_preset("hf://${modelId}", dtype="bfloat16")\ncausal_lm.compile(sampler="greedy")  # (optional) specify a sampler\n\n# Generate text\ncausal_lm.generate("Keras: deep learning for", max_length=64)\n`,TextToImage:modelId=>`\nimport keras_hub\n\n# Load TextToImage model (optional: use half precision for inference)\ntext_to_image = keras_hub.models.TextToImage.from_preset("hf://${modelId}", dtype="bfloat16")\n\n# Generate images with a TextToImage model.\ntext_to_image.generate("Astronaut in a jungle")\n`,TextClassifier:modelId=>`\nimport keras_hub\n\n# Load TextClassifier model\ntext_classifier = keras_hub.models.TextClassifier.from_preset(\n    "hf://${modelId}",\n    num_classes=2,\n)\n# Fine-tune\ntext_classifier.fit(x=["Thilling adventure!", "Total snoozefest."], y=[1, 0])\n# Classify text\ntext_classifier.predict(["Not my cup of tea."])\n`,ImageClassifier:modelId=>`\nimport keras_hub\nimport keras\n\n# Load ImageClassifier model\nimage_classifier = keras_hub.models.ImageClassifier.from_preset(\n    "hf://${modelId}",\n    num_classes=2,\n)\n# Fine-tune\nimage_classifier.fit(\n    x=keras.random.randint((32, 64, 64, 3), 0, 256),\n    y=keras.random.randint((32, 1), 0, 2),\n)\n# Classify image\nimage_classifier.predict(keras.random.randint((1, 64, 64, 3), 0, 256))\n`},_keras_hub_task_without_example=(task,modelId)=>`\nimport keras_hub\n\n# Create a ${task} model\ntask = keras_hub.models.${task}.from_preset("hf://${modelId}")\n`;var GGMLFileQuantizationType,GGMLFileQuantizationType2,transformers=model=>{const info=model.transformersInfo;if(!info)return["# ‚ö†Ô∏è Type of model unknown"];const remote_code_snippet=model.tags.includes("custom_code")?", trust_remote_code=True":"",autoSnippet=[];if(info.processor){const processorVarName="AutoTokenizer"===info.processor?"tokenizer":"AutoFeatureExtractor"===info.processor?"extractor":"processor";autoSnippet.push("# Load model directly",`from transformers import ${info.processor}, ${info.auto_model}`,"",`${processorVarName} = ${info.processor}.from_pretrained("${model.id}"`+remote_code_snippet+")",`model = ${info.auto_model}.from_pretrained("${model.id}"`+remote_code_snippet+")"),model.tags.includes("conversational")&&(model=>void 0!==model.config?.tokenizer_config?.chat_template||void 0!==model.config?.processor_config?.chat_template||void 0!==model.config?.chat_template_jinja)(model)&&(model.tags.includes("image-text-to-text")?autoSnippet.push("messages = [",["    {",'        "role": "user",','        "content": [','            {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG"},','            {"type": "text", "text": "What animal is on the candy?"}',"        ]","    },"].join("\n"),"]"):autoSnippet.push("messages = [",'    {"role": "user", "content": "Who are you?"},',"]"),autoSnippet.push(`inputs = ${processorVarName}.apply_chat_template(`,"\tmessages,","\tadd_generation_prompt=True,","\ttokenize=True,","\treturn_dict=True,",'\treturn_tensors="pt",',").to(model.device)","","outputs = model.generate(**inputs, max_new_tokens=40)",`print(${processorVarName}.decode(outputs[0][inputs["input_ids"].shape[-1]:]))`))}else autoSnippet.push("# Load model directly",`from transformers import ${info.auto_model}`,`model = ${info.auto_model}.from_pretrained("${model.id}"`+remote_code_snippet+', dtype="auto")');if(model.pipeline_tag&&LIBRARY_TASK_MAPPING.transformers?.includes(model.pipeline_tag)){const pipelineSnippet=["# Use a pipeline as a high-level helper","from transformers import pipeline","",`pipe = pipeline("${model.pipeline_tag}", model="${model.id}"`+remote_code_snippet+")"];return model.tags.includes("conversational")?model.tags.includes("image-text-to-text")?(pipelineSnippet.push("messages = [",["    {",'        "role": "user",','        "content": [','            {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG"},','            {"type": "text", "text": "What animal is on the candy?"}',"        ]","    },"].join("\n"),"]"),pipelineSnippet.push("pipe(text=messages)")):(pipelineSnippet.push("messages = [",'    {"role": "user", "content": "Who are you?"},',"]"),pipelineSnippet.push("pipe(messages)")):"zero-shot-image-classification"===model.pipeline_tag?pipelineSnippet.push("pipe(",'    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png",','    candidate_labels=["animals", "humans", "landscape"],',")"):"image-classification"===model.pipeline_tag&&pipelineSnippet.push('pipe("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png")'),[pipelineSnippet.join("\n"),autoSnippet.join("\n")]}return[autoSnippet.join("\n")]},ultralytics=model=>{const versionTag=model.tags.find(tag=>tag.match(/^yolov\d+$/)),className=versionTag?`YOLOv${versionTag.slice(4)}`:"YOLOvXX";return[(versionTag?"":"# Couldn't find a valid YOLO version tag.\n# Replace XX with the correct version.\n")+`from ultralytics import ${className}\n\nmodel = ${className}.from_pretrained("${model.id}")\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)`]},pruna_diffusers=model=>diffusers(model).map(snippet=>snippet.replace(/\b\w*Pipeline\w*\b/g,"PrunaModel").replace(/from diffusers import ([^,\n]*PrunaModel[^,\n]*)/g,"").replace(/from diffusers import ([^,\n]+),?\s*([^,\n]*PrunaModel[^,\n]*)/g,"from diffusers import $1").replace(/from diffusers import\s*(\n|$)/g,"").replace(/from diffusers import PrunaModel/g,"from pruna import PrunaModel").replace(/from diffusers import ([^,\n]+), PrunaModel/g,"from diffusers import $1").replace(/from diffusers import PrunaModel, ([^,\n]+)/g,"from diffusers import $1").replace(/\n\n+/g,"\n").trim()),pruna_transformers=model=>{const info=model.transformersInfo;let processedSnippets=transformers(model).map(snippet=>snippet.replace(/from transformers import pipeline/g,"from pruna import PrunaModel").replace(/pipeline\([^)]*\)/g,`PrunaModel.from_pretrained("${model.id}")`));return info?.auto_model&&(processedSnippets=processedSnippets.map(snippet=>snippet.replace(new RegExp(`from transformers import ${info.auto_model}\n?`,"g"),"").replace(new RegExp(`${info.auto_model}.from_pretrained`,"g"),"PrunaModel.from_pretrained").replace(new RegExp(`^.*from.*import.*(, *${info.auto_model})+.*$`,"gm"),line=>line.replace(new RegExp(`, *${info.auto_model}`,"g"),"")))),processedSnippets},pruna_default=model=>[`from pruna import PrunaModel\nmodel = PrunaModel.from_pretrained("${model.id}")\n`],MODEL_LIBRARIES_UI_ELEMENTS={acestep:{prettyLabel:"ACE-Step",repoName:"ACE-Step",repoUrl:"https://github.com/ace-step/ACE-Step",filter:!1,countDownloads:'path:"ace_step_transformer/config.json"'},"adapter-transformers":{prettyLabel:"Adapters",repoName:"adapters",repoUrl:"https://github.com/Adapter-Hub/adapters",docsUrl:"https://huggingface.co/docs/hub/adapters",snippets:model=>[`from adapters import AutoAdapterModel\n\nmodel = AutoAdapterModel.from_pretrained("${model.config?.adapter_transformers?.model_name}")\nmodel.load_adapter("${model.id}", set_active=True)`],filter:!0,countDownloads:'path:"adapter_config.json"'},allennlp:{prettyLabel:"AllenNLP",repoName:"AllenNLP",repoUrl:"https://github.com/allenai/allennlp",docsUrl:"https://huggingface.co/docs/hub/allennlp",snippets:model=>model.tags.includes("question-answering")?(model=>[`import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path("hf://${model.id}")\npredictor_input = {"passage": "My name is Wolfgang and I live in Berlin", "question": "Where do I live?"}\npredictions = predictor.predict_json(predictor_input)`])(model):(model=>[`import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path("hf://${model.id}")`])(model),filter:!0},anemoi:{prettyLabel:"AnemoI",repoName:"AnemoI",repoUrl:"https://github.com/ecmwf/anemoi-inference",docsUrl:"https://anemoi.readthedocs.io/en/latest/",filter:!1,countDownloads:'path_extension:"ckpt"',snippets:model=>[`from anemoi.inference.runners.default import DefaultRunner\nfrom anemoi.inference.config.run import RunConfiguration\n# Create Configuration\nconfig = RunConfiguration(checkpoint = {"huggingface":"${model.id}"})\n# Load Runner\nrunner = DefaultRunner(config)`]},araclip:{prettyLabel:"AraClip",repoName:"AraClip",repoUrl:"https://huggingface.co/Arabic-Clip/araclip",filter:!1,snippets:model=>[`from araclip import AraClip\n\nmodel = AraClip.from_pretrained("${model.id}")`]},asteroid:{prettyLabel:"Asteroid",repoName:"Asteroid",repoUrl:"https://github.com/asteroid-team/asteroid",docsUrl:"https://huggingface.co/docs/hub/asteroid",snippets:model=>[`from asteroid.models import BaseModel\n\nmodel = BaseModel.from_pretrained("${model.id}")`],filter:!0,countDownloads:'path:"pytorch_model.bin"'},audiocraft:{prettyLabel:"Audiocraft",repoName:"audiocraft",repoUrl:"https://github.com/facebookresearch/audiocraft",snippets:model=>model.tags.includes("musicgen")?(model=>[`from audiocraft.models import MusicGen\n\nmodel = MusicGen.get_pretrained("${model.id}")\n\ndescriptions = ['happy rock', 'energetic EDM', 'sad jazz']\nwav = model.generate(descriptions)  # generates 3 samples.`])(model):model.tags.includes("audiogen")?(model=>[`from audiocraft.models import AudioGen\n\t\nmodel = AudioGen.get_pretrained("${model.id}")\nmodel.set_generation_params(duration=5)  # generate 5 seconds.\ndescriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']\nwav = model.generate(descriptions)  # generates 3 samples.`])(model):model.tags.includes("magnet")?(model=>[`from audiocraft.models import MAGNeT\n\t\nmodel = MAGNeT.get_pretrained("${model.id}")\n\ndescriptions = ['disco beat', 'energetic EDM', 'funky groove']\nwav = model.generate(descriptions)  # generates 3 samples.`])(model):["# Type of model unknown."],filter:!1,countDownloads:'path:"state_dict.bin"'},audioseal:{prettyLabel:"AudioSeal",repoName:"audioseal",repoUrl:"https://github.com/facebookresearch/audioseal",filter:!1,countDownloads:'path_extension:"pth"',snippets:model=>[`# Watermark Generator\nfrom audioseal import AudioSeal\n\nmodel = AudioSeal.load_generator("${model.id}")\n# pass a tensor (tensor_wav) of shape (batch, channels, samples) and a sample rate\nwav, sr = tensor_wav, 16000\n\t\nwatermark = model.get_watermark(wav, sr)\nwatermarked_audio = wav + watermark`,`# Watermark Detector\nfrom audioseal import AudioSeal\n\ndetector = AudioSeal.load_detector("${model.id}")\n\t\nresult, message = detector.detect_watermark(watermarked_audio, sr)`]},"bagel-mot":{prettyLabel:"Bagel",repoName:"Bagel",repoUrl:"https://github.com/ByteDance-Seed/Bagel/",filter:!1,countDownloads:'path:"llm_config.json"'},bboxmaskpose:{prettyLabel:"BBoxMaskPose",repoName:"BBoxMaskPose",repoUrl:"https://github.com/MiraPurkrabek/BBoxMaskPose",filter:!1,countDownloads:'path_extension:"pth"'},ben2:{prettyLabel:"BEN2",repoName:"BEN2",repoUrl:"https://github.com/PramaLLC/BEN2",snippets:model=>[`import requests\nfrom PIL import Image\nfrom ben2 import AutoModel\n\nurl = "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = AutoModel.from_pretrained("${model.id}")\nmodel.to("cuda").eval()\nforeground = model.inference(image)\n`],filter:!1},bertopic:{prettyLabel:"BERTopic",repoName:"BERTopic",repoUrl:"https://github.com/MaartenGr/BERTopic",snippets:model=>[`from bertopic import BERTopic\n\nmodel = BERTopic.load("${model.id}")`],filter:!0},big_vision:{prettyLabel:"Big Vision",repoName:"big_vision",repoUrl:"https://github.com/google-research/big_vision",filter:!1,countDownloads:'path_extension:"npz"'},birder:{prettyLabel:"Birder",repoName:"Birder",repoUrl:"https://gitlab.com/birder/birder",filter:!1,countDownloads:'path_extension:"pt"'},birefnet:{prettyLabel:"BiRefNet",repoName:"BiRefNet",repoUrl:"https://github.com/ZhengPeng7/BiRefNet",snippets:model=>[`# Option 1: use with transformers\n\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained("${model.id}", trust_remote_code=True)\n`,`# Option 2: use with BiRefNet\n\n# Install from https://github.com/ZhengPeng7/BiRefNet\n\nfrom models.birefnet import BiRefNet\nmodel = BiRefNet.from_pretrained("${model.id}")`],filter:!1},bm25s:{prettyLabel:"BM25S",repoName:"bm25s",repoUrl:"https://github.com/xhluca/bm25s",snippets:model=>[`from bm25s.hf import BM25HF\n\nretriever = BM25HF.load_from_hub("${model.id}")`],filter:!1,countDownloads:'path:"params.index.json"'},boltzgen:{prettyLabel:"BoltzGen",repoName:"BoltzGen",repoUrl:"https://github.com/HannesStark/boltzgen",filter:!1,countDownloads:'path:"boltzgen1_diverse.ckpt"'},champ:{prettyLabel:"Champ",repoName:"Champ",repoUrl:"https://github.com/fudan-generative-vision/champ",countDownloads:'path:"champ/motion_module.pth"'},chatterbox:{prettyLabel:"Chatterbox",repoName:"Chatterbox",repoUrl:"https://github.com/resemble-ai/chatterbox",snippets:()=>['# pip install chatterbox-tts\nimport torchaudio as ta\nfrom chatterbox.tts import ChatterboxTTS\n\nmodel = ChatterboxTTS.from_pretrained(device="cuda")\n\ntext = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy\'s Nexus in an epic late-game pentakill."\nwav = model.generate(text)\nta.save("test-1.wav", wav, model.sr)\n\n# If you want to synthesize with a different voice, specify the audio prompt\nAUDIO_PROMPT_PATH="YOUR_FILE.wav"\nwav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)\nta.save("test-2.wav", wav, model.sr)'],countDownloads:'path:"tokenizer.json"',filter:!1},chat_tts:{prettyLabel:"ChatTTS",repoName:"ChatTTS",repoUrl:"https://github.com/2noise/ChatTTS.git",snippets:()=>['import ChatTTS\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = ["PUT YOUR TEXT HERE",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)'],filter:!1,countDownloads:'path:"asset/GPT.pt"'},"chronos-forecasting":{prettyLabel:"Chronos",repoName:"Chronos",repoUrl:"https://github.com/amazon-science/chronos-forecasting",snippets:model=>["pip install chronos-forecasting",`import pandas as pd\nfrom chronos import BaseChronosPipeline\n\npipeline = BaseChronosPipeline.from_pretrained("${model.id}", device_map="cuda")\n\n# Load historical data\ncontext_df = pd.read_csv("https://autogluon.s3.us-west-2.amazonaws.com/datasets/timeseries/misc/AirPassengers.csv")\n\n# Generate predictions\npred_df = pipeline.predict_df(\n    context_df,\n    prediction_length=36,  # Number of steps to forecast\n    quantile_levels=[0.1, 0.5, 0.9],  # Quantiles for probabilistic forecast\n    id_column="item_id",  # Column identifying different time series\n    timestamp_column="Month",  # Column with datetime information\n    target="#Passengers",  # Column(s) with time series values to predict\n)`]},"cloud-agents":{prettyLabel:"Cloud Agents",repoName:"Cloud Agents",repoUrl:"https://huggingface.co/OpenPeerAI/Cloud-Agents",filter:!1,countDownloads:'path:"setup.py"'},colpali:{prettyLabel:"ColPali",repoName:"ColPali",repoUrl:"https://github.com/ManuelFay/colpali",filter:!1,countDownloads:'path:"adapter_config.json"'},comet:{prettyLabel:"COMET",repoName:"COMET",repoUrl:"https://github.com/Unbabel/COMET/",countDownloads:'path:"hparams.yaml"'},cosmos:{prettyLabel:"Cosmos",repoName:"Cosmos",repoUrl:"https://github.com/NVIDIA/Cosmos",countDownloads:'path:"config.json" OR path_extension:"pt"'},"cxr-foundation":{prettyLabel:"CXR Foundation",repoName:"cxr-foundation",repoUrl:"https://github.com/google-health/cxr-foundation",snippets:()=>["# pip install git+https://github.com/Google-Health/cxr-foundation.git#subdirectory=python\n\n# Load image as grayscale (Stillwaterising, CC0, via Wikimedia Commons)\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimg = Image.open(requests.get(image_url, headers={'User-Agent': 'Demo'}, stream=True).raw).convert('L')\n\n# Run inference\nfrom clientside.clients import make_hugging_face_client\ncxr_client = make_hugging_face_client('cxr_model')\nprint(cxr_client.get_image_embeddings_from_images([img]))"],filter:!1,countDownloads:'path:"precomputed_embeddings/embeddings.npz" OR path:"pax-elixr-b-text/saved_model.pb"'},deepforest:{prettyLabel:"DeepForest",repoName:"deepforest",docsUrl:"https://deepforest.readthedocs.io/en/latest/",repoUrl:"https://github.com/weecology/DeepForest"},"depth-anything-v2":{prettyLabel:"DepthAnythingV2",repoName:"Depth Anything V2",repoUrl:"https://github.com/DepthAnything/Depth-Anything-V2",snippets:model=>{let encoder,features,out_channels;return encoder="<ENCODER>",features="<NUMBER_OF_FEATURES>",out_channels="<OUT_CHANNELS>","depth-anything/Depth-Anything-V2-Small"===model.id?(encoder="vits",features="64",out_channels="[48, 96, 192, 384]"):"depth-anything/Depth-Anything-V2-Base"===model.id?(encoder="vitb",features="128",out_channels="[96, 192, 384, 768]"):"depth-anything/Depth-Anything-V2-Large"===model.id&&(encoder="vitl",features="256",out_channels="[256, 512, 1024, 1024"),[`\n# Install from https://github.com/DepthAnything/Depth-Anything-V2\n\n# Load the model and infer depth from an image\nimport cv2\nimport torch\n\nfrom depth_anything_v2.dpt import DepthAnythingV2\n\n# instantiate the model\nmodel = DepthAnythingV2(encoder="${encoder}", features=${features}, out_channels=${out_channels})\n\n# load the weights\nfilepath = hf_hub_download(repo_id="${model.id}", filename="depth_anything_v2_${encoder}.pth", repo_type="model")\nstate_dict = torch.load(filepath, map_location="cpu")\nmodel.load_state_dict(state_dict).eval()\n\nraw_img = cv2.imread("your/image/path")\ndepth = model.infer_image(raw_img) # HxW raw depth map in numpy\n    `]},filter:!1,countDownloads:'path_extension:"pth"'},"depth-pro":{prettyLabel:"Depth Pro",repoName:"Depth Pro",repoUrl:"https://github.com/apple/ml-depth-pro",countDownloads:'path_extension:"pt"',snippets:model=>[`# Download checkpoint\npip install huggingface-hub\nhuggingface-cli download --local-dir checkpoints ${model.id}`,'import depth_pro\n\n# Load model and preprocessing transform\nmodel, transform = depth_pro.create_model_and_transforms()\nmodel.eval()\n\n# Load and preprocess an image.\nimage, _, f_px = depth_pro.load_rgb("example.png")\nimage = transform(image)\n\n# Run inference.\nprediction = model.infer(image, f_px=f_px)\n\n# Results: 1. Depth in meters\ndepth = prediction["depth"]\n# Results: 2. Focal length in pixels\nfocallength_px = prediction["focallength_px"]'],filter:!1},"derm-foundation":{prettyLabel:"Derm Foundation",repoName:"derm-foundation",repoUrl:"https://github.com/google-health/derm-foundation",snippets:()=>['from huggingface_hub import from_pretrained_keras\nimport tensorflow as tf, requests\n\n# Load and format input\nIMAGE_URL = "https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png"\ninput_tensor = tf.train.Example(\n    features=tf.train.Features(\n        feature={\n            "image/encoded": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[requests.get(IMAGE_URL, stream=True).content])\n            )\n        }\n    )\n).SerializeToString()\n\n# Load model and run inference\nloaded_model = from_pretrained_keras("google/derm-foundation")\ninfer = loaded_model.signatures["serving_default"]\nprint(infer(inputs=tf.constant([input_tensor])))'],filter:!1,countDownloads:'path:"scin_dataset_precomputed_embeddings.npz" OR path:"saved_model.pb"'},"describe-anything":{prettyLabel:"Describe Anything",repoName:"Describe Anything",repoUrl:"https://github.com/NVlabs/describe-anything",snippets:model=>[`# pip install git+https://github.com/NVlabs/describe-anything\nfrom huggingface_hub import snapshot_download\nfrom dam import DescribeAnythingModel\n\nsnapshot_download(${model.id}, local_dir="checkpoints")\n\ndam = DescribeAnythingModel(\n\tmodel_path="checkpoints",\n\tconv_mode="v1",\n\tprompt_mode="focal_prompt",\n)`],filter:!1},"dia-tts":{prettyLabel:"Dia",repoName:"Dia",repoUrl:"https://github.com/nari-labs/dia",snippets:model=>[`import soundfile as sf\nfrom dia.model import Dia\n\nmodel = Dia.from_pretrained("${model.id}")\ntext = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."\noutput = model.generate(text)\n\nsf.write("simple.mp3", output, 44100)`],filter:!1},"diff-interpretation-tuning":{prettyLabel:"Diff Interpretation Tuning",repoName:"Diff Interpretation Tuning",repoUrl:"https://github.com/Aviously/diff-interpretation-tuning",filter:!1,countDownloads:'path_extension:"pt"'},diffree:{prettyLabel:"Diffree",repoName:"Diffree",repoUrl:"https://github.com/OpenGVLab/Diffree",filter:!1,countDownloads:'path:"diffree-step=000010999.ckpt"'},diffusers:{prettyLabel:"Diffusers",repoName:"ü§ó/diffusers",repoUrl:"https://github.com/huggingface/diffusers",docsUrl:"https://huggingface.co/docs/hub/diffusers",snippets:diffusers,filter:!0},diffusionkit:{prettyLabel:"DiffusionKit",repoName:"DiffusionKit",repoUrl:"https://github.com/argmaxinc/DiffusionKit",snippets:model=>{const sd3Snippet=`# Pipeline for Stable Diffusion 3\nfrom diffusionkit.mlx import DiffusionPipeline\n\npipeline = DiffusionPipeline(\n\tshift=3.0,\n\tuse_t5=False,\n\tmodel_version=${model.id},\n\tlow_memory_mode=True,\n\ta16=True,\n\tw16=True,\n)`,fluxSnippet=`# Pipeline for Flux\nfrom diffusionkit.mlx import FluxPipeline\n\npipeline = FluxPipeline(\n  shift=1.0,\n  model_version=${model.id},\n  low_memory_mode=True,\n  a16=True,\n  w16=True,\n)`,generateSnippet=`# Image Generation\nHEIGHT = 512\nWIDTH = 512\nNUM_STEPS = ${model.tags.includes("flux")?4:50}\nCFG_WEIGHT = ${model.tags.includes("flux")?0:5}\n\nimage, _ = pipeline.generate_image(\n  "a photo of a cat",\n  cfg_weight=CFG_WEIGHT,\n  num_steps=NUM_STEPS,\n  latent_size=(HEIGHT // 8, WIDTH // 8),\n)`;return[model.tags.includes("flux")?fluxSnippet:sd3Snippet,generateSnippet]}},doctr:{prettyLabel:"docTR",repoName:"doctr",repoUrl:"https://github.com/mindee/doctr"},cartesia_pytorch:{prettyLabel:"Cartesia Pytorch",repoName:"Cartesia Pytorch",repoUrl:"https://github.com/cartesia-ai/cartesia_pytorch",snippets:model=>[`# pip install --no-binary :all: cartesia-pytorch\nfrom cartesia_pytorch import ReneLMHeadModel\nfrom transformers import AutoTokenizer\n\nmodel = ReneLMHeadModel.from_pretrained("${model.id}")\ntokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-1B-hf")\n\nin_message = ["Rene Descartes was"]\ninputs = tokenizer(in_message, return_tensors="pt")\n\noutputs = model.generate(inputs.input_ids, max_length=50, top_k=100, top_p=0.99)\nout_message = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\nprint(out_message)\n)`]},cartesia_mlx:{prettyLabel:"Cartesia MLX",repoName:"Cartesia MLX",repoUrl:"https://github.com/cartesia-ai/cartesia_mlx",snippets:model=>[`import mlx.core as mx\nimport cartesia_mlx as cmx\n\nmodel = cmx.from_pretrained("${model.id}")\nmodel.set_dtype(mx.float32)   \n\nprompt = "Rene Descartes was"\n\nfor text in model.generate(\n    prompt,\n    max_tokens=500,\n    eval_every_n=5,\n    verbose=True,\n    top_p=0.99,\n    temperature=0.85,\n):\n    print(text, end="", flush=True)\n`]},clipscope:{prettyLabel:"clipscope",repoName:"clipscope",repoUrl:"https://github.com/Lewington-pitsos/clipscope",filter:!1,countDownloads:'path_extension:"pt"'},cosyvoice:{prettyLabel:"CosyVoice",repoName:"CosyVoice",repoUrl:"https://github.com/FunAudioLLM/CosyVoice",filter:!1,countDownloads:'path_extension:"onnx" OR path_extension:"pt"'},cotracker:{prettyLabel:"CoTracker",repoName:"CoTracker",repoUrl:"https://github.com/facebookresearch/co-tracker",filter:!1,countDownloads:'path_extension:"pth"'},edsnlp:{prettyLabel:"EDS-NLP",repoName:"edsnlp",repoUrl:"https://github.com/aphp/edsnlp",docsUrl:"https://aphp.github.io/edsnlp/latest/",filter:!1,snippets:model=>{const packageName=nameWithoutNamespace(model.id).replaceAll("-","_");return[`# Load it from the Hub directly\nimport edsnlp\nnlp = edsnlp.load("${model.id}")\n`,`# Or install it as a package\n!pip install git+https://huggingface.co/${model.id}\n\n# and import it as a module\nimport ${packageName}\n\nnlp = ${packageName}.load()  # or edsnlp.load("${packageName}")\n`]},countDownloads:'path_filename:"config" AND path_extension:"cfg"'},elm:{prettyLabel:"ELM",repoName:"elm",repoUrl:"https://github.com/slicex-ai/elm",filter:!1,countDownloads:'path_filename:"slicex_elm_config" AND path_extension:"json"'},espnet:{prettyLabel:"ESPnet",repoName:"ESPnet",repoUrl:"https://github.com/espnet/espnet",docsUrl:"https://huggingface.co/docs/hub/espnet",snippets:model=>model.tags.includes("text-to-speech")?(model=>[`from espnet2.bin.tts_inference import Text2Speech\n\nmodel = Text2Speech.from_pretrained("${model.id}")\n\nspeech, *_ = model("text to generate speech from")`])(model):model.tags.includes("automatic-speech-recognition")?(model=>[`from espnet2.bin.asr_inference import Speech2Text\n\nmodel = Speech2Text.from_pretrained(\n  "${model.id}"\n)\n\nspeech, rate = soundfile.read("speech.wav")\ntext, *_ = model(speech)[0]`])(model):["unknown model type (must be text-to-speech or automatic-speech-recognition)"],filter:!0},fairseq:{prettyLabel:"Fairseq",repoName:"fairseq",repoUrl:"https://github.com/pytorch/fairseq",snippets:model=>[`from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n    "${model.id}"\n)`],filter:!0},fastai:{prettyLabel:"fastai",repoName:"fastai",repoUrl:"https://github.com/fastai/fastai",docsUrl:"https://huggingface.co/docs/hub/fastai",snippets:model=>[`from huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai("${model.id}")`],filter:!0},fastprint:{prettyLabel:"Fast Print",repoName:"Fast Print",repoUrl:"https://huggingface.co/OpenPeerAI/FastPrint",countDownloads:'path_extension:"cs"'},fasttext:{prettyLabel:"fastText",repoName:"fastText",repoUrl:"https://fasttext.cc/",snippets:model=>[`from huggingface_hub import hf_hub_download\nimport fasttext\n\nmodel = fasttext.load_model(hf_hub_download("${model.id}", "model.bin"))`],filter:!0,countDownloads:'path_extension:"bin"'},fixer:{prettyLabel:"Fixer",repoName:"Fixer",repoUrl:"https://github.com/nv-tlabs/Fixer",filter:!1,countDownloads:'path:"pretrained/pretrained_fixer.pkl"'},flair:{prettyLabel:"Flair",repoName:"Flair",repoUrl:"https://github.com/flairNLP/flair",docsUrl:"https://huggingface.co/docs/hub/flair",snippets:model=>[`from flair.models import SequenceTagger\n\ntagger = SequenceTagger.load("${model.id}")`],filter:!0,countDownloads:'path:"pytorch_model.bin"'},fme:{prettyLabel:"Full Model Emulation",repoName:"Full Model Emulation",repoUrl:"https://github.com/ai2cm/ace",docsUrl:"https://ai2-climate-emulator.readthedocs.io/en/latest/",filter:!1,countDownloads:'path_extension:"tar"'},"gemma.cpp":{prettyLabel:"gemma.cpp",repoName:"gemma.cpp",repoUrl:"https://github.com/google/gemma.cpp",filter:!1,countDownloads:'path_extension:"sbs"'},"geometry-crafter":{prettyLabel:"GeometryCrafter",repoName:"GeometryCrafter",repoUrl:"https://github.com/TencentARC/GeometryCrafter",countDownloads:'path:"point_map_vae/diffusion_pytorch_model.safetensors"'},gliner:{prettyLabel:"GLiNER",repoName:"GLiNER",repoUrl:"https://github.com/urchade/GLiNER",snippets:model=>[`from gliner import GLiNER\n\nmodel = GLiNER.from_pretrained("${model.id}")`],filter:!1,countDownloads:'path:"gliner_config.json"'},gliner2:{prettyLabel:"GLiNER2",repoName:"GLiNER2",repoUrl:"https://github.com/fastino-ai/GLiNER2",snippets:model=>[`from gliner2 import GLiNER2\n\nmodel = GLiNER2.from_pretrained("${model.id}")\n\n# Extract entities\ntext = "Apple CEO Tim Cook announced iPhone 15 in Cupertino yesterday."\nresult = extractor.extract_entities(text, ["company", "person", "product", "location"])\n\nprint(result)`],filter:!1},"glyph-byt5":{prettyLabel:"Glyph-ByT5",repoName:"Glyph-ByT5",repoUrl:"https://github.com/AIGText/Glyph-ByT5",filter:!1,countDownloads:'path:"checkpoints/byt5_model.pt"'},grok:{prettyLabel:"Grok",repoName:"Grok",repoUrl:"https://github.com/xai-org/grok-1",filter:!1,countDownloads:'path:"ckpt/tensor00000_000" OR path:"ckpt-0/tensor00000_000"'},hallo:{prettyLabel:"Hallo",repoName:"Hallo",repoUrl:"https://github.com/fudan-generative-vision/hallo",countDownloads:'path:"hallo/net.pth"'},hermes:{prettyLabel:"HERMES",repoName:"HERMES",repoUrl:"https://github.com/LMD0311/HERMES",filter:!1,countDownloads:'path:"ckpt/hermes_final.pth"'},hezar:{prettyLabel:"Hezar",repoName:"Hezar",repoUrl:"https://github.com/hezarai/hezar",docsUrl:"https://hezarai.github.io/hezar",countDownloads:'path:"model_config.yaml" OR path:"embedding/embedding_config.yaml"'},htrflow:{prettyLabel:"HTRflow",repoName:"HTRflow",repoUrl:"https://github.com/AI-Riksarkivet/htrflow",docsUrl:"https://ai-riksarkivet.github.io/htrflow",snippets:model=>["# CLI usage\n# see docs: https://ai-riksarkivet.github.io/htrflow/latest/getting_started/quick_start.html\nhtrflow pipeline <path/to/pipeline.yaml> <path/to/image>",`# Python usage\nfrom htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.pipeline.steps import Task\nfrom htrflow.models.framework.model import ModelClass\n\npipeline = Pipeline(\n    [\n        Task(\n            ModelClass, {"model": "${model.id}"}, {}\n        ),\n    ])`]},"hunyuan-dit":{prettyLabel:"HunyuanDiT",repoName:"HunyuanDiT",repoUrl:"https://github.com/Tencent/HunyuanDiT",countDownloads:'path:"pytorch_model_ema.pt" OR path:"pytorch_model_distill.pt"'},"hunyuan3d-2":{prettyLabel:"Hunyuan3D-2",repoName:"Hunyuan3D-2",repoUrl:"https://github.com/Tencent/Hunyuan3D-2",countDownloads:'path_filename:"model_index" OR path_filename:"config"'},"hunyuanworld-voyager":{prettyLabel:"HunyuanWorld-voyager",repoName:"HunyuanWorld-voyager",repoUrl:"https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager"},"image-matching-models":{prettyLabel:"Image Matching Models",repoName:"Image Matching Models",repoUrl:"https://github.com/alexstoken/image-matching-models",filter:!1,countDownloads:'path_extension:"safetensors"'},imstoucan:{prettyLabel:"IMS Toucan",repoName:"IMS-Toucan",repoUrl:"https://github.com/DigitalPhonetics/IMS-Toucan",countDownloads:'path:"embedding_gan.pt" OR path:"Vocoder.pt" OR path:"ToucanTTS.pt"'},"index-tts":{prettyLabel:"IndexTTS",repoName:"IndexTTS",repoUrl:"https://github.com/index-tts/index-tts",snippets:model=>[`# Download model\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(${model.id}, local_dir="checkpoints")\n\nfrom indextts.infer import IndexTTS\n\n# Ensure config.yaml is present in the checkpoints directory\ntts = IndexTTS(model_dir="checkpoints", cfg_path="checkpoints/config.yaml")\n\nvoice = "path/to/your/reference_voice.wav"  # Path to the voice reference audio file\ntext = "Hello, how are you?"\noutput_path = "output_index.wav"\n\ntts.infer(voice, text, output_path)`],filter:!1},infinitetalk:{prettyLabel:"InfiniteTalk",repoName:"InfiniteTalk",repoUrl:"https://github.com/MeiGen-AI/InfiniteTalk",filter:!1,countDownloads:'path_extension:"safetensors"'},"infinite-you":{prettyLabel:"InfiniteYou",repoName:"InfiniteYou",repoUrl:"https://github.com/bytedance/InfiniteYou",filter:!1,countDownloads:'path:"infu_flux_v1.0/sim_stage1/image_proj_model.bin" OR path:"infu_flux_v1.0/aes_stage2/image_proj_model.bin"'},keras:{prettyLabel:"Keras",repoName:"Keras",repoUrl:"https://github.com/keras-team/keras",docsUrl:"https://huggingface.co/docs/hub/keras",snippets:model=>[`# Available backend options are: "jax", "torch", "tensorflow".\nimport os\nos.environ["KERAS_BACKEND"] = "jax"\n\t\nimport keras\n\nmodel = keras.saving.load_model("hf://${model.id}")\n`],filter:!0,countDownloads:'path:"config.json" OR path_extension:"keras"'},"tf-keras":{prettyLabel:"TF-Keras",repoName:"TF-Keras",repoUrl:"https://github.com/keras-team/tf-keras",docsUrl:"https://huggingface.co/docs/hub/tf-keras",snippets:model=>[`# Note: 'keras<3.x' or 'tf_keras' must be installed (legacy)\n# See https://github.com/keras-team/tf-keras for more details.\nfrom huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras("${model.id}")\n`],countDownloads:'path:"saved_model.pb"'},"keras-hub":{prettyLabel:"KerasHub",repoName:"KerasHub",repoUrl:"https://github.com/keras-team/keras-hub",docsUrl:"https://keras.io/keras_hub/",snippets:model=>{const modelId=model.id,tasks2=model.config?.keras_hub?.tasks??[],snippets2=[];for(const[task,snippet]of Object.entries(_keras_hub_tasks_with_example))tasks2.includes(task)&&snippets2.push(snippet(modelId));for(const task of tasks2)Object.keys(_keras_hub_tasks_with_example).includes(task)||snippets2.push(_keras_hub_task_without_example(task,modelId));return snippets2.push((modelId=>`\nimport keras_hub\n\n# Create a Backbone model unspecialized for any task\nbackbone = keras_hub.models.Backbone.from_preset("hf://${modelId}")\n`)(modelId)),snippets2},filter:!0},kernels:{prettyLabel:"Kernels",repoName:"Kernels",repoUrl:"https://github.com/huggingface/kernels",docsUrl:"https://huggingface.co/docs/kernels",snippets:model=>[`# !pip install kernels\n\nfrom kernels import get_kernel\n\nkernel = get_kernel("${model.id}")`],countDownloads:'path_filename:"_ops" AND path_extension:"py"'},"kimi-audio":{prettyLabel:"KimiAudio",repoName:"KimiAudio",repoUrl:"https://github.com/MoonshotAI/Kimi-Audio",snippets:model=>[`# Example usage for KimiAudio\n# pip install git+https://github.com/MoonshotAI/Kimi-Audio.git\n\nfrom kimia_infer.api.kimia import KimiAudio\n\nmodel = KimiAudio(model_path="${model.id}", load_detokenizer=True)\n\nsampling_params = {\n    "audio_temperature": 0.8,\n    "audio_top_k": 10,\n    "text_temperature": 0.0,\n    "text_top_k": 5,\n}\n\n# For ASR\nasr_audio = "asr_example.wav"\nmessages_asr = [\n    {"role": "user", "message_type": "text", "content": "Please transcribe the following audio:"},\n    {"role": "user", "message_type": "audio", "content": asr_audio}\n]\n_, text = model.generate(messages_asr, **sampling_params, output_type="text")\nprint(text)\n\n# For Q&A\nqa_audio = "qa_example.wav"\nmessages_conv = [{"role": "user", "message_type": "audio", "content": qa_audio}]\nwav, text = model.generate(messages_conv, **sampling_params, output_type="both")\nsf.write("output_audio.wav", wav.cpu().view(-1).numpy(), 24000)\nprint(text)\n`],filter:!1},kittentts:{prettyLabel:"KittenTTS",repoName:"KittenTTS",repoUrl:"https://github.com/KittenML/KittenTTS",snippets:model=>[`from kittentts import KittenTTS\nm = KittenTTS("${model.id}")\n\naudio = m.generate("This high quality TTS model works without a GPU")\n\n# Save the audio\nimport soundfile as sf\nsf.write('output.wav', audio, 24000)`]},kronos:{prettyLabel:"KRONOS",repoName:"KRONOS",repoUrl:"https://github.com/mahmoodlab/KRONOS",filter:!1,countDownloads:'path_extension:"pt"'},k2:{prettyLabel:"K2",repoName:"k2",repoUrl:"https://github.com/k2-fsa/k2"},"lightning-ir":{prettyLabel:"Lightning IR",repoName:"Lightning IR",repoUrl:"https://github.com/webis-de/lightning-ir",snippets:model=>model.tags.includes("bi-encoder")?[`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule\nmodel = BiEncoderModule("${model.id}")\n\nmodel.score("query", ["doc1", "doc2", "doc3"])`]:model.tags.includes("cross-encoder")?[`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import CrossEncoderModule\nmodel = CrossEncoderModule("${model.id}")\n\nmodel.score("query", ["doc1", "doc2", "doc3"])`]:[`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule, CrossEncoderModule\n\n# depending on the model type, use either BiEncoderModule or CrossEncoderModule\nmodel = BiEncoderModule("${model.id}") \n# model = CrossEncoderModule("${model.id}")\n\nmodel.score("query", ["doc1", "doc2", "doc3"])`]},litert:{prettyLabel:"LiteRT",repoName:"LiteRT",repoUrl:"https://github.com/google-ai-edge/LiteRT",filter:!1,countDownloads:'path_extension:"tflite"'},"litert-lm":{prettyLabel:"LiteRT-LM",repoName:"LiteRT-LM",repoUrl:"https://github.com/google-ai-edge/LiteRT-LM",filter:!1,countDownloads:'path_extension:"litertlm" OR path_extension:"task"'},lerobot:{prettyLabel:"LeRobot",repoName:"LeRobot",repoUrl:"https://github.com/huggingface/lerobot",docsUrl:"https://huggingface.co/docs/lerobot",filter:!1,snippets:model=>{if(model.tags.includes("smolvla")){const smolvlaSnippets=["# See https://github.com/huggingface/lerobot?tab=readme-ov-file#installation for more details\ngit clone https://github.com/huggingface/lerobot.git\ncd lerobot\npip install -e .[smolvla]",`# Launch finetuning on your dataset\npython lerobot/scripts/train.py \\\n--policy.path=${model.id} \\\n--dataset.repo_id=lerobot/svla_so101_pickplace \\ \n--batch_size=64 \\\n--steps=20000 \\\n--output_dir=outputs/train/my_smolvla \\\n--job_name=my_smolvla_training \\\n--policy.device=cuda \\\n--wandb.enable=true`];return"lerobot/smolvla_base"!==model.id&&smolvlaSnippets.push(`# Run the policy using the record function\t\npython -m lerobot.record \\\n  --robot.type=so101_follower \\\n  --robot.port=/dev/ttyACM0 \\ # <- Use your port\n  --robot.id=my_blue_follower_arm \\ # <- Use your robot id\n  --robot.cameras="{ front: {type: opencv, index_or_path: 8, width: 640, height: 480, fps: 30}}" \\ # <- Use your cameras\n  --dataset.single_task="Grasp a lego block and put it in the bin." \\ # <- Use the same task description you used in your dataset recording\n  --dataset.repo_id=HF_USER/dataset_name \\  # <- This will be the dataset name on HF Hub\n  --dataset.episode_time_s=50 \\\n  --dataset.num_episodes=10 \\\n  --policy.path=${model.id}`),smolvlaSnippets}return[]}},lightglue:{prettyLabel:"LightGlue",repoName:"LightGlue",repoUrl:"https://github.com/cvg/LightGlue",filter:!1,countDownloads:'path_extension:"pth" OR path:"config.json"'},liveportrait:{prettyLabel:"LivePortrait",repoName:"LivePortrait",repoUrl:"https://github.com/KwaiVGI/LivePortrait",filter:!1,countDownloads:'path:"liveportrait/landmark.onnx"'},"llama-cpp-python":{prettyLabel:"llama-cpp-python",repoName:"llama-cpp-python",repoUrl:"https://github.com/abetlen/llama-cpp-python",snippets:model=>{const snippets2=[`# !pip install llama-cpp-python\n\nfrom llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n\trepo_id="${model.id}",\n\tfilename="{{GGUF_FILE}}",\n)\n`];if(model.tags.includes("conversational")){const messages=getModelInputSnippet(model);snippets2.push(`llm.create_chat_completion(\n\tmessages = ${function(messages,opts){let messagesStr=JSON.stringify(messages,null,"\t");return opts?.indent&&(messagesStr=messagesStr.replaceAll("\n",`\n${opts.indent}`)),opts?.attributeKeyQuotes||(messagesStr=messagesStr.replace(/"([^"]+)":/g,"$1:")),opts?.customContentEscaper&&(messagesStr=opts.customContentEscaper(messagesStr)),messagesStr}(messages,{attributeKeyQuotes:!0,indent:"\t"})}\n)`)}else snippets2.push('output = llm(\n\t"Once upon a time,",\n\tmax_tokens=512,\n\techo=True\n)\nprint(output)');return snippets2}},"mini-omni2":{prettyLabel:"Mini-Omni2",repoName:"Mini-Omni2",repoUrl:"https://github.com/gpt-omni/mini-omni2",countDownloads:'path:"model_config.yaml"'},mindspore:{prettyLabel:"MindSpore",repoName:"mindspore",repoUrl:"https://github.com/mindspore-ai/mindspore"},"magi-1":{prettyLabel:"MAGI-1",repoName:"MAGI-1",repoUrl:"https://github.com/SandAI-org/MAGI-1",countDownloads:'path:"ckpt/vae/config.json"'},"magenta-realtime":{prettyLabel:"Magenta RT",repoName:"Magenta RT",repoUrl:"https://github.com/magenta/magenta-realtime",countDownloads:'path:"checkpoints/llm_base_x4286_c1860k.tar" OR path:"checkpoints/llm_large_x3047_c1860k.tar" OR path:"checkpoints/llm_large_x3047_c1860k/checkpoint"'},"mamba-ssm":{prettyLabel:"MambaSSM",repoName:"MambaSSM",repoUrl:"https://github.com/state-spaces/mamba",filter:!1,snippets:model=>[`from mamba_ssm import MambaLMHeadModel\n\nmodel = MambaLMHeadModel.from_pretrained("${model.id}")`]},"mars5-tts":{prettyLabel:"MARS5-TTS",repoName:"MARS5-TTS",repoUrl:"https://github.com/Camb-ai/MARS5-TTS",filter:!1,countDownloads:'path:"mars5_ar.safetensors"',snippets:model=>[`# Install from https://github.com/Camb-ai/MARS5-TTS\n\nfrom inference import Mars5TTS\nmars5 = Mars5TTS.from_pretrained("${model.id}")`]},matanyone:{prettyLabel:"MatAnyone",repoName:"MatAnyone",repoUrl:"https://github.com/pq-yang/MatAnyone",snippets:model=>[`# Install from https://github.com/pq-yang/MatAnyone.git\n\nfrom matanyone.model.matanyone import MatAnyone\nmodel = MatAnyone.from_pretrained("${model.id}")`,`\nfrom matanyone import InferenceCore\nprocessor = InferenceCore("${model.id}")`],filter:!1},"mesh-anything":{prettyLabel:"MeshAnything",repoName:"MeshAnything",repoUrl:"https://github.com/buaacyw/MeshAnything",filter:!1,countDownloads:'path:"MeshAnything_350m.pth"',snippets:()=>["# Install from https://github.com/buaacyw/MeshAnything.git\n\nfrom MeshAnything.models.meshanything import MeshAnything\n\n# refer to https://github.com/buaacyw/MeshAnything/blob/main/main.py#L91 on how to define args\n# and https://github.com/buaacyw/MeshAnything/blob/main/app.py regarding usage\nmodel = MeshAnything(args)"]},merlin:{prettyLabel:"Merlin",repoName:"Merlin",repoUrl:"https://github.com/StanfordMIMI/Merlin",filter:!1,countDownloads:'path_extension:"pt"'},medvae:{prettyLabel:"MedVAE",repoName:"MedVAE",repoUrl:"https://github.com/StanfordMIMI/MedVAE",filter:!1,countDownloads:'path_extension:"ckpt"'},mitie:{prettyLabel:"MITIE",repoName:"MITIE",repoUrl:"https://github.com/mit-nlp/MITIE",countDownloads:'path_filename:"total_word_feature_extractor"'},"ml-agents":{prettyLabel:"ml-agents",repoName:"ml-agents",repoUrl:"https://github.com/Unity-Technologies/ml-agents",docsUrl:"https://huggingface.co/docs/hub/ml-agents",snippets:model=>[`mlagents-load-from-hf --repo-id="${model.id}" --local-dir="./download: string[]s"`],filter:!0,countDownloads:'path_extension:"onnx"'},mlx:{prettyLabel:"MLX",repoName:"MLX",repoUrl:"https://github.com/ml-explore/mlx-examples/tree/main",snippets:model=>"image-text-to-text"===model.pipeline_tag?(model=>[`# Make sure mlx-vlm is installed\n# pip install --upgrade mlx-vlm\n\nfrom mlx_vlm import load, generate\nfrom mlx_vlm.prompt_utils import apply_chat_template\nfrom mlx_vlm.utils import load_config\n\n# Load the model\nmodel, processor = load("${model.id}")\nconfig = load_config("${model.id}")\n\n# Prepare input\nimage = ["http://images.cocodataset.org/val2017/000000039769.jpg"]\nprompt = "Describe this image."\n\n# Apply chat template\nformatted_prompt = apply_chat_template(\n    processor, config, prompt, num_images=1\n)\n\n# Generate output\noutput = generate(model, processor, formatted_prompt, image)\nprint(output)`])(model):"text-generation"===model.pipeline_tag?model.tags.includes("conversational")?(model=>[`# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load("${model.id}")\n\nprompt = "Write a story about Einstein"\nmessages = [{"role": "user", "content": prompt}]\nprompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`])(model):(model=>[`# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n# if on a CUDA device, also pip install mlx[cuda]\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load("${model.id}")\n\nprompt = "Once upon a time in"\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`])(model):(model=>[`# Download the model from the Hub\npip install huggingface_hub[hf_xet]\n\nhuggingface-cli download --local-dir ${nameWithoutNamespace(model.id)} ${model.id}`])(model),filter:!0},"mlx-image":{prettyLabel:"mlx-image",repoName:"mlx-image",repoUrl:"https://github.com/riccardomusmeci/mlx-image",docsUrl:"https://huggingface.co/docs/hub/mlx-image",snippets:model=>[`from mlxim.model import create_model\n\nmodel = create_model(${model.id})`],filter:!1,countDownloads:'path:"model.safetensors"'},"mlc-llm":{prettyLabel:"MLC-LLM",repoName:"MLC-LLM",repoUrl:"https://github.com/mlc-ai/mlc-llm",docsUrl:"https://llm.mlc.ai/docs/",filter:!1,countDownloads:'path:"mlc-chat-config.json"'},model2vec:{prettyLabel:"Model2Vec",repoName:"model2vec",repoUrl:"https://github.com/MinishLab/model2vec",snippets:model=>[`from model2vec import StaticModel\n\nmodel = StaticModel.from_pretrained("${model.id}")`],filter:!1},moshi:{prettyLabel:"Moshi",repoName:"Moshi",repoUrl:"https://github.com/kyutai-labs/moshi",filter:!1,countDownloads:'path:"tokenizer-e351c8d8-checkpoint125.safetensors"'},mtvcraft:{prettyLabel:"MTVCraft",repoName:"MTVCraft",repoUrl:"https://github.com/baaivision/MTVCraft",filter:!1,countDownloads:'path:"vae/3d-vae.pt"'},nemo:{prettyLabel:"NeMo",repoName:"NeMo",repoUrl:"https://github.com/NVIDIA/NeMo",snippets:model=>{let command;return model.tags.includes("automatic-speech-recognition")&&(command=((domain,model)=>{if("ASR"===domain)return[`import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained("${model.id}")\n\ntranscriptions = asr_model.transcribe(["file.wav"])`]})("ASR",model)),command??["# tag did not correspond to a valid NeMo domain."]},filter:!0,countDownloads:'path_extension:"nemo" OR path:"model_config.yaml" OR path_extension:"json"'},"open-oasis":{prettyLabel:"open-oasis",repoName:"open-oasis",repoUrl:"https://github.com/etched-ai/open-oasis",countDownloads:'path:"oasis500m.safetensors"'},open_clip:{prettyLabel:"OpenCLIP",repoName:"OpenCLIP",repoUrl:"https://github.com/mlfoundations/open_clip",snippets:model=>[`import open_clip\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:${model.id}')\ntokenizer = open_clip.get_tokenizer('hf-hub:${model.id}')`],filter:!0,countDownloads:'path:"open_clip_model.safetensors"\n\t\t\tOR path:"model.safetensors"\n\t\t\tOR path:"open_clip_pytorch_model.bin"\n\t\t\tOR path:"pytorch_model.bin"'},openpeerllm:{prettyLabel:"OpenPeerLLM",repoName:"OpenPeerLLM",repoUrl:"https://huggingface.co/openpeerai/openpeerllm",docsUrl:"https://huggingface.co/OpenPeerAI/OpenPeerLLM/blob/main/README.md",countDownloads:'path:".meta-huggingface.json"',filter:!1},"open-sora":{prettyLabel:"Open-Sora",repoName:"Open-Sora",repoUrl:"https://github.com/hpcaitech/Open-Sora",filter:!1,countDownloads:'path:"Open_Sora_v2.safetensors"'},outetts:{prettyLabel:"OuteTTS",repoName:"OuteTTS",repoUrl:"https://github.com/edwko/OuteTTS",snippets:model=>{const t=model.tags??[];return t.includes("gguf")||t.includes("onnx")?[]:[`\n  import outetts\n  \n  enum = outetts.Models("${model.id}".split("/", 1)[1])       # VERSION_1_0_SIZE_1B\n  cfg  = outetts.ModelConfig.auto_config(enum, outetts.Backend.HF)\n  tts  = outetts.Interface(cfg)\n  \n  speaker = tts.load_default_speaker("EN-FEMALE-1-NEUTRAL")\n  tts.generate(\n\t  outetts.GenerationConfig(\n\t\t  text="Hello there, how are you doing?",\n\t\t  speaker=speaker,\n\t  )\n  ).save("output.wav")\n  `]},filter:!1},paddlenlp:{prettyLabel:"paddlenlp",repoName:"PaddleNLP",repoUrl:"https://github.com/PaddlePaddle/PaddleNLP",docsUrl:"https://huggingface.co/docs/hub/paddlenlp",snippets:model=>{if(model.config?.architectures?.[0]){const architecture=model.config.architectures[0];return[[`from paddlenlp.transformers import AutoTokenizer, ${architecture}`,"",`tokenizer = AutoTokenizer.from_pretrained("${model.id}", from_hf_hub=True)`,`model = ${architecture}.from_pretrained("${model.id}", from_hf_hub=True)`].join("\n")]}return[["# ‚ö†Ô∏è Type of model unknown","from paddlenlp.transformers import AutoTokenizer, AutoModel","",`tokenizer = AutoTokenizer.from_pretrained("${model.id}", from_hf_hub=True)`,`model = AutoModel.from_pretrained("${model.id}", from_hf_hub=True)`].join("\n")]},filter:!0,countDownloads:'path:"model_config.json"'},PaddleOCR:{prettyLabel:"PaddleOCR",repoName:"PaddleOCR",repoUrl:"https://github.com/PaddlePaddle/PaddleOCR",docsUrl:"https://www.paddleocr.ai/",snippets:model=>{const mapping={textline_detection:{className:"TextDetection"},textline_recognition:{className:"TextRecognition"},seal_text_detection:{className:"SealTextDetection"},doc_img_unwarping:{className:"TextImageUnwarping"},doc_img_orientation_classification:{className:"DocImgOrientationClassification"},textline_orientation_classification:{className:"TextLineOrientationClassification"},chart_parsing:{className:"ChartParsing"},formula_recognition:{className:"FormulaRecognition"},layout_detection:{className:"LayoutDetection"},table_cells_detection:{className:"TableCellsDetection"},wired_table_classification:{className:"TableClassification"},table_structure_recognition:{className:"TableStructureRecognition"}};if(model.tags.includes("doc_vlm"))return[`# 1. See https://www.paddlepaddle.org.cn/en/install to install paddlepaddle\n# 2. pip install paddleocr\n\nfrom paddleocr import DocVLM\nmodel = DocVLM(model_name="${nameWithoutNamespace(model.id)}")\noutput = model.predict(\n    input={"image": "path/to/image.png", "query": "Parsing this image and output the content in Markdown format."},\n    batch_size=1\n)\nfor res in output:\n    res.print()\n    res.save_to_json(save_path="./output/res.json")`];if(model.tags.includes("document-parse"))return['# See https://www.paddleocr.ai/latest/version3.x/pipeline_usage/PaddleOCR-VL.html to installation\n\nfrom paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict("path/to/document_image.png")\nfor res in output:\n\tres.print()\n\tres.save_to_json(save_path="output")\n\tres.save_to_markdown(save_path="output")'];for(const tag of model.tags)if(tag in mapping){const{className:className}=mapping[tag];return[`# 1. See https://www.paddlepaddle.org.cn/en/install to install paddlepaddle\n# 2. pip install paddleocr\n\nfrom paddleocr import ${className}\nmodel = ${className}(model_name="${nameWithoutNamespace(model.id)}")\noutput = model.predict(input="path/to/image.png", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path="./output/")\n    res.save_to_json(save_path="./output/res.json")`]}return["# Please refer to the document for information on how to use the model. \n# https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/module_usage/module_overview.html"]},filter:!0,countDownloads:'path_extension:"safetensors" OR path:"inference.pdiparams"'},peft:{prettyLabel:"PEFT",repoName:"PEFT",repoUrl:"https://github.com/huggingface/peft",snippets:model=>{const{base_model_name_or_path:peftBaseModel,task_type:peftTaskType}=model.config?.peft??{},pefttask=(peftTaskType=>{switch(peftTaskType){case"CAUSAL_LM":return"CausalLM";case"SEQ_2_SEQ_LM":return"Seq2SeqLM";case"TOKEN_CLS":return"TokenClassification";case"SEQ_CLS":return"SequenceClassification";default:return}})(peftTaskType);return pefttask?peftBaseModel?[`from peft import PeftModel\nfrom transformers import AutoModelFor${pefttask}\n\nbase_model = AutoModelFor${pefttask}.from_pretrained("${peftBaseModel}")\nmodel = PeftModel.from_pretrained(base_model, "${model.id}")`]:["Base model is not found."]:["Task type is invalid."]},filter:!0,countDownloads:'path:"adapter_config.json"'},"perception-encoder":{prettyLabel:"PerceptionEncoder",repoName:"PerceptionModels",repoUrl:"https://github.com/facebookresearch/perception_models",filter:!1,snippets:model=>{const clip_model=`# Use PE-Core models as CLIP models\nimport core.vision_encoder.pe as pe\n\nmodel = pe.CLIP.from_config("${model.id}", pretrained=True)`,vision_encoder=`# Use any PE model as a vision encoder\nimport core.vision_encoder.pe as pe\n\nmodel = pe.VisionTransformer.from_config("${model.id}", pretrained=True)`;return model.id.includes("Core")?[clip_model,vision_encoder]:[vision_encoder]},countDownloads:'path_extension:"pt"'},"phantom-wan":{prettyLabel:"Phantom",repoName:"Phantom",repoUrl:"https://github.com/Phantom-video/Phantom",snippets:model=>[`from huggingface_hub import snapshot_download\nfrom phantom_wan import WANI2V, configs\n\ncheckpoint_dir = snapshot_download("${model.id}")\nwan_i2v = WanI2V(\n            config=configs.WAN_CONFIGS['i2v-14B'],\n            checkpoint_dir=checkpoint_dir,\n        )\n video = wan_i2v.generate(text_prompt, image_prompt)`],filter:!1,countDownloads:'path_extension:"pth"'},"pruna-ai":{prettyLabel:"Pruna AI",repoName:"Pruna AI",repoUrl:"https://github.com/PrunaAI/pruna",snippets:model=>{let snippets2;snippets2=model.tags.includes("diffusers")?pruna_diffusers(model):model.tags.includes("transformers")?pruna_transformers(model):pruna_default(model);return snippets2=snippets2.map(snippet=>/^from pruna import PrunaModel/m.test(snippet)?snippet:`from pruna import PrunaModel\n${snippet}`),model.tags.includes("pruna_pro-ai")?snippets2.map(snippet=>snippet.replace(/\bpruna\b/g,"pruna_pro").replace(/\bPrunaModel\b/g,"PrunaProModel")):snippets2},docsUrl:"https://docs.pruna.ai"},pxia:{prettyLabel:"pxia",repoName:"pxia",repoUrl:"https://github.com/not-lain/pxia",snippets:model=>[`from pxia import AutoModel\n\nmodel = AutoModel.from_pretrained("${model.id}")`],filter:!1},"pyannote-audio":{prettyLabel:"pyannote.audio",repoName:"pyannote-audio",repoUrl:"https://github.com/pyannote/pyannote-audio",snippets:model=>model.tags.includes("pyannote-audio-pipeline")?(model=>[`from pyannote.audio import Pipeline\n  \npipeline = Pipeline.from_pretrained("${model.id}")\n\n# inference on the whole file\npipeline("file.wav")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\n\nfrom pyannote.audio import Audio\nwaveform, sample_rate = Audio().crop("file.wav", excerpt)\npipeline({"waveform": waveform, "sample_rate": sample_rate})`])(model):(model=>[`from pyannote.audio import Model, Inference\n\nmodel = Model.from_pretrained("${model.id}")\ninference = Inference(model)\n\n# inference on the whole file\ninference("file.wav")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\ninference.crop("file.wav", excerpt)`])(model),filter:!0},"py-feat":{prettyLabel:"Py-Feat",repoName:"Py-Feat",repoUrl:"https://github.com/cosanlab/py-feat",docsUrl:"https://py-feat.org/",filter:!1},pythae:{prettyLabel:"pythae",repoName:"pythae",repoUrl:"https://github.com/clementchadebec/benchmark_VAE",snippets:model=>[`from pythae.models import AutoModel\n\nmodel = AutoModel.load_from_hf_hub("${model.id}")`],filter:!1},quantumpeer:{prettyLabel:"QuantumPeer",repoName:"QuantumPeer",repoUrl:"https://github.com/OpenPeer-AI/QuantumPeer",filter:!1,countDownloads:'path_extension:"setup.py"'},recurrentgemma:{prettyLabel:"RecurrentGemma",repoName:"recurrentgemma",repoUrl:"https://github.com/google-deepmind/recurrentgemma",filter:!1,countDownloads:'path:"tokenizer.model"'},relik:{prettyLabel:"Relik",repoName:"Relik",repoUrl:"https://github.com/SapienzaNLP/relik",snippets:model=>[`from relik import Relik\n \nrelik = Relik.from_pretrained("${model.id}")`],filter:!1},refiners:{prettyLabel:"Refiners",repoName:"Refiners",repoUrl:"https://github.com/finegrain-ai/refiners",docsUrl:"https://refine.rs/",filter:!1,countDownloads:'path:"model.safetensors"'},renderformer:{prettyLabel:"RenderFormer",repoName:"RenderFormer",repoUrl:"https://github.com/microsoft/renderformer",snippets:model=>[`# Install from https://github.com/microsoft/renderformer\n\nfrom renderformer import RenderFormerRenderingPipeline\npipeline = RenderFormerRenderingPipeline.from_pretrained("${model.id}")`],filter:!1},reverb:{prettyLabel:"Reverb",repoName:"Reverb",repoUrl:"https://github.com/revdotcom/reverb",filter:!1},rkllm:{prettyLabel:"RKLLM",repoName:"RKLLM",repoUrl:"https://github.com/airockchip/rknn-llm",countDownloads:'path_extension:"rkllm"'},saelens:{prettyLabel:"SAELens",repoName:"SAELens",repoUrl:"https://github.com/jbloomAus/SAELens",snippets:()=>['# pip install sae-lens\nfrom sae_lens import SAE\n\nsae, cfg_dict, sparsity = SAE.from_pretrained(\n    release = "RELEASE_ID", # e.g., "gpt2-small-res-jb". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n    sae_id = "SAE_ID", # e.g., "blocks.8.hook_resid_pre". Won\'t always be a hook point\n)'],filter:!1},sam2:{prettyLabel:"sam2",repoName:"sam2",repoUrl:"https://github.com/facebookresearch/segment-anything-2",filter:!1,snippets:model=>[`# Use SAM2 with images\nimport torch\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\npredictor = SAM2ImagePredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):\n    predictor.set_image(<your_image>)\n    masks, _, _ = predictor.predict(<input_prompts>)`,`# Use SAM2 with videos\nimport torch\nfrom sam2.sam2_video_predictor import SAM2VideoPredictor\n\t\npredictor = SAM2VideoPredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):\n    state = predictor.init_state(<your_video>)\n\n    # add new prompts and instantly get the output on the same frame\n    frame_idx, object_ids, masks = predictor.add_new_points(state, <your_prompts>):\n\n    # propagate the prompts to get masklets throughout the video\n    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n        ...`],countDownloads:'path_extension:"pt"'},"sample-factory":{prettyLabel:"sample-factory",repoName:"sample-factory",repoUrl:"https://github.com/alex-petrenko/sample-factory",docsUrl:"https://huggingface.co/docs/hub/sample-factory",snippets:model=>[`python -m sample_factory.huggingface.load_from_hub -r ${model.id} -d ./train_dir`],filter:!0,countDownloads:'path:"cfg.json"'},"sap-rpt-1-oss":{prettyLabel:"sap-rpt-1-oss",repoName:"sap-rpt-1-oss",repoUrl:"https://github.com/SAP-samples/sap-rpt-1-oss",countDownloads:'path_extension:"pt"',snippets:()=>["pip install git+https://github.com/SAP-samples/sap-rpt-1-oss",'# Run a classification task\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sap_rpt_oss import SAP_RPT_OSS_Classifier\n\n# Load sample data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize a classifier, 8k context and 8-fold bagging gives best performance, reduce if running out of memory\nclf = SAP_RPT_OSS_Classifier(max_context_size=8192, bagging=8)\n\nclf.fit(X_train, y_train)\n\n# Predict probabilities\nprediction_probabilities = clf.predict_proba(X_test)\n# Predict labels\npredictions = clf.predict(X_test)\nprint("Accuracy", accuracy_score(y_test, predictions))','# Run a regression task\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sap_rpt_oss import SAP_RPT_OSS_Regressor\n\n# Load sample data\ndf = fetch_openml(data_id=531, as_frame=True)\nX = df.data\ny = df.target.astype(float)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize the regressor, 8k context and 8-fold bagging gives best performance, reduce if running out of memory\nregressor = SAP_RPT_OSS_Regressor(max_context_size=8192, bagging=8)\n\nregressor.fit(X_train, y_train)\n\n# Predict on the test set\npredictions = regressor.predict(X_test)\n\nr2 = r2_score(y_test, predictions)\nprint("R¬≤ Score:", r2)']},sapiens:{prettyLabel:"sapiens",repoName:"sapiens",repoUrl:"https://github.com/facebookresearch/sapiens",filter:!1,countDownloads:'path_extension:"pt2" OR path_extension:"pth" OR path_extension:"onnx"'},seedvr:{prettyLabel:"SeedVR",repoName:"SeedVR",repoUrl:"https://github.com/ByteDance-Seed/SeedVR",filter:!1,countDownloads:'path_extension:"pth"'},"self-forcing":{prettyLabel:"SelfForcing",repoName:"SelfForcing",repoUrl:"https://github.com/guandeh17/Self-Forcing",filter:!1,countDownloads:'path_extension:"pt"'},"sentence-transformers":{prettyLabel:"sentence-transformers",repoName:"sentence-transformers",repoUrl:"https://github.com/UKPLab/sentence-transformers",docsUrl:"https://huggingface.co/docs/hub/sentence-transformers",snippets:model=>{const remote_code_snippet=model.tags.includes("custom_code")?", trust_remote_code=True":"";if(model.tags.includes("PyLate"))return[`from pylate import models\n\nqueries = [\n    "Which planet is known as the Red Planet?",\n    "What is the largest planet in our solar system?",\n]\n\ndocuments = [\n    ["Mars is the Red Planet.", "Venus is Earth's twin."],\n    ["Jupiter is the largest planet.", "Saturn has rings."],\n]\n\nmodel = models.ColBERT(model_name_or_path="${model.id}")\n\nqueries_emb = model.encode(queries, is_query=True)\ndocs_emb = model.encode(documents, is_query=False)`];if(model.tags.includes("cross-encoder")||"text-ranking"==model.pipeline_tag)return[`from sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder("${model.id}"${remote_code_snippet})\n\nquery = "Which planet is known as the Red Planet?"\npassages = [\n\t"Venus is often called Earth's twin because of its similar size and proximity.",\n\t"Mars, known for its reddish appearance, is often referred to as the Red Planet.",\n\t"Jupiter, the largest planet in our solar system, has a prominent red spot.",\n\t"Saturn, famous for its rings, is sometimes mistaken for the Red Planet."\n]\n\nscores = model.predict([(query, passage) for passage in passages])\nprint(scores)`];const exampleSentences=function(model){const widgetExample=model.widgetData?.[0];if(widgetExample?.source_sentence&&widgetExample?.sentences?.length)return[widgetExample.source_sentence,...widgetExample.sentences]}(model)??["The weather is lovely today.","It's so sunny outside!","He drove to the stadium."];return[`from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("${model.id}"${remote_code_snippet})\n\nsentences = ${JSON.stringify(exampleSentences,null,4)}\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [${exampleSentences.length}, ${exampleSentences.length}]`]},filter:!0},setfit:{prettyLabel:"setfit",repoName:"setfit",repoUrl:"https://github.com/huggingface/setfit",docsUrl:"https://huggingface.co/docs/hub/setfit",snippets:model=>[`from setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained("${model.id}")`],filter:!0},sklearn:{prettyLabel:"Scikit-learn",repoName:"Scikit-learn",repoUrl:"https://github.com/scikit-learn/scikit-learn",snippets:model=>{if(model.tags.includes("skops")){const skopsmodelFile=model.config?.sklearn?.model?.file,skopssaveFormat=model.config?.sklearn?.model_format;return skopsmodelFile?"pickle"===skopssaveFormat?((model,modelFile)=>[`import joblib\nfrom skops.hub_utils import download\ndownload("${model.id}", "path_to_folder")\nmodel = joblib.load(\n\t"${modelFile}"\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`])(model,skopsmodelFile):((model,modelFile)=>[`from skops.hub_utils import download\nfrom skops.io import load\ndownload("${model.id}", "path_to_folder")\n# make sure model file is in skops format\n# if model is a pickle file, make sure it's from a source you trust\nmodel = load("path_to_folder/${modelFile}")`])(model,skopsmodelFile):["# ‚ö†Ô∏è Model filename not specified in config.json"]}return(model=>[`from huggingface_hub import hf_hub_download\nimport joblib\nmodel = joblib.load(\n\thf_hub_download("${model.id}", "sklearn_model.joblib")\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`])(model)},filter:!0,countDownloads:'path:"sklearn_model.joblib"'},spacy:{prettyLabel:"spaCy",repoName:"spaCy",repoUrl:"https://github.com/explosion/spaCy",docsUrl:"https://huggingface.co/docs/hub/spacy",snippets:model=>[`!pip install https://huggingface.co/${model.id}/resolve/main/${nameWithoutNamespace(model.id)}-any-py3-none-any.whl\n\n# Using spacy.load().\nimport spacy\nnlp = spacy.load("${nameWithoutNamespace(model.id)}")\n\n# Importing as module.\nimport ${nameWithoutNamespace(model.id)}\nnlp = ${nameWithoutNamespace(model.id)}.load()`],filter:!0,countDownloads:'path_extension:"whl"'},"span-marker":{prettyLabel:"SpanMarker",repoName:"SpanMarkerNER",repoUrl:"https://github.com/tomaarsen/SpanMarkerNER",docsUrl:"https://huggingface.co/docs/hub/span_marker",snippets:model=>[`from span_marker import SpanMarkerModel\n\nmodel = SpanMarkerModel.from_pretrained("${model.id}")`],filter:!0},speechbrain:{prettyLabel:"speechbrain",repoName:"speechbrain",repoUrl:"https://github.com/speechbrain/speechbrain",docsUrl:"https://huggingface.co/docs/hub/speechbrain",snippets:model=>{const speechbrainInterface=model.config?.speechbrain?.speechbrain_interface;if(void 0===speechbrainInterface)return["# interface not specified in config.json"];const speechbrainMethod=(speechbrainInterface=>{switch(speechbrainInterface){case"EncoderClassifier":return"classify_file";case"EncoderDecoderASR":case"EncoderASR":return"transcribe_file";case"SpectralMaskEnhancement":return"enhance_file";case"SepformerSeparation":return"separate_file";default:return}})(speechbrainInterface);return void 0===speechbrainMethod?["# interface in config.json invalid"]:[`from speechbrain.pretrained import ${speechbrainInterface}\nmodel = ${speechbrainInterface}.from_hparams(\n  "${model.id}"\n)\nmodel.${speechbrainMethod}("file.wav")`]},filter:!0,countDownloads:'path:"hyperparams.yaml"'},"ssr-speech":{prettyLabel:"SSR-Speech",repoName:"SSR-Speech",repoUrl:"https://github.com/WangHelin1997/SSR-Speech",filter:!1,countDownloads:'path_extension:".pth"'},"stable-audio-tools":{prettyLabel:"Stable Audio Tools",repoName:"stable-audio-tools",repoUrl:"https://github.com/Stability-AI/stable-audio-tools.git",filter:!1,countDownloads:'path:"model.safetensors"',snippets:model=>[`import torch\nimport torchaudio\nfrom einops import rearrange\nfrom stable_audio_tools import get_pretrained_model\nfrom stable_audio_tools.inference.generation import generate_diffusion_cond\n\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Download model\nmodel, model_config = get_pretrained_model("${model.id}")\nsample_rate = model_config["sample_rate"]\nsample_size = model_config["sample_size"]\n\nmodel = model.to(device)\n\n# Set up text and timing conditioning\nconditioning = [{\n\t"prompt": "128 BPM tech house drum loop",\n}]\n\n# Generate stereo audio\noutput = generate_diffusion_cond(\n\tmodel,\n\tconditioning=conditioning,\n\tsample_size=sample_size,\n\tdevice=device\n)\n\n# Rearrange audio batch to a single sequence\noutput = rearrange(output, "b d n -> d (b n)")\n\n# Peak normalize, clip, convert to int16, and save to file\noutput = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\ntorchaudio.save("output.wav", output, sample_rate)`]},monkeyocr:{prettyLabel:"MonkeyOCR",repoName:"monkeyocr",repoUrl:"https://github.com/Yuliang-Liu/MonkeyOCR",filter:!1,countDownloads:'path:"Recognition/config.json"'},"diffusion-single-file":{prettyLabel:"Diffusion Single File",repoName:"diffusion-single-file",repoUrl:"https://github.com/comfyanonymous/ComfyUI",filter:!1,countDownloads:'path_extension:"safetensors"'},"seed-story":{prettyLabel:"SEED-Story",repoName:"SEED-Story",repoUrl:"https://github.com/TencentARC/SEED-Story",filter:!1,countDownloads:'path:"cvlm_llama2_tokenizer/tokenizer.model"',snippets:()=>["# seed_story_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/agent_7b_sft.yaml'\n# llm_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/llama2chat7b_lora.yaml'\nfrom omegaconf import OmegaConf\nimport hydra\n\n# load Llama2\nllm_cfg = OmegaConf.load(llm_cfg_path)\nllm = hydra.utils.instantiate(llm_cfg, torch_dtype=\"fp16\")\n\n# initialize seed_story\nseed_story_cfg = OmegaConf.load(seed_story_cfg_path)\nseed_story = hydra.utils.instantiate(seed_story_cfg, llm=llm) "]},soloaudio:{prettyLabel:"SoloAudio",repoName:"SoloAudio",repoUrl:"https://github.com/WangHelin1997/SoloAudio",filter:!1,countDownloads:'path:"soloaudio_v2.pt"'},songbloom:{prettyLabel:"SongBloom",repoName:"SongBloom",repoUrl:"https://github.com/Cypress-Yang/SongBloom",filter:!1,countDownloads:'path_extension:"pt"'},"stable-baselines3":{prettyLabel:"stable-baselines3",repoName:"stable-baselines3",repoUrl:"https://github.com/huggingface/huggingface_sb3",docsUrl:"https://huggingface.co/docs/hub/stable-baselines3",snippets:model=>[`from huggingface_sb3 import load_from_hub\ncheckpoint = load_from_hub(\n\trepo_id="${model.id}",\n\tfilename="{MODEL FILENAME}.zip",\n)`],filter:!0,countDownloads:'path_extension:"zip"'},stanza:{prettyLabel:"Stanza",repoName:"stanza",repoUrl:"https://github.com/stanfordnlp/stanza",docsUrl:"https://huggingface.co/docs/hub/stanza",snippets:model=>[`import stanza\n\nstanza.download("${nameWithoutNamespace(model.id).replace("stanza-","")}")\nnlp = stanza.Pipeline("${nameWithoutNamespace(model.id).replace("stanza-","")}")`],filter:!0,countDownloads:'path:"models/default.zip"'},swarmformer:{prettyLabel:"SwarmFormer",repoName:"SwarmFormer",repoUrl:"https://github.com/takara-ai/SwarmFormer",snippets:model=>[`from swarmformer import SwarmFormerModel\n\nmodel = SwarmFormerModel.from_pretrained("${model.id}")\n`],filter:!1},"f5-tts":{prettyLabel:"F5-TTS",repoName:"F5-TTS",repoUrl:"https://github.com/SWivid/F5-TTS",filter:!1,countDownloads:'path_extension:"safetensors" OR path_extension:"pt"'},genmo:{prettyLabel:"Genmo",repoName:"Genmo",repoUrl:"https://github.com/genmoai/models",filter:!1,countDownloads:'path:"vae_stats.json"'},"tencent-song-generation":{prettyLabel:"SongGeneration",repoName:"SongGeneration",repoUrl:"https://github.com/tencent-ailab/songgeneration",filter:!1,countDownloads:'path:"ckpt/songgeneration_base/model.pt"'},tensorflowtts:{prettyLabel:"TensorFlowTTS",repoName:"TensorFlowTTS",repoUrl:"https://github.com/TensorSpeech/TensorFlowTTS",snippets:model=>model.tags.includes("text-to-mel")?(model=>[`from tensorflow_tts.inference import AutoProcessor, TFAutoModel\n\nprocessor = AutoProcessor.from_pretrained("${model.id}")\nmodel = TFAutoModel.from_pretrained("${model.id}")\n`])(model):model.tags.includes("mel-to-wav")?(model=>[`from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained("${model.id}")\naudios = model.inference(mels)\n`])(model):(model=>[`from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained("${model.id}")\n`])(model)},tensorrt:{prettyLabel:"TensorRT",repoName:"TensorRT",repoUrl:"https://github.com/NVIDIA/TensorRT",countDownloads:'path_extension:"onnx"'},tabpfn:{prettyLabel:"TabPFN",repoName:"TabPFN",repoUrl:"https://github.com/PriorLabs/TabPFN"},terratorch:{prettyLabel:"TerraTorch",repoName:"TerraTorch",repoUrl:"https://github.com/IBM/terratorch",docsUrl:"https://ibm.github.io/terratorch/",filter:!1,countDownloads:'path_extension:"pt" OR path_extension:"ckpt"',snippets:model=>[`from terratorch.registry import BACKBONE_REGISTRY\n\nmodel = BACKBONE_REGISTRY.build("${model.id}")`]},"tic-clip":{prettyLabel:"TiC-CLIP",repoName:"TiC-CLIP",repoUrl:"https://github.com/apple/ml-tic-clip",filter:!1,countDownloads:'path_extension:"pt" AND path_prefix:"checkpoints/"'},timesfm:{prettyLabel:"TimesFM",repoName:"timesfm",repoUrl:"https://github.com/google-research/timesfm",filter:!1,countDownloads:'path:"checkpoints/checkpoint_1100000/state/checkpoint" OR path:"checkpoints/checkpoint_2150000/state/checkpoint" OR path_extension:"ckpt"'},timm:{prettyLabel:"timm",repoName:"pytorch-image-models",repoUrl:"https://github.com/rwightman/pytorch-image-models",docsUrl:"https://huggingface.co/docs/hub/timm",snippets:model=>[`import timm\n\nmodel = timm.create_model("hf_hub:${model.id}", pretrained=True)`],filter:!0,countDownloads:'path:"pytorch_model.bin" OR path:"model.safetensors"'},tirex:{prettyLabel:"TiRex",repoName:"TiRex",repoUrl:"https://github.com/NX-AI/tirex",countDownloads:'path_extension:"ckpt"'},torchgeo:{prettyLabel:"TorchGeo",repoName:"TorchGeo",repoUrl:"https://github.com/microsoft/torchgeo",docsUrl:"https://torchgeo.readthedocs.io/",filter:!1,countDownloads:'path_extension:"pt" OR path_extension:"pth"'},transformers:{prettyLabel:"Transformers",repoName:"ü§ó/transformers",repoUrl:"https://github.com/huggingface/transformers",docsUrl:"https://huggingface.co/docs/hub/transformers",snippets:transformers,filter:!0},"transformers.js":{prettyLabel:"Transformers.js",repoName:"transformers.js",repoUrl:"https://github.com/huggingface/transformers.js",docsUrl:"https://huggingface.co/docs/hub/transformers-js",snippets:model=>{if(!model.pipeline_tag)return["// ‚ö†Ô∏è Unknown pipeline tag"];const libName="@huggingface/transformers";return[`// npm i ${libName}\nimport { pipeline } from '${libName}';\n\n// Allocate pipeline\nconst pipe = await pipeline('${model.pipeline_tag}', '${model.id}');`]},filter:!0},trellis:{prettyLabel:"Trellis",repoName:"Trellis",repoUrl:"https://github.com/microsoft/TRELLIS",countDownloads:'path_extension:"safetensors"'},ultralytics:{prettyLabel:"ultralytics",repoName:"ultralytics",repoUrl:"https://github.com/ultralytics/ultralytics",docsUrl:"https://github.com/ultralytics/ultralytics",filter:!1,countDownloads:'path_extension:"pt"',snippets:ultralytics},univa:{prettyLabel:"univa",repoName:"univa",repoUrl:"https://github.com/PKU-YuanGroup/UniWorld-V1",snippets:model=>[`# Follow installation instructions at https://github.com/PKU-YuanGroup/UniWorld-V1\n\nfrom univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import UnivaQwen2p5VLForConditionalGeneration\n\tmodel = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n        "${model.id}",\n        torch_dtype=torch.bfloat16,\n        attn_implementation="flash_attention_2",\n    ).to("cuda")\n\tprocessor = AutoProcessor.from_pretrained("${model.id}")\n`],filter:!0,countDownloads:'path:"config.json"'},"uni-3dar":{prettyLabel:"Uni-3DAR",repoName:"Uni-3DAR",repoUrl:"https://github.com/dptech-corp/Uni-3DAR",docsUrl:"https://github.com/dptech-corp/Uni-3DAR",countDownloads:'path_extension:"pt"'},"unity-sentis":{prettyLabel:"unity-sentis",repoName:"unity-sentis",repoUrl:"https://github.com/Unity-Technologies/sentis-samples",snippets:()=>['string modelName = "[Your model name here].sentis";\nModel model = ModelLoader.Load(Application.streamingAssetsPath + "/" + modelName);\nIWorker engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);\n// Please see provided C# file for more details\n'],filter:!0,countDownloads:'path_extension:"sentis"'},sana:{prettyLabel:"Sana",repoName:"Sana",repoUrl:"https://github.com/NVlabs/Sana",countDownloads:'path_extension:"pth"',snippets:model=>[`\n# Load the model and infer image from text\nimport torch\nfrom app.sana_pipeline import SanaPipeline\nfrom torchvision.utils import save_image\n\nsana = SanaPipeline("configs/sana_config/1024ms/Sana_1600M_img1024.yaml")\nsana.from_pretrained("hf://${model.id}")\n\nimage = sana(\n    prompt='a cyberpunk cat with a neon sign that says "Sana"',\n    height=1024,\n    width=1024,\n    guidance_scale=5.0,\n    pag_guidance_scale=2.0,\n    num_inference_steps=18,\n) `]},videoprism:{prettyLabel:"VideoPrism",repoName:"VideoPrism",repoUrl:"https://github.com/google-deepmind/videoprism",countDownloads:'path_extension:"npz"',snippets:model=>[`# Install from https://github.com/google-deepmind/videoprism\nimport jax\nfrom videoprism import models as vp\n\nflax_model = vp.get_model("${model.id}")\nloaded_state = vp.load_pretrained_weights("${model.id}")\n\n@jax.jit\ndef forward_fn(inputs, train=False):\n  return flax_model.apply(loaded_state, inputs, train=train)`]},"vfi-mamba":{prettyLabel:"VFIMamba",repoName:"VFIMamba",repoUrl:"https://github.com/MCG-NJU/VFIMamba",countDownloads:'path_extension:"pkl"',snippets:model=>[`from Trainer_finetune import Model\n\nmodel = Model.from_pretrained("${model.id}")`]},lvface:{prettyLabel:"LVFace",repoName:"LVFace",repoUrl:"https://github.com/bytedance/LVFace",countDownloads:'path_extension:"pt" OR path_extension:"onnx"',snippets:model=>[`from huggingface_hub import hf_hub_download\n\t from inference_onnx import LVFaceONNXInferencer\n\nmodel_path = hf_hub_download("${model.id}", "LVFace-L_Glint360K/LVFace-L_Glint360K.onnx")\ninferencer = LVFaceONNXInferencer(model_path, use_gpu=True, timeout=300)\nimg_path = 'path/to/image1.jpg'\nembedding = inferencer.infer_from_image(img_path)`]},voicecraft:{prettyLabel:"VoiceCraft",repoName:"VoiceCraft",repoUrl:"https://github.com/jasonppy/VoiceCraft",docsUrl:"https://github.com/jasonppy/VoiceCraft",snippets:model=>[`from voicecraft import VoiceCraft\n\nmodel = VoiceCraft.from_pretrained("${model.id}")`]},voxcpm:{prettyLabel:"VoxCPM",repoName:"VoxCPM",repoUrl:"https://github.com/OpenBMB/VoxCPM",snippets:model=>[`import soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained("${model.id}")\n\nwav = model.generate(\n    text="VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write("output.wav", wav, 16000)\nprint("saved: output.wav")`],filter:!1},vui:{prettyLabel:"Vui",repoName:"Vui",repoUrl:"https://github.com/vui-ai/vui",countDownloads:'path_extension:"pt"',snippets:()=>['# !pip install git+https://github.com/fluxions-ai/vui\n\nimport torchaudio\n\nfrom vui.inference import render\nfrom vui.model import Vui,\n\nmodel = Vui.from_pretrained().cuda()\nwaveform = render(\n    model,\n    "Hey, here is some random stuff, usually something quite long as the shorter the text the less likely the model can cope!",\n)\nprint(waveform.shape)\ntorchaudio.save("out.opus", waveform[0], 22050)\n']},vibevoice:{prettyLabel:"VibeVoice",repoName:"VibeVoice",repoUrl:"https://github.com/microsoft/VibeVoice",snippets:model=>[`import torch, soundfile as sf, librosa, numpy as np\nfrom vibevoice.processor.vibevoice_processor import VibeVoiceProcessor\nfrom vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n\n# Load voice sample (should be 24kHz mono)\nvoice, sr = sf.read("path/to/voice_sample.wav")\nif voice.ndim > 1: voice = voice.mean(axis=1)\nif sr != 24000: voice = librosa.resample(voice, sr, 24000)\n\nprocessor = VibeVoiceProcessor.from_pretrained("${model.id}")\nmodel = VibeVoiceForConditionalGenerationInference.from_pretrained(\n    "${model.id}", torch_dtype=torch.bfloat16\n).to("cuda").eval()\nmodel.set_ddpm_inference_steps(5)\n\ninputs = processor(text=["Speaker 0: Hello!\\nSpeaker 1: Hi there!"],\n                   voice_samples=[[voice]], return_tensors="pt")\naudio = model.generate(**inputs, cfg_scale=1.3,\n                       tokenizer=processor.tokenizer).speech_outputs[0]\nsf.write("output.wav", audio.cpu().numpy().squeeze(), 24000)`],filter:!1},"wan2.2":{prettyLabel:"Wan2.2",repoName:"Wan2.2",repoUrl:"https://github.com/Wan-Video/Wan2.2",countDownloads:'path_filename:"config" AND path_extension:"json"'},wham:{prettyLabel:"WHAM",repoName:"wham",repoUrl:"https://huggingface.co/microsoft/wham",docsUrl:"https://huggingface.co/microsoft/wham/blob/main/README.md",countDownloads:'path_extension:"ckpt"'},whisperkit:{prettyLabel:"WhisperKit",repoName:"WhisperKit",repoUrl:"https://github.com/argmaxinc/WhisperKit",docsUrl:"https://github.com/argmaxinc/WhisperKit?tab=readme-ov-file#homebrew",snippets:()=>['# Install CLI with Homebrew on macOS device\nbrew install whisperkit-cli\n\n# View all available inference options\nwhisperkit-cli transcribe --help\n\t\n# Download and run inference using whisper base model\nwhisperkit-cli transcribe --audio-path /path/to/audio.mp3\n\n# Or use your preferred model variant\nwhisperkit-cli transcribe --model "large-v3" --model-prefix "distil" --audio-path /path/to/audio.mp3 --verbose'],countDownloads:'path_filename:"model" AND path_extension:"mil" AND _exists_:"path_prefix"'},yolov10:{prettyLabel:"YOLOv10",repoName:"YOLOv10",repoUrl:"https://github.com/THU-MIG/yolov10",docsUrl:"https://github.com/THU-MIG/yolov10",countDownloads:'path_extension:"pt" OR path_extension:"safetensors"',snippets:ultralytics},zonos:{prettyLabel:"Zonos",repoName:"Zonos",repoUrl:"https://github.com/Zyphra/Zonos",docsUrl:"https://github.com/Zyphra/Zonos",snippets:model=>[`# pip install git+https://github.com/Zyphra/Zonos.git\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained("${model.id}", device="cuda")\n\nwav, sr = torchaudio.load("speaker.wav")           # 5-10s reference clip\nspeaker = model.make_speaker_embedding(wav, sr)\n\ncond  = make_cond_dict(text="Hello, world!", speaker=speaker, language="en-us")\ncodes = model.generate(model.prepare_conditioning(cond))\n\naudio = model.autoencoder.decode(codes)[0].cpu()\ntorchaudio.save("sample.wav", audio, model.autoencoder.sampling_rate)\n`],filter:!1},"3dtopia-xl":{prettyLabel:"3DTopia-XL",repoName:"3DTopia-XL",repoUrl:"https://github.com/3DTopia/3DTopia-XL",filter:!1,countDownloads:'path:"model_vae_fp16.pt"',snippets:model=>[`from threedtopia_xl.models import threedtopia_xl\n\nmodel = threedtopia_xl.from_pretrained("${model.id}")\nmodel.generate(cond="path/to/image.png")`]}};Object.entries(MODEL_LIBRARIES_UI_ELEMENTS).filter(([_,v])=>v.filter).map(([k])=>k),(GGMLFileQuantizationType2=GGMLFileQuantizationType||(GGMLFileQuantizationType={}))[GGMLFileQuantizationType2.F32=0]="F32",GGMLFileQuantizationType2[GGMLFileQuantizationType2.F16=1]="F16",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_0=2]="Q4_0",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_1=3]="Q4_1",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_1_SOME_F16=4]="Q4_1_SOME_F16",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_2=5]="Q4_2",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_3=6]="Q4_3",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q8_0=7]="Q8_0",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q5_0=8]="Q5_0",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q5_1=9]="Q5_1",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q2_K=10]="Q2_K",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q3_K_S=11]="Q3_K_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q3_K_M=12]="Q3_K_M",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q3_K_L=13]="Q3_K_L",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_K_S=14]="Q4_K_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_K_M=15]="Q4_K_M",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q5_K_S=16]="Q5_K_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q5_K_M=17]="Q5_K_M",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q6_K=18]="Q6_K",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ2_XXS=19]="IQ2_XXS",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ2_XS=20]="IQ2_XS",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q2_K_S=21]="Q2_K_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ3_XS=22]="IQ3_XS",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ3_XXS=23]="IQ3_XXS",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ1_S=24]="IQ1_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ4_NL=25]="IQ4_NL",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ3_S=26]="IQ3_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ3_M=27]="IQ3_M",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ2_S=28]="IQ2_S",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ2_M=29]="IQ2_M",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ4_XS=30]="IQ4_XS",GGMLFileQuantizationType2[GGMLFileQuantizationType2.IQ1_M=31]="IQ1_M",GGMLFileQuantizationType2[GGMLFileQuantizationType2.BF16=32]="BF16",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_0_4_4=33]="Q4_0_4_4",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_0_4_8=34]="Q4_0_4_8",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_0_8_8=35]="Q4_0_8_8",GGMLFileQuantizationType2[GGMLFileQuantizationType2.TQ1_0=36]="TQ1_0",GGMLFileQuantizationType2[GGMLFileQuantizationType2.TQ2_0=37]="TQ2_0",GGMLFileQuantizationType2[GGMLFileQuantizationType2.MXFP4_MOE=38]="MXFP4_MOE",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q2_K_XL=1e3]="Q2_K_XL",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q3_K_XL=1001]="Q3_K_XL",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q4_K_XL=1002]="Q4_K_XL",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q5_K_XL=1003]="Q5_K_XL",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q6_K_XL=1004]="Q6_K_XL",GGMLFileQuantizationType2[GGMLFileQuantizationType2.Q8_K_XL=1005]="Q8_K_XL";var GGMLQuantizationType,GGMLQuantizationType2,ggufQuants=Object.values(GGMLFileQuantizationType).filter(v=>"string"==typeof v);new RegExp(`(?<quant>${ggufQuants.join("|")})(_(?<sizeVariation>[A-Z]+))?`),GGMLFileQuantizationType.F32,GGMLFileQuantizationType.BF16,GGMLFileQuantizationType.F16,GGMLFileQuantizationType.Q8_K_XL,GGMLFileQuantizationType.Q8_0,GGMLFileQuantizationType.Q6_K_XL,GGMLFileQuantizationType.Q6_K,GGMLFileQuantizationType.Q5_K_XL,GGMLFileQuantizationType.Q5_K_M,GGMLFileQuantizationType.Q5_K_S,GGMLFileQuantizationType.Q5_0,GGMLFileQuantizationType.Q5_1,GGMLFileQuantizationType.Q4_K_XL,GGMLFileQuantizationType.Q4_K_M,GGMLFileQuantizationType.Q4_K_S,GGMLFileQuantizationType.IQ4_NL,GGMLFileQuantizationType.IQ4_XS,GGMLFileQuantizationType.Q4_0_4_4,GGMLFileQuantizationType.Q4_0_4_8,GGMLFileQuantizationType.Q4_0_8_8,GGMLFileQuantizationType.Q4_1_SOME_F16,GGMLFileQuantizationType.Q4_0,GGMLFileQuantizationType.Q4_1,GGMLFileQuantizationType.Q4_2,GGMLFileQuantizationType.Q4_3,GGMLFileQuantizationType.MXFP4_MOE,GGMLFileQuantizationType.Q3_K_XL,GGMLFileQuantizationType.Q3_K_L,GGMLFileQuantizationType.Q3_K_M,GGMLFileQuantizationType.Q3_K_S,GGMLFileQuantizationType.IQ3_M,GGMLFileQuantizationType.IQ3_S,GGMLFileQuantizationType.IQ3_XS,GGMLFileQuantizationType.IQ3_XXS,GGMLFileQuantizationType.Q2_K_XL,GGMLFileQuantizationType.Q2_K,GGMLFileQuantizationType.Q2_K_S,GGMLFileQuantizationType.IQ2_M,GGMLFileQuantizationType.IQ2_S,GGMLFileQuantizationType.IQ2_XS,GGMLFileQuantizationType.IQ2_XXS,GGMLFileQuantizationType.IQ1_S,GGMLFileQuantizationType.IQ1_M,GGMLFileQuantizationType.TQ1_0,GGMLFileQuantizationType.TQ2_0,(GGMLQuantizationType2=GGMLQuantizationType||(GGMLQuantizationType={}))[GGMLQuantizationType2.F32=0]="F32",GGMLQuantizationType2[GGMLQuantizationType2.F16=1]="F16",GGMLQuantizationType2[GGMLQuantizationType2.Q4_0=2]="Q4_0",GGMLQuantizationType2[GGMLQuantizationType2.Q4_1=3]="Q4_1",GGMLQuantizationType2[GGMLQuantizationType2.Q5_0=6]="Q5_0",GGMLQuantizationType2[GGMLQuantizationType2.Q5_1=7]="Q5_1",GGMLQuantizationType2[GGMLQuantizationType2.Q8_0=8]="Q8_0",GGMLQuantizationType2[GGMLQuantizationType2.Q8_1=9]="Q8_1",GGMLQuantizationType2[GGMLQuantizationType2.Q2_K=10]="Q2_K",GGMLQuantizationType2[GGMLQuantizationType2.Q3_K=11]="Q3_K",GGMLQuantizationType2[GGMLQuantizationType2.Q4_K=12]="Q4_K",GGMLQuantizationType2[GGMLQuantizationType2.Q5_K=13]="Q5_K",GGMLQuantizationType2[GGMLQuantizationType2.Q6_K=14]="Q6_K",GGMLQuantizationType2[GGMLQuantizationType2.Q8_K=15]="Q8_K",GGMLQuantizationType2[GGMLQuantizationType2.IQ2_XXS=16]="IQ2_XXS",GGMLQuantizationType2[GGMLQuantizationType2.IQ2_XS=17]="IQ2_XS",GGMLQuantizationType2[GGMLQuantizationType2.IQ3_XXS=18]="IQ3_XXS",GGMLQuantizationType2[GGMLQuantizationType2.IQ1_S=19]="IQ1_S",GGMLQuantizationType2[GGMLQuantizationType2.IQ4_NL=20]="IQ4_NL",GGMLQuantizationType2[GGMLQuantizationType2.IQ3_S=21]="IQ3_S",GGMLQuantizationType2[GGMLQuantizationType2.IQ2_S=22]="IQ2_S",GGMLQuantizationType2[GGMLQuantizationType2.IQ4_XS=23]="IQ4_XS",GGMLQuantizationType2[GGMLQuantizationType2.I8=24]="I8",GGMLQuantizationType2[GGMLQuantizationType2.I16=25]="I16",GGMLQuantizationType2[GGMLQuantizationType2.I32=26]="I32",GGMLQuantizationType2[GGMLQuantizationType2.I64=27]="I64",GGMLQuantizationType2[GGMLQuantizationType2.F64=28]="F64",GGMLQuantizationType2[GGMLQuantizationType2.IQ1_M=29]="IQ1_M",GGMLQuantizationType2[GGMLQuantizationType2.BF16=30]="BF16",GGMLQuantizationType2[GGMLQuantizationType2.TQ1_0=34]="TQ1_0",GGMLQuantizationType2[GGMLQuantizationType2.TQ2_0=35]="TQ2_0",GGMLQuantizationType2[GGMLQuantizationType2.MXFP4=39]="MXFP4";var inferenceSnippetLanguages=["python","js","sh"],templates={js:{fetch:{basic:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',basicAudio:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "audio/flac",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',basicImage:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "image/jpeg",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',conversational:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ \n{{ autoInputs.asTsString }}\n}).then((response) => {\n    console.log(JSON.stringify(response));\n});',imageToImage:'const image = fs.readFileSync("{{inputs.asObj.inputs}}");\n\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "image/jpeg",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: {\n\t\t\t\t"inputs": `data:image/png;base64,${data.inputs.encode("base64")}`,\n\t\t\t\t"parameters": data.parameters,\n\t\t\t}\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ \n\tinputs: image,\n\tparameters: {\n\t\tprompt: "{{ inputs.asObj.parameters.prompt }}",\n\t}\n}).then((response) => {\n    console.log(JSON.stringify(response));\n});',imageToVideo:'const image = fs.readFileSync("{{inputs.asObj.inputs}}");\n\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "image/jpeg",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: {\n\t\t\t\t"image_url": `data:image/png;base64,${data.image.encode("base64")}`,\n\t\t\t\t"prompt": data.prompt,\n\t\t\t}\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({\n\t"image": image,\n\t"prompt": "{{inputs.asObj.parameters.prompt}}",\n}).then((response) => {\n    // Use video\n});',textToAudio:'{% if model.library_name == "transformers" %}\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.blob();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    // Returns a byte object of the Audio wavform. Use it directly!\n});\n{% else %}\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n    const result = await response.json();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});\n{% endif %} ',textToImage:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.blob();\n\treturn result;\n}\n\n\nquery({ {{ providerInputs.asTsString }} }).then((response) => {\n    // Use image\n});',textToSpeech:'{% if model.library_name == "transformers" %}\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n{% if billTo %}\n\t\t\t\t"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.blob();\n    return result;\n}\n\nquery({ text: {{ inputs.asObj.inputs }} }).then((response) => {\n    // Returns a byte object of the Audio wavform. Use it directly!\n});\n{% else %}\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n    const result = await response.json();\n    return result;\n}\n\nquery({ text: {{ inputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});\n{% endif %} ',zeroShotClassification:'async function query(data) {\n    const response = await fetch(\n\t\t"{{ fullUrl }}",\n        {\n            headers: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n                "Content-Type": "application/json",\n{% if billTo %}\n                "X-HF-Bill-To": "{{ billTo }}",\n{% endif %}         },\n            method: "POST",\n            body: JSON.stringify(data),\n        }\n    );\n    const result = await response.json();\n    return result;\n}\n\nquery({\n    inputs: {{ providerInputs.asObj.inputs }},\n    parameters: { candidate_labels: ["refund", "legal", "faq"] }\n}).then((response) => {\n    console.log(JSON.stringify(response));\n});'},"huggingface.js":{basic:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst output = await client.{{ methodName }}({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n\tmodel: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n\tprovider: "{{ provider }}",\n}{% if billTo %}, {\n\tbillTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(output);',basicAudio:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n\tdata,\n\tmodel: "{{ model.id }}",\n\tprovider: "{{ provider }}",\n}{% if billTo %}, {\n\tbillTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(output);',basicImage:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n\tdata,\n\tmodel: "{{ model.id }}",\n\tprovider: "{{ provider }}",\n}{% if billTo %}, {\n\tbillTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(output);',conversational:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst chatCompletion = await client.chatCompletion({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n{% if directRequest %}\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{% else %}\n    model: "{{ providerModelId }}",\n{% endif %}\n{{ inputs.asTsString }}\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(chatCompletion.choices[0].message);',conversationalStream:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nlet out = "";\n\nconst stream = client.chatCompletionStream({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n    model: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n\nfor await (const chunk of stream) {\n\tif (chunk.choices && chunk.choices.length > 0) {\n\t\tconst newContent = chunk.choices[0].delta.content;\n\t\tout += newContent;\n\t\tconsole.log(newContent);\n\t}\n}',imageToImage:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync("{{inputs.asObj.inputs}}");\n\nconst image = await client.imageToImage({\n{% if endpointUrl %}\n\tendpointUrl: "{{ endpointUrl }}",\n{% endif %}\n\tprovider: "{{provider}}",\n\tmodel: "{{model.id}}",\n\tinputs: data,\n\tparameters: { prompt: "{{inputs.asObj.parameters.prompt}}", },\n}{% if billTo %}, {\n\tbillTo: "{{ billTo }}",\n}{% endif %});\n/// Use the generated image (it\'s a Blob)\n// For example, you can save it to a file or display it in an image element\n',imageToVideo:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync("{{inputs.asObj.inputs}}");\n\nconst video = await client.imageToVideo({\n{% if endpointUrl %}\n\tendpointUrl: "{{ endpointUrl }}",\n{% endif %}\n\tprovider: "{{provider}}",\n\tmodel: "{{model.id}}",\n\tinputs: data,\n\tparameters: { prompt: "{{inputs.asObj.parameters.prompt}}", },\n}{% if billTo %}, {\n\tbillTo: "{{ billTo }}",\n}{% endif %});\n\n/// Use the generated video (it\'s a Blob)\n// For example, you can save it to a file or display it in a video element\n',textToImage:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst image = await client.textToImage({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n\tparameters: { num_inference_steps: 5 },\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n/// Use the generated image (it\'s a Blob)',textToSpeech:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst audio = await client.textToSpeech({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n// Use the generated audio (it\'s a Blob)',textToVideo:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst video = await client.textToVideo({\n{% if endpointUrl %}\n    endpointUrl: "{{ endpointUrl }}",\n{% endif %}\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n// Use the generated video (it\'s a Blob)'},openai:{conversational:'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n\tbaseURL: "{{ baseUrl }}",\n\tapiKey: "{{ accessToken }}",\n{% if billTo %}\n\tdefaultHeaders: {\n\t\t"X-HF-Bill-To": "{{ billTo }}" \n\t}\n{% endif %}\n});\n\nconst chatCompletion = await client.chat.completions.create({\n\tmodel: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n});\n\nconsole.log(chatCompletion.choices[0].message);',conversationalStream:'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n\tbaseURL: "{{ baseUrl }}",\n\tapiKey: "{{ accessToken }}",\n{% if billTo %}\n    defaultHeaders: {\n\t\t"X-HF-Bill-To": "{{ billTo }}" \n\t}\n{% endif %}\n});\n\nconst stream = await client.chat.completions.create({\n    model: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n    stream: true,\n});\n\nfor await (const chunk of stream) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || "");\n}'}},python:{fal_client:{imageToImage:'{%if provider == "fal-ai" %}\nimport fal_client\nimport base64\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log["message"])\n\nwith open("{{inputs.asObj.inputs}}", "rb") as image_file:\n    image_base_64 = base64.b64encode(image_file.read()).decode(\'utf-8\')\n\nresult = fal_client.subscribe(\n    "fal-ai/flux-kontext/dev",\n    arguments={\n        "prompt": f"data:image/png;base64,{image_base_64}",\n        "image_url": "{{ providerInputs.asObj.inputs }}",\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n{%endif%}\n',imageToVideo:'{%if provider == "fal-ai" %}\nimport fal_client\nimport base64\n\ndef on_queue_update(update):\n    if isinstance(update, fal_client.InProgress):\n        for log in update.logs:\n           print(log["message"])\n\nwith open("{{inputs.asObj.inputs}}", "rb") as image_file:\n    image_base_64 = base64.b64encode(image_file.read()).decode(\'utf-8\')\n\nresult = fal_client.subscribe(\n    "{{model.id}}",\n    arguments={\n        "image_url": f"data:image/png;base64,{image_base_64}",\n        "prompt": "{{inputs.asObj.parameters.prompt}}",\n    },\n    with_logs=True,\n    on_queue_update=on_queue_update,\n)\nprint(result)\n{%endif%}\n',textToImage:'{% if provider == "fal-ai" %}\nimport fal_client\n\n{% if providerInputs.asObj.loras is defined and providerInputs.asObj.loras != none %}\nresult = fal_client.subscribe(\n    "{{ providerModelId }}",\n    arguments={\n        "prompt": {{ inputs.asObj.inputs }},\n        "loras":{{ providerInputs.asObj.loras | tojson }},\n    },\n)\n{% else %}\nresult = fal_client.subscribe(\n    "{{ providerModelId }}",\n    arguments={\n        "prompt": {{ inputs.asObj.inputs }},\n    },\n)\n{% endif %} \nprint(result)\n{% endif %} '},huggingface_hub:{basic:'result = client.{{ methodName }}(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n)',basicAudio:'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',basicImage:'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',conversational:'completion = client.chat.completions.create(\n{% if directRequest %}\n    model="{{ model.id }}",\n{% else %}\n    model="{{ providerModelId }}",\n{% endif %}\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',conversationalStream:'stream = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="") ',documentQuestionAnswering:'output = client.document_question_answering(\n    "{{ inputs.asObj.image }}",\n    question="{{ inputs.asObj.question }}",\n    model="{{ model.id }}",\n) ',imageToImage:'with open("{{ inputs.asObj.inputs }}", "rb") as image_file:\n   input_image = image_file.read()\n\n# output is a PIL.Image object\nimage = client.image_to_image(\n    input_image,\n    prompt="{{ inputs.asObj.parameters.prompt }}",\n    model="{{ model.id }}",\n)\n',imageToVideo:'with open("{{ inputs.asObj.inputs }}", "rb") as image_file:\n   input_image = image_file.read()\n\nvideo = client.image_to_video(\n    input_image,\n    prompt="{{ inputs.asObj.parameters.prompt }}",\n    model="{{ model.id }}",\n) \n',importInferenceClient:'from huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n{% if endpointUrl %}\n    base_url="{{ baseUrl }}",\n{% endif %}\n{% if task != "conversational" or directRequest %}\n    provider="{{ provider }}",\n{% endif %}\n    api_key="{{ accessToken }}",\n{% if billTo %}\n    bill_to="{{ billTo }}",\n{% endif %}\n)',questionAnswering:'answer = client.question_answering(\n    question="{{ inputs.asObj.question }}",\n    context="{{ inputs.asObj.context }}",\n    model="{{ model.id }}",\n) ',tableQuestionAnswering:'answer = client.table_question_answering(\n    query="{{ inputs.asObj.query }}",\n    table={{ inputs.asObj.table }},\n    model="{{ model.id }}",\n) ',textToImage:'# output is a PIL.Image object\nimage = client.text_to_image(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) ',textToSpeech:'# audio is returned as bytes\naudio = client.text_to_speech(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) \n',textToVideo:'video = client.text_to_video(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) '},openai:{conversational:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}",\n{% if billTo %}\n    default_headers={\n        "X-HF-Bill-To": "{{ billTo }}"\n    }\n{% endif %}\n)\n\ncompletion = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',conversationalStream:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}",\n{% if billTo %}\n    default_headers={\n        "X-HF-Bill-To": "{{ billTo }}"\n    }\n{% endif %}\n)\n\nstream = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="")'},requests:{basic:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n}) ',basicAudio:'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "audio/flac", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',basicImage:'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "image/jpeg", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',conversational:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\nresponse = query({\n{{ autoInputs.asJsonString }}\n})\n\nprint(response["choices"][0]["message"])',conversationalStream:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n    for line in response.iter_lines():\n        if not line.startswith(b"data:"):\n            continue\n        if line.strip() == b"data: [DONE]":\n            return\n        yield json.loads(line.decode("utf-8").lstrip("data:").rstrip("/n"))\n\nchunks = query({\n{{ autoInputs.asJsonString }},\n    "stream": True,\n})\n\nfor chunk in chunks:\n    print(chunk["choices"][0]["delta"]["content"], end="")',documentQuestionAnswering:'def query(payload):\n    with open(payload["image"], "rb") as f:\n        img = f.read()\n        payload["image"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {\n        "image": "{{ inputs.asObj.image }}",\n        "question": "{{ inputs.asObj.question }}",\n    },\n}) ',imageToImage:'\ndef query(payload):\n    with open(payload["inputs"], "rb") as f:\n        img = f.read()\n        payload["inputs"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n{{ providerInputs.asJsonString }}\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes)) ',imageToVideo:'\ndef query(payload):\n    with open(payload["inputs"], "rb") as f:\n        img = f.read()\n        payload["inputs"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nvideo_bytes = query({\n{{ inputs.asJsonString }}\n})\n',importRequests:'{% if importBase64 %}\nimport base64\n{% endif %}\n{% if importJson %}\nimport json\n{% endif %}\nimport requests\n\nAPI_URL = "{{ fullUrl }}"\nheaders = {\n    "Authorization": "{{ authorizationHeader }}",\n{% if billTo %}\n    "X-HF-Bill-To": "{{ billTo }}"\n{% endif %}\n}',tabular:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nresponse = query({\n    "inputs": {\n        "data": {{ providerInputs.asObj.inputs }}\n    },\n}) ',textToAudio:'{% if model.library_name == "transformers" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\naudio_bytes = query({\n    "inputs": {{ inputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio_bytes)\n{% else %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\naudio, sampling_rate = query({\n    "inputs": {{ inputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio, rate=sampling_rate)\n{% endif %} ',textToImage:'{% if provider == "hf-inference" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes))\n{% endif %}',textToSpeech:'{% if model.library_name == "transformers" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\naudio_bytes = query({\n    "text": {{ inputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio_bytes)\n{% else %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\naudio, sampling_rate = query({\n    "text": {{ inputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio, rate=sampling_rate)\n{% endif %} ',zeroShotClassification:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},\n}) ',zeroShotImageClassification:'def query(data):\n    with open(data["image_path"], "rb") as f:\n        img = f.read()\n    payload={\n        "parameters": data["parameters"],\n        "inputs": base64.b64encode(img).decode("utf-8")\n    }\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "image_path": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["cat", "dog", "llama"]},\n}) '}},sh:{curl:{basic:"curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    -d '{\n{{ providerInputs.asCurlString }}\n    }'",basicAudio:"curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: audio/flac' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    --data-binary @{{ providerInputs.asObj.inputs }}",basicImage:"curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: image/jpeg' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    --data-binary @{{ providerInputs.asObj.inputs }}",conversational:"curl {{ fullUrl }} \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    -d '{\n{{ autoInputs.asCurlString }},\n        \"stream\": false\n    }'",conversationalStream:"curl {{ fullUrl }} \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    -d '{\n{{ autoInputs.asCurlString }},\n        \"stream\": true\n    }'",zeroShotClassification:'curl {{ fullUrl }} \\\n    -X POST \\\n    -d \'{"inputs": {{ providerInputs.asObj.inputs }}, "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}\' \\\n    -H \'Content-Type: application/json\' \\\n    -H \'Authorization: {{ authorizationHeader }}\'\n{% if billTo %} \\\n    -H \'X-HF-Bill-To: {{ billTo }}\'\n{% endif %}'}}},CLIENTS={js:["openai","huggingface.js","fetch"],python:["openai","huggingface_hub","fal_client","requests"],sh:["curl"]},CLIENTS_NON_CONVERSATIONAL_AUTO_POLICY={js:["huggingface.js"],python:["huggingface_hub"]},loadTemplate=(language,client,templateName)=>{const template=templates[language]?.[client]?.[templateName];if(!template)throw new Error(`Template not found: ${language}/${client}/${templateName}`);return data=>new Template(template).render({...data})},snippetImportPythonInferenceClient=loadTemplate("python","huggingface_hub","importInferenceClient"),snippetImportRequests=loadTemplate("python","requests","importRequests"),HF_PYTHON_METHODS={"audio-classification":"audio_classification","audio-to-audio":"audio_to_audio","automatic-speech-recognition":"automatic_speech_recognition","document-question-answering":"document_question_answering","feature-extraction":"feature_extraction","fill-mask":"fill_mask","image-classification":"image_classification","image-segmentation":"image_segmentation","image-to-image":"image_to_image","image-to-text":"image_to_text","object-detection":"object_detection","question-answering":"question_answering","sentence-similarity":"sentence_similarity",summarization:"summarization","table-question-answering":"table_question_answering","tabular-classification":"tabular_classification","tabular-regression":"tabular_regression","text-classification":"text_classification","text-generation":"text_generation","text-to-image":"text_to_image","text-to-speech":"text_to_speech","text-to-video":"text_to_video","token-classification":"token_classification",translation:"translation","visual-question-answering":"visual_question_answering","zero-shot-classification":"zero_shot_classification","zero-shot-image-classification":"zero_shot_image_classification"},HF_JS_METHODS={"automatic-speech-recognition":"automaticSpeechRecognition","feature-extraction":"featureExtraction","fill-mask":"fillMask","image-classification":"imageClassification","question-answering":"questionAnswering","sentence-similarity":"sentenceSimilarity",summarization:"summarization","table-question-answering":"tableQuestionAnswering","text-classification":"textClassification","text-generation":"textGeneration","token-classification":"tokenClassification","text-to-speech":"textToSpeech",translation:"translation"},snippetGenerator=(templateName,inputPreparationFn)=>(model,provider,inferenceProviderMapping,opts)=>{const logger=getLogger(),providerModelId=inferenceProviderMapping?.providerId??model.id;let providerHelper,task=model.pipeline_tag;model.pipeline_tag&&["text-generation","image-text-to-text"].includes(model.pipeline_tag)&&model.tags.includes("conversational")&&(templateName=opts?.streaming?"conversationalStream":"conversational",inputPreparationFn=prepareConversationalInput,task="conversational");try{providerHelper=getProviderHelper(provider,task)}catch(e){return logger.error(`Failed to get provider helper for ${provider} (${task})`,e),[]}const placeholder=opts?.directRequest?"not_hf_token_placeholder":"hf_token_placeholder",accessTokenOrPlaceholder=opts?.accessToken??placeholder,inputs=opts?.inputs?{inputs:opts.inputs}:inputPreparationFn?inputPreparationFn(model,opts):{inputs:getModelInputSnippet(model)},request2=makeRequestOptionsFromResolvedModel(providerModelId,providerHelper,{accessToken:accessTokenOrPlaceholder,provider:provider,endpointUrl:opts?.endpointUrl??("auto"===provider?HF_ROUTER_AUTO_ENDPOINT:void 0),...inputs},inferenceProviderMapping,{task:task,billTo:opts?.billTo});let providerInputs=inputs;const bodyAsObj=request2.info.body;if("string"==typeof bodyAsObj)try{providerInputs=JSON.parse(bodyAsObj)}catch(e){logger.error("Failed to parse body as JSON",e)}const autoInputs=opts?.endpointUrl||opts?.directRequest?providerInputs:"auto"!==provider?{...inputs,model:`${model.id}:${provider}`}:{...inputs,model:`${model.id}`},params={accessToken:accessTokenOrPlaceholder,authorizationHeader:request2.info.headers?.Authorization,baseUrl:"conversational"!==task||opts?.endpointUrl||opts?.directRequest?(str=request2.url,suffix="/chat/completions",str.endsWith(suffix)?str.slice(0,-suffix.length):str):HF_ROUTER_AUTO_ENDPOINT,fullUrl:"conversational"!==task||opts?.endpointUrl||opts?.directRequest?request2.url:HF_ROUTER_AUTO_ENDPOINT+"/chat/completions",inputs:{asObj:inputs,asCurlString:formatBody(inputs,"curl"),asJsonString:formatBody(inputs,"json"),asPythonString:formatBody(inputs,"python"),asTsString:formatBody(inputs,"ts")},providerInputs:{asObj:providerInputs,asCurlString:formatBody(providerInputs,"curl"),asJsonString:formatBody(providerInputs,"json"),asPythonString:formatBody(providerInputs,"python"),asTsString:formatBody(providerInputs,"ts")},autoInputs:{asObj:autoInputs,asCurlString:formatBody(autoInputs,"curl"),asJsonString:formatBody(autoInputs,"json"),asPythonString:formatBody(autoInputs,"python"),asTsString:formatBody(autoInputs,"ts")},model:model,provider:provider,providerModelId:"conversational"!==task||opts?.endpointUrl||opts?.directRequest?providerModelId??model.id:"auto"!==provider?`${model.id}:${provider}`:model.id,billTo:opts?.billTo,endpointUrl:opts?.endpointUrl,task:task,directRequest:!!opts?.directRequest};var str,suffix;const clients="auto"===provider&&"conversational"!==task?CLIENTS_NON_CONVERSATIONAL_AUTO_POLICY:CLIENTS;return inferenceSnippetLanguages.map(language=>(clients[language]??[]).map(client=>{if(!((language,client,templateName)=>void 0!==templates[language]?.[client]?.[templateName])(language,client,templateName))return;const template=loadTemplate(language,client,templateName);if("huggingface_hub"===client&&templateName.includes("basic")){if(!model.pipeline_tag||!(model.pipeline_tag in HF_PYTHON_METHODS))return;params.methodName=HF_PYTHON_METHODS[model.pipeline_tag]}if("huggingface.js"===client&&templateName.includes("basic")){if(!model.pipeline_tag||!(model.pipeline_tag in HF_JS_METHODS))return;params.methodName=HF_JS_METHODS[model.pipeline_tag]}let snippet=template(params).trim();if(snippet){if("huggingface_hub"===client){snippet=`${snippetImportPythonInferenceClient({...params})}\n\n${snippet}`}else if("requests"===client){const importSection=snippetImportRequests({...params,importBase64:snippet.includes("base64"),importJson:snippet.includes("json.")});snippet=`${importSection}\n\n${snippet}`}return snippet.includes(placeholder)&&(snippet=function(directRequest,placeholder,snippet,language,provider,endpointUrl){const useHfToken=!endpointUrl&&("hf-inference"==provider||!directRequest&&(snippet.includes("InferenceClient")||snippet.includes("https://router.huggingface.co"))),accessTokenEnvVar=useHfToken?"HF_TOKEN":endpointUrl?"API_TOKEN":provider.toUpperCase().replace("-","_")+"_API_KEY";"sh"===language?snippet=snippet.replace(`'Authorization: Bearer ${placeholder}'`,`"Authorization: Bearer $${accessTokenEnvVar}"`):"python"===language?snippet=(snippet=(snippet=(snippet=(snippet="import os\n"+snippet).replace(`"${placeholder}"`,`os.environ["${accessTokenEnvVar}"]`)).replace(`"Bearer ${placeholder}"`,`f"Bearer {os.environ['${accessTokenEnvVar}']}"`)).replace(`"Key ${placeholder}"`,`f"Key {os.environ['${accessTokenEnvVar}']}"`)).replace(`"X-Key ${placeholder}"`,`f"X-Key {os.environ['${accessTokenEnvVar}']}"`):"js"===language&&(snippet=(snippet=(snippet=(snippet=snippet.replace(`"${placeholder}"`,`process.env.${accessTokenEnvVar}`)).replace(`Authorization: "Bearer ${placeholder}",`,`Authorization: \`Bearer \${process.env.${accessTokenEnvVar}}\`,`)).replace(`Authorization: "Key ${placeholder}",`,`Authorization: \`Key \${process.env.${accessTokenEnvVar}}\`,`)).replace(`Authorization: "X-Key ${placeholder}",`,`Authorization: \`X-Key \${process.env.${accessTokenEnvVar}}\`,`));return snippet}(opts?.directRequest,placeholder,snippet,language,provider,opts?.endpointUrl)),{language:language,client:client,content:snippet}}}).filter(snippet=>void 0!==snippet)).flat()},prepareImageToImageInput=model=>{const data=JSON.parse(getModelInputSnippet(model));return{inputs:data.image,parameters:{prompt:data.prompt}}},prepareConversationalInput=(model,opts)=>({messages:opts?.messages??getModelInputSnippet(model),...opts?.temperature?{temperature:opts?.temperature}:void 0,...opts?.max_tokens?{max_tokens:opts?.max_tokens}:void 0,...opts?.top_p?{top_p:opts?.top_p}:void 0}),snippets={"audio-classification":snippetGenerator("basicAudio"),"audio-to-audio":snippetGenerator("basicAudio"),"automatic-speech-recognition":snippetGenerator("basicAudio"),"document-question-answering":snippetGenerator("documentQuestionAnswering",model=>JSON.parse(getModelInputSnippet(model))),"feature-extraction":snippetGenerator("basic"),"fill-mask":snippetGenerator("basic"),"image-classification":snippetGenerator("basicImage"),"image-segmentation":snippetGenerator("basicImage"),"image-text-to-text":snippetGenerator("conversational"),"image-to-image":snippetGenerator("imageToImage",prepareImageToImageInput),"image-to-text":snippetGenerator("basicImage"),"image-to-video":snippetGenerator("imageToVideo",prepareImageToImageInput),"object-detection":snippetGenerator("basicImage"),"question-answering":snippetGenerator("questionAnswering",model=>{const data=JSON.parse(getModelInputSnippet(model));return{question:data.question,context:data.context}}),"sentence-similarity":snippetGenerator("basic"),summarization:snippetGenerator("basic"),"tabular-classification":snippetGenerator("tabular"),"tabular-regression":snippetGenerator("tabular"),"table-question-answering":snippetGenerator("tableQuestionAnswering",model=>{const data=JSON.parse(getModelInputSnippet(model));return{query:data.query,table:JSON.stringify(data.table)}}),"text-classification":snippetGenerator("basic"),"text-generation":snippetGenerator("basic"),"text-to-audio":snippetGenerator("textToAudio"),"text-to-image":snippetGenerator("textToImage"),"text-to-speech":snippetGenerator("textToSpeech"),"text-to-video":snippetGenerator("textToVideo"),"token-classification":snippetGenerator("basic"),translation:snippetGenerator("basic"),"zero-shot-classification":snippetGenerator("zeroShotClassification"),"zero-shot-image-classification":snippetGenerator("zeroShotImageClassification")};function getInferenceSnippets(model,provider,inferenceProviderMapping,opts){return model.pipeline_tag&&model.pipeline_tag in snippets?snippets[model.pipeline_tag]?.(model,provider,inferenceProviderMapping,opts)??[]:[]}function formatBody(obj,format2){switch(format2){case"curl":return indentString(formatBody(obj,"json"));case"json":return JSON.stringify(obj,null,4).split("\n").slice(1,-1).join("\n");case"python":return indentString(Object.entries(obj).map(([key,value])=>`${key}=${JSON.stringify(value,null,4).replace(/"/g,'"')},`).join("\n"));case"ts":return formatTsObject(obj).split("\n").slice(1,-1).join("\n");default:throw new Error(`Unsupported format: ${format2}`)}}function formatTsObject(obj,depth){if(depth=depth??0,"object"!=typeof obj||null===obj)return JSON.stringify(obj);if(Array.isArray(obj)){return`[\n${obj.map(item=>{const formatted=formatTsObject(item,depth+1);return`${" ".repeat(4*(depth+1))}${formatted},`}).join("\n")}\n${" ".repeat(4*depth)}]`}return`{\n${Object.entries(obj).map(([key,value])=>{const formattedValue=formatTsObject(value,depth+1),keyStr=/^[a-zA-Z_$][a-zA-Z0-9_$]*$/.test(key)?key:`"${key}"`;return`${" ".repeat(4*(depth+1))}${keyStr}: ${formattedValue},`}).join("\n")}\n${" ".repeat(4*depth)}}`}function indentString(str){return str.split("\n").map(line=>" ".repeat(4)+line).join("\n")}async function*streamChatApi(state2,newMsgContent,signal){console.log("[DEBUG] streamChatApi called",{provider:state2.provider,model:state2.model});const{provider:provider,model:model,temperature:temperature,systemPrompt:systemPrompt}=state2,messages=state2.sessions.find(s=>s.id===state2.currentSessionId)?.messages||[];console.log("[DEBUG] Getting credentials for provider:",provider);const credentials=getCurrentProviderCredentials();if(!credentials)throw console.error("[DEBUG] No credentials found for provider:",provider),new Error(`No credentials configured for provider: ${provider}`);console.log("[DEBUG] Credentials:",credentials);let url="",headers={"Content-Type":"application/json"};const history=[];systemPrompt&&history.push({role:"system",content:systemPrompt});const rawMessages=messages.map(m=>({role:m.role,content:m.content}));if(rawMessages.push({role:"user",content:newMsgContent}),"openai"===provider||"openrouter"===provider){credentials.apiKey&&(headers.Authorization=`Bearer ${credentials.apiKey}`),history.push(...rawMessages),url="openai"===provider?"https://api.openai.com/v1/chat/completions":"https://openrouter.ai/api/v1/chat/completions";const body={model:model,messages:history,stream:!0,temperature:temperature},response=await fetch(url,{method:"POST",headers:headers,body:JSON.stringify(body),signal:signal});if(!response.ok)throw new Error(await response.text());const reader=response.body.getReader(),decoder=new TextDecoder;for(;;){const{done:done,value:value}=await reader.read();if(done)break;const lines=decoder.decode(value,{stream:!0}).split("\n");for(const line of lines)if(line.startsWith("data: ")&&!line.includes("[DONE]"))try{const json=JSON.parse(line.replace("data: ","")),content=json.choices?.[0]?.delta?.content||"";content&&(yield content)}catch(e){}}}else if("ollama"===provider){url=`${credentials.endpoint?credentials.endpoint.replace(/\/$/,""):"http://localhost:11434"}/api/chat`;const body={model:model,messages:rawMessages.map(msg=>{if(Array.isArray(msg.content)){let text="";return msg.content.forEach(part=>{"text"===part.type&&(text+=part.text)}),{role:msg.role,content:text}}return msg}),stream:!0};systemPrompt&&(body.system=systemPrompt),void 0!==temperature&&(body.options={temperature:temperature});const response=await fetch(url,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify(body),signal:signal});if(!response.ok){const errorText=await response.text();throw new Error(`Ollama error ${response.status}: ${errorText}`)}const reader=response.body.getReader(),decoder=new TextDecoder;for(;;){const{done:done,value:value}=await reader.read();if(done)break;const lines=decoder.decode(value,{stream:!0}).split("\n");for(const line of lines)if(line.trim())try{const json=JSON.parse(line);json.message?.content&&(yield json.message.content)}catch(e){}}}else if("huggingface"===provider){url="https://router.huggingface.co/v1/chat/completions";const hfMessages=rawMessages.map(m=>{const txt=Array.isArray(m.content)?m.content.find(c=>"text"===c.type)?.text:m.content;return{role:m.role,content:txt}});systemPrompt&&hfMessages.unshift({role:"system",content:systemPrompt});const hfBody={model:model,messages:hfMessages,max_tokens:2048,temperature:temperature,stream:!0},hfRes=await fetch(url,{method:"POST",headers:{Authorization:`Bearer ${credentials.apiKey}`,"Content-Type":"application/json"},body:JSON.stringify(hfBody),signal:signal});if(!hfRes.ok){const errorText=await hfRes.text();if(404===hfRes.status||422===hfRes.status)throw new Error(`Model "${model}" not found. Check the model name and provider format.`);throw new Error(`Hugging Face API error (${hfRes.status}): ${errorText}`)}const reader=hfRes.body.getReader(),decoder=new TextDecoder;for(;;){const{done:done,value:value}=await reader.read();if(done)break;const lines=decoder.decode(value,{stream:!0}).split("\n");for(const line of lines)if(line.startsWith("data: ")&&!line.includes("[DONE]"))try{const json=JSON.parse(line.replace("data: ","")),content=json.choices?.[0]?.delta?.content||"";content&&(yield content)}catch(e){}}}}async function generateChatTitle(state2,messages){if(!messages||messages.length<2)return null;const{provider:provider,model:model}=state2,credentials=getCurrentProviderCredentials();if(!credentials)return null;if(!("openai"===provider&&credentials.apiKey||"openrouter"===provider&&credentials.apiKey||"anthropic"===provider&&credentials.apiKey||"huggingface"===provider&&credentials.apiKey||"ollama"===provider&&credentials.endpoint))return null;const titlePrompt=`Generate a very short title (max 5 words) for this conversation. Reply with ONLY the title, no quotes or punctuation:\n\n${messages.slice(0,4).map(m=>{let text="";return"string"==typeof m.content?text=m.content:Array.isArray(m.content)&&(text=m.content.find(c=>"text"===c.type)?.text||"[media]"),`${m.role}: ${text.slice(0,200)}`}).join("\n")}`;try{let generatedTitle="";if("openai"===provider||"openrouter"===provider){const url="openai"===provider?"https://api.openai.com/v1/chat/completions":"https://openrouter.ai/api/v1/chat/completions",response=await fetch(url,{method:"POST",headers:{"Content-Type":"application/json",Authorization:`Bearer ${credentials.apiKey}`},body:JSON.stringify({model:model,messages:[{role:"user",content:titlePrompt}],max_tokens:20,temperature:.3})});if(!response.ok)return null;const json=await response.json();generatedTitle=json.choices?.[0]?.message?.content?.trim()||""}else if("ollama"===provider){const base=credentials.endpoint?.replace(/\/$/,"")||"http://localhost:11434",response=await fetch(`${base}/api/chat`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({model:model,messages:[{role:"user",content:titlePrompt}],stream:!1})});if(!response.ok)return null;const json=await response.json();generatedTitle=json.message?.content?.trim()||""}else if("huggingface"===provider){const response=await fetch("https://router.huggingface.co/v1/chat/completions",{method:"POST",headers:{"Content-Type":"application/json",Authorization:`Bearer ${credentials.apiKey}`},body:JSON.stringify({model:model,messages:[{role:"user",content:titlePrompt}],max_tokens:20,temperature:.3})});if(!response.ok)return null;const json=await response.json();generatedTitle=json.choices?.[0]?.message?.content?.trim()||""}return generatedTitle=generatedTitle.replace(/^["']|["']$/g,"").trim(),generatedTitle.length>50&&(generatedTitle=generatedTitle.slice(0,47)+"..."),generatedTitle||null}catch(err){return console.warn("Failed to generate chat title:",err),null}}"undefined"!=typeof chrome&&chrome.storage||(console.log("Running in web preview mode - mocking Chrome APIs"),window.chrome={storage:{local:{get:async keys=>{const result={};return(Array.isArray(keys)?keys:[keys]).forEach(key=>{const val=localStorage.getItem(key);val&&(result[key]=JSON.parse(val))}),result},set:async items=>{Object.entries(items).forEach(([key,value])=>{localStorage.setItem(key,JSON.stringify(value))})}}},runtime:{onInstalled:{addListener:()=>{}},onMessage:{addListener:()=>{}}},tabs:{query:async()=>[{id:1,url:"https://example.com"}]},scripting:{executeScript:async({target:target,func:func})=>[{result:"This is a mock page content from the web preview mode.\n\nIt simulates what the extension would read from a real webpage."}]},sidePanel:{setPanelBehavior:async()=>{}}});let abortController=null,pendingAttachments=[],isTitleGenerationInProgress=!1;async function generatePendingTitles(){if(isTitleGenerationInProgress||!state.provider||!state.model)return;const sessionsNeedingTitles=state.sessions.filter(s=>("New Chat"===s.title||s.needsAITitle)&&s.messages&&s.messages.length>=2);if(0!==sessionsNeedingTitles.length){isTitleGenerationInProgress=!0;try{for(const session of sessionsNeedingTitles)if(session.messages&&!(session.messages.length<2))try{const title=await generateChatTitle(state,session.messages);title&&(updateSessionTitle(session.id,title),renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession))}catch(err){console.warn("Failed to generate title for session:",session.id,err)}}finally{isTitleGenerationInProgress=!1}}}async function generateSessionTitle(session){if(session&&state.provider&&state.model&&!(session.messages.length<2)&&("New Chat"===session.title||session.needsAITitle))try{const title=await generateChatTitle(state,session.messages);title&&(updateSessionTitle(session.id,title),renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession))}catch(err){console.warn("Failed to generate session title:",err)}}function saveCurrentProviderCredentials(){const currentProvider=state.provider;if(!currentProvider)return;let credentials={};switch(currentProvider){case"openai":case"openrouter":case"anthropic":case"huggingface":credentials.apiKey=elements.apiKeyInput.value.trim();break;case"ollama":credentials.endpoint=elements.endpointInput.value.trim()}updateProviderCredentials(currentProvider,credentials)}function loadProviderCredentials(provider){const credentials=getProviderCredentials(provider);if(credentials){switch(elements.apiKeyInput.value="",elements.endpointInput.value="",provider){case"openai":case"openrouter":case"anthropic":case"huggingface":credentials.apiKey&&(elements.apiKeyInput.value=credentials.apiKey);break;case"ollama":credentials.endpoint&&(elements.endpointInput.value=credentials.endpoint)}credentials.models&&credentials.models.length>0&&(populateModelSelect(credentials.models),elements.modelSelectionDiv.classList.remove("hidden"),elements.advancedSettings.classList.remove("hidden"),elements.startChatBtn.classList.remove("hidden"),credentials.selectedModel&&(elements.modelSelect.value=credentials.selectedModel))}}document.addEventListener("DOMContentLoaded",async()=>{if(await async function(){const stored=await chrome.storage.local.get(["chatState"]);if(stored.chatState){if(!stored.chatState.providerCredentials){if(stored.chatState.providerCredentials={openai:{apiKey:"",selectedModel:"",models:[]},openrouter:{apiKey:"",selectedModel:"",models:[]},anthropic:{apiKey:"",selectedModel:"",models:[]},huggingface:{apiKey:"",selectedModel:"",models:[]},ollama:{endpoint:"http://localhost:11434",selectedModel:"",models:[]}},stored.chatState.apiKey){const provider=stored.chatState.provider;"openai"!==provider&&"openrouter"!==provider&&"anthropic"!==provider&&"huggingface"!==provider||(stored.chatState.providerCredentials[provider].apiKey=stored.chatState.apiKey)}stored.chatState.endpoint&&(stored.chatState.providerCredentials.ollama.endpoint=stored.chatState.endpoint||"http://localhost:11434")}if(state.providerCredentials.ollama.endpoint||(state.providerCredentials.ollama.endpoint="http://localhost:11434"),Object.keys(state.providerCredentials).forEach(provider=>{state.providerCredentials[provider].models||(state.providerCredentials[provider].models=[]),state.providerCredentials[provider].selectedModel||(state.providerCredentials[provider].selectedModel="")}),stored.chatState.messages&&!stored.chatState.sessions){const legacySession={id:createId(),title:"Previous Chat",messages:stored.chatState.messages,lastModified:Date.now()};state={...state,...stored.chatState,sessions:[legacySession],currentSessionId:legacySession.id},delete state.messages}else state={...state,...stored.chatState};void 0===state.temperature&&(state.temperature=.7),void 0===state.autoRead&&(state.autoRead=!1),state.quickPrompts||(state.quickPrompts=DEFAULT_QUICK_PROMPTS),state.endpoint||(state.endpoint="http://localhost:11434"),state.awsRegion||(state.awsRegion="us-east-1"),0===state.sessions.length?createNewSession():state.currentSessionId&&state.sessions.find(s=>s.id===state.currentSessionId)||(state.currentSessionId=state.sessions[0].id)}else createNewSession();return state}(),applyTheme(state.theme),isFullPageMode()?(document.body.classList.add("full-page-mode"),elements.expandBtn&&elements.expandBtn.classList.add("hidden"),elements.collapseBtn&&elements.collapseBtn.classList.remove("hidden")):elements.collapseBtn&&elements.collapseBtn.classList.add("hidden"),document.addEventListener("click",e=>{const header=e.target.closest(".think-header");if(header){const thinkBlock=header.closest(".think-block");thinkBlock&&thinkBlock.classList.toggle("expanded")}}),document.addEventListener("click",e=>{if(e.target.classList.contains("copy-btn")){const codeWrapper=e.target.closest(".code-wrapper");if(codeWrapper){const code=codeWrapper.querySelector("code");if(code){const text=code.textContent;navigator.clipboard.writeText(text).then(()=>{e.target.textContent="Copied!",e.target.classList.add("copied"),setTimeout(()=>{e.target.textContent="Copy",e.target.classList.remove("copied")},2e3)}).catch(err=>{console.error("Failed to copy:",err),e.target.textContent="Error",setTimeout(()=>{e.target.textContent="Copy"},2e3)})}}}}),document.addEventListener("keydown",e=>{const modifier=navigator.platform.toUpperCase().indexOf("MAC")>=0?e.metaKey:e.ctrlKey;return modifier&&"Enter"===e.key?(e.preventDefault(),void(elements.chatSection.classList.contains("hidden")||elements.sendBtn.click())):modifier&&"n"===e.key?(e.preventDefault(),void(elements.newChatBtn.classList.contains("hidden")||elements.newChatBtn.click())):modifier&&"k"===e.key?(e.preventDefault(),void(elements.historySidebar.classList.contains("open")?elements.historySearch.focus():(elements.historySearch.value="",renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession),toggleSidebar(!0),setTimeout(()=>elements.historySearch.focus(),100)))):"Escape"===e.key?abortController?void abortController.abort():elements.historySidebar.classList.contains("open")?void toggleSidebar(!1):void 0:modifier&&"/"===e.key?(e.preventDefault(),void(elements.chatSection.classList.contains("hidden")||elements.messageInput.focus())):void 0}),state.provider&&(elements.providerSelect.value=state.provider,loadProviderCredentials(state.provider),handleProviderChange(state.provider)),state.systemPrompt&&(elements.systemPromptInput.value=state.systemPrompt),state.temperature&&(elements.temperatureInput.value=state.temperature,elements.tempValueLabel.textContent=state.temperature),state.quickPrompts&&(elements.quickPromptsInput.value=state.quickPrompts),state.autoRead&&(elements.autoReadInput.checked=state.autoRead),state.model){const session=getCurrentSession();session&&session.messages.length>0&&(elements.modelSelect.innerHTML=`<option value="${state.model}" selected>${state.model}</option>`,elements.modelSelectionDiv.classList.remove("hidden"),elements.advancedSettings.classList.remove("hidden"),elements.startChatBtn.classList.remove("hidden"),switchToChat())}}),elements.providerSelect.addEventListener("change",e=>handleProviderChange(e.target.value));const MANUAL_MODEL_PROVIDERS=["huggingface","anthropic"];function updateHfTaskHint(task){const hints={chat:"e.g., <code>meta-llama/Llama-3.2-3B-Instruct</code>, <code>deepseek-ai/DeepSeek-V3:novita</code>","text-to-image":"e.g., <code>black-forest-labs/FLUX.1-dev</code>, <code>Tongyi-MAI/Z-Image-Turbo</code>","image-to-image":"e.g., <code>black-forest-labs/FLUX.1-dev</code> (attach an image + enter transformation prompt)","text-to-video":"e.g., <code>ali-vilab/text-to-video-ms-1.7b</code>, <code>cerspense/zeroscope_v2_576w</code>","image-to-text":"e.g., <code>Salesforce/blip-image-captioning-large</code>","text-to-speech":"e.g., <code>facebook/mms-tts-eng</code>, <code>suno/bark-small</code>","speech-to-text":"e.g., <code>openai/whisper-large-v3</code>"};elements.modelHint.innerHTML=hints[task]||hints.chat}function handleProviderChange(provider){saveCurrentProviderCredentials(),updateState({provider:provider}),loadProviderCredentials(provider),function(provider){if(elements.apiKeyGroup.classList.add("hidden"),elements.endpointGroup.classList.add("hidden"),elements.hfTaskGroup.classList.add("hidden"),elements.hfProviderGroup.classList.add("hidden"),"ollama"===provider?elements.endpointGroup.classList.remove("hidden"):elements.apiKeyGroup.classList.remove("hidden"),"huggingface"===provider){elements.hfTaskGroup.classList.remove("hidden"),elements.hfProviderGroup.classList.remove("hidden");const credentials=getProviderCredentials(provider);credentials?.task&&(elements.hfTaskSelect.value=credentials.task),credentials?.hfProvider&&(elements.hfProviderSelect.value=credentials.hfProvider)}}(provider),function(provider){MANUAL_MODEL_PROVIDERS.includes(provider)?(elements.manualModelGroup.classList.remove("hidden"),"huggingface"===provider?updateHfTaskHint(elements.hfTaskSelect.value||"chat"):"anthropic"===provider&&(elements.modelHint.innerHTML="e.g., <code>claude-sonnet-4-20250514</code>, <code>claude-3-5-haiku-20241022</code>")):elements.manualModelGroup.classList.add("hidden")}(provider),elements.modelSelectionDiv.classList.add("hidden"),elements.advancedSettings.classList.add("hidden"),elements.startChatBtn.classList.add("hidden")}function handleSwitchSession(id){!function(id){state.sessions.find(s=>s.id===id)&&(state.currentSessionId=id,saveState())}(id),switchToChat(),toggleSidebar(!1)}function handleDeleteSession(id){confirm("Delete this chat?")&&(!function(id){state.sessions=state.sessions.filter(s=>s.id!==id),state.currentSessionId===id&&(state.sessions.length>0?state.currentSessionId=state.sessions[0].id:createNewSession()),saveState()}(id),renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession),state.currentSessionId!==id&&switchToChat())}function switchToChat(){var quickPromptsText,onChipClick;toggleView("chat"),quickPromptsText=state.quickPrompts,onChipClick=prompt=>{elements.messageInput.value=prompt+" ",elements.messageInput.focus()},elements.promptChipsContainer.innerHTML="",quickPromptsText.split("\n").forEach(line=>{if(!line.trim())return;const parts=line.split("|"),label=parts[0].trim(),prompt=parts.length>1?parts.slice(1).join("|").trim():label;if(!label)return;const btn=document.createElement("button");btn.className="chip",btn.textContent=label,btn.title=prompt,btn.addEventListener("click",()=>onChipClick(prompt)),elements.promptChipsContainer.appendChild(btn)});const session=getCurrentSession();renderChat(session?session.messages:[],state.model),elements.expandBtn&&!isFullPageMode()&&elements.expandBtn.classList.remove("hidden")}var callbacks;async function sendMessage(){const text=elements.messageInput.value.trim();if(!text&&0===pendingAttachments.length)return;const session=getCurrentSession();if(!session)return;stopSpeaking();const includePage=elements.includePageContent.checked;let finalText=text;if(toggleLoading(!0),includePage){const pageText=await async function(){try{const[tab]=await chrome.tabs.query({active:!0,currentWindow:!0});if(!tab)return null;if(!tab.url)return"Cannot access page URL. Please ensure the extension has the required permissions.";if(tab.url.startsWith("chrome://")||tab.url.startsWith("about:")||tab.url.startsWith("moz-extension://")||tab.url.startsWith("chrome-extension://"))return"Cannot read system or extension pages.";const results=await chrome.scripting.executeScript({target:{tabId:tab.id},func:()=>document.body.innerText});if(results&&results[0]&&results[0].result){const text=results[0].result;return text.length>2e4?text.substring(0,2e4)+"\n...[Content Truncated]":text}return null}catch(err){return console.error("Failed to read page:",err),navigator.userAgent.includes("Firefox")&&(err.message.includes("Missing host permission")||err.message.includes("permission"))?'‚ö†Ô∏è Firefox Permission Required\n\nTo read page content, please enable website access:\n\n1. Go to about:addons\n2. Click on "Sidekick AI"\n3. Go to "Permissions" tab\n4. Enable "Access your data for all websites"\n\nThen try again.':err.message.includes("Extension manifest must request permission")?"‚ö†Ô∏è Permission Required\n\nThe extension doesn't have permission to read this page. Please check extension permissions in your browser settings.":`Error reading page: ${err.message}`}}();pageText&&(finalText+=`\n\n[Page Content]:\n${pageText}`)}const provider=state.provider,hfTask="huggingface"===provider?getProviderCredentials("huggingface")?.task||"chat":null;if("huggingface"===provider&&"chat"!==hfTask)return void(await async function(task,text,session){const credentials=getProviderCredentials("huggingface"),model=state.model,apiKey=credentials.apiKey,hfProvider=credentials.hfProvider||"auto";elements.messageInput.value="",autoResizeInput();try{switch(task){case"text-to-image":{session.messages.push({role:"user",content:text}),updateCurrentSession(session.messages),appendMessageToDOM("user",text,null,!1,session.messages.length-1);const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length);const imageUrl=await async function(model,prompt,apiKey,options={}){const client=new InferenceClient(apiKey),provider=options.provider||"auto",blob=await client.textToImage({provider:provider,model:model,inputs:prompt,parameters:{guidance_scale:options.guidanceScale||7.5,num_inference_steps:options.steps||5,width:options.width||512,height:options.height||512,negative_prompt:options.negativePrompt||""}});return new Promise((resolve,reject)=>{const reader=new FileReader;reader.onloadend=()=>resolve(reader.result),reader.onerror=reject,reader.readAsDataURL(blob)})}(model,text,apiKey,{provider:hfProvider});removeMessage(msgId);const responseContent=[{type:"generated_image",url:imageUrl,prompt:text}];session.messages.push({role:"assistant",content:responseContent}),updateCurrentSession(session.messages),appendMessageToDOM("assistant",responseContent,null,!1,session.messages.length-1);break}case"image-to-image":{if(0===pendingAttachments.length)return showStatus("Please attach an image first.","error"),void toggleLoading(!1);if(!text)return showStatus('Please enter a transformation prompt (e.g., "Turn the cat into a tiger").',"error"),void toggleLoading(!1);const imageData=pendingAttachments[0].base64,userContent=[{type:"text",text:text},{type:"image_url",image_url:{url:imageData}}];session.messages.push({role:"user",content:userContent}),updateCurrentSession(session.messages),appendMessageToDOM("user",userContent,null,!1,session.messages.length-1),pendingAttachments=[],renderAttachments([],()=>{});const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length);const transformedImageUrl=await async function(model,imageData,prompt,apiKey,options={}){const client=new InferenceClient(apiKey),provider=options.provider||"fal-ai";let imageBlob;if("string"==typeof imageData&&imageData.startsWith("data:")){const base64=imageData.split(",")[1],mimeType=imageData.split(";")[0].split(":")[1]||"image/png",binaryData=Uint8Array.from(atob(base64),c=>c.charCodeAt(0));imageBlob=new Blob([binaryData],{type:mimeType})}else imageBlob=imageData instanceof Blob?imageData:new Blob([imageData],{type:"image/png"});const blob=await client.imageToImage({provider:provider,model:model,inputs:imageBlob,parameters:{prompt:prompt,guidance_scale:options.guidanceScale||7.5,num_inference_steps:options.steps||25,strength:options.strength||.8,negative_prompt:options.negativePrompt||""}});return new Promise((resolve,reject)=>{const reader=new FileReader;reader.onloadend=()=>resolve(reader.result),reader.onerror=reject,reader.readAsDataURL(blob)})}(model,imageData,text,apiKey,{provider:hfProvider});removeMessage(msgId);const responseContent=[{type:"generated_image",url:transformedImageUrl,prompt:text}];session.messages.push({role:"assistant",content:responseContent}),updateCurrentSession(session.messages),appendMessageToDOM("assistant",responseContent,null,!1,session.messages.length-1);break}case"image-to-text":{if(0===pendingAttachments.length)return showStatus("Please attach an image first.","error"),void toggleLoading(!1);const imageData=pendingAttachments[0].base64,userContent=[{type:"text",text:text||"Describe this image"},{type:"image_url",image_url:{url:imageData}}];session.messages.push({role:"user",content:userContent}),updateCurrentSession(session.messages),appendMessageToDOM("user",userContent,null,!1,session.messages.length-1),pendingAttachments=[],renderAttachments([],()=>{});const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length);const caption=await async function(model,imageData,apiKey,options={}){const client=new InferenceClient(apiKey),provider=options.provider||"auto";let imageBlob;if("string"==typeof imageData&&imageData.startsWith("data:")){const base64=imageData.split(",")[1],mimeType=imageData.split(";")[0].split(":")[1],binaryData=Uint8Array.from(atob(base64),c=>c.charCodeAt(0));imageBlob=new Blob([binaryData],{type:mimeType})}else imageBlob=imageData instanceof Blob?imageData:new Blob([imageData]);const result=await client.imageToText({provider:provider,model:model,data:imageBlob});return result.generated_text||JSON.stringify(result)}(model,imageData,apiKey,{provider:hfProvider});removeMessage(msgId),session.messages.push({role:"assistant",content:caption}),updateCurrentSession(session.messages),appendMessageToDOM("assistant",caption,null,!1,session.messages.length-1);break}case"text-to-video":{session.messages.push({role:"user",content:text}),updateCurrentSession(session.messages),appendMessageToDOM("user",text,null,!1,session.messages.length-1);const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length),showStatus("Generating video... This may take 30-120 seconds.","info");const videoUrl=await async function(model,prompt,apiKey,options={}){const client=new InferenceClient(apiKey),provider=options.provider||"auto",blob=await client.textToVideo({provider:provider,model:model,inputs:prompt,parameters:{num_frames:options.numFrames||16,num_inference_steps:options.steps||25,guidance_scale:options.guidanceScale||9,negative_prompt:options.negativePrompt||"low quality, blurry, distorted"}});return new Promise((resolve,reject)=>{const reader=new FileReader;reader.onloadend=()=>resolve(reader.result),reader.onerror=reject,reader.readAsDataURL(blob)})}(model,text,apiKey,{provider:hfProvider});removeMessage(msgId);const responseContent=[{type:"generated_video",url:videoUrl,prompt:text}];session.messages.push({role:"assistant",content:responseContent}),updateCurrentSession(session.messages),appendMessageToDOM("assistant",responseContent,null,!1,session.messages.length-1);break}case"text-to-speech":{session.messages.push({role:"user",content:text}),updateCurrentSession(session.messages),appendMessageToDOM("user",text,null,!1,session.messages.length-1);const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length);const audioUrl=await async function(model,text,apiKey,options={}){const client=new InferenceClient(apiKey),provider=options.provider||"auto",blob=await client.textToSpeech({provider:provider,model:model,inputs:text});return new Promise((resolve,reject)=>{const reader=new FileReader;reader.onloadend=()=>resolve(reader.result),reader.onerror=reject,reader.readAsDataURL(blob)})}(model,text,apiKey,{provider:hfProvider});removeMessage(msgId);const responseContent=[{type:"generated_audio",url:audioUrl}];session.messages.push({role:"assistant",content:responseContent}),updateCurrentSession(session.messages),appendMessageToDOM("assistant",responseContent,null,!1,session.messages.length-1);break}case"speech-to-text":{if(0===pendingAttachments.length)return showStatus("Please attach an audio file first.","error"),void toggleLoading(!1);const audioData=pendingAttachments[0].base64,userContent=[{type:"audio_input",url:audioData}];session.messages.push({role:"user",content:userContent}),updateCurrentSession(session.messages),appendMessageToDOM("user",userContent,null,!1,session.messages.length-1),pendingAttachments=[],renderAttachments([],()=>{});const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length);const transcription=await async function(model,audioData,apiKey,options={}){const client=new InferenceClient(apiKey),provider=options.provider||"auto";let audioBlob;if("string"==typeof audioData&&audioData.startsWith("data:")){const base64=audioData.split(",")[1],mimeType=audioData.split(";")[0].split(":")[1]||"audio/flac",binaryData=Uint8Array.from(atob(base64),c=>c.charCodeAt(0));audioBlob=new Blob([binaryData],{type:mimeType})}else audioBlob=audioData instanceof Blob?audioData:new Blob([audioData],{type:"audio/flac"});const result=await client.automaticSpeechRecognition({provider:provider,model:model,data:audioBlob});return result.text||JSON.stringify(result)}(model,audioData,apiKey,{provider:hfProvider});removeMessage(msgId),session.messages.push({role:"assistant",content:transcription}),updateCurrentSession(session.messages),appendMessageToDOM("assistant",transcription,null,!1,session.messages.length-1);break}default:showStatus(`Unknown task: ${task}`,"error")}"New Chat"===session.title&&session.messages.length>=2&&generateSessionTitle(session)}catch(err){appendMessageToDOM("error",`Error: ${err.message}`)}finally{toggleLoading(!1)}}(hfTask,finalText,session));let messageContent;pendingAttachments.length>0?(messageContent=[],finalText&&messageContent.push({type:"text",text:finalText}),pendingAttachments.forEach(att=>{messageContent.push({type:"image_url",image_url:{url:att.base64}})})):messageContent=finalText,session.messages.push({role:"user",content:messageContent}),updateCurrentSession(session.messages);const userMsgIndex=session.messages.length-1;appendMessageToDOM("user",messageContent,null,!1,userMsgIndex),elements.messageInput.value="",elements.includePageContent.checked=!1,autoResizeInput(),pendingAttachments=[],renderAttachments([],()=>{});const msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length),abortController=new AbortController;let fullResponse="";try{const stream=streamChatApi(state,messageContent,abortController.signal);for await(const chunk of stream)fullResponse+=chunk,updateStreamingMessage(msgId,fullResponse);session.messages.push({role:"assistant",content:fullResponse}),updateCurrentSession(session.messages),updateTokenCount(session.messages),finalizeMessageInDOM(msgId,fullResponse),state.autoRead&&speakText(fullResponse),"New Chat"===session.title&&session.messages.length>=2&&generateSessionTitle(session)}catch(err){"AbortError"!==err.name?(removeMessage(msgId),appendMessageToDOM("error",`Error: ${err.message}`)):fullResponse&&(session.messages.push({role:"assistant",content:fullResponse}),updateCurrentSession(session.messages),finalizeMessageInDOM(msgId,fullResponse))}finally{toggleLoading(!1),abortController=null}}elements.hfTaskSelect.addEventListener("change",()=>{const task=elements.hfTaskSelect.value;updateProviderCredentials("huggingface",{task:task}),updateHfTaskHint(task);const taskModels=HF_TASK_MODELS[task]||[];populateModelSelect(taskModels),taskModels.length>0&&(elements.modelSelectionDiv.classList.remove("hidden"),elements.advancedSettings.classList.remove("hidden"),elements.startChatBtn.classList.remove("hidden")),updateProviderCredentials("huggingface",{models:taskModels})}),elements.hfProviderSelect.addEventListener("change",()=>{updateProviderCredentials("huggingface",{hfProvider:elements.hfProviderSelect.value})}),elements.themeBtn.addEventListener("click",()=>{const newTheme="light"===state.theme?"dark":"light";updateState({theme:newTheme}),applyTheme(newTheme)}),elements.expandBtn&&elements.expandBtn.addEventListener("click",()=>{chrome.tabs.create({url:chrome.runtime.getURL("sidepanel.html")}),window.close&&window.close()}),elements.collapseBtn&&elements.collapseBtn.addEventListener("click",()=>{const isFirefox="undefined"!=typeof browser||navigator.userAgent.includes("Firefox");alert(isFirefox?"To return to popup mode:\n\n1. Close this tab\n2. Click the extension icon in your browser toolbar\n\nThe popup will open with your chat history preserved.":"To return to sidebar mode:\n\n1. Close this tab\n2. Click the extension icon in your browser toolbar\n\nThe sidebar will open with your chat history preserved.")}),document.querySelectorAll(".gallery-tab").forEach(tab=>{tab.addEventListener("click",()=>{document.querySelectorAll(".gallery-tab").forEach(t=>t.classList.remove("active")),tab.classList.add("active");const filter=tab.dataset.filter,session=getCurrentSession();session&&function(messages,filter="all"){elements.galleryGrid&&(elements.galleryGrid.innerHTML="",messages.forEach(msg=>{"assistant"===msg.role&&Array.isArray(msg.content)&&msg.content.forEach(part=>{if("generated_image"!==part.type||"all"!==filter&&"image"!==filter){if("generated_video"===part.type&&("all"===filter||"video"===filter)){const card=createMediaCard("video",part.url,part.prompt);elements.galleryGrid.appendChild(card)}}else{const card=createMediaCard("image",part.url,part.prompt);elements.galleryGrid.appendChild(card)}})}))}(session.messages,filter)})}),elements.settingsBtn.addEventListener("click",()=>toggleView("config")),elements.startChatBtn.addEventListener("click",()=>{const selectedModel=elements.modelSelect.value;if(!selectedModel)return showStatus("Please select a model.","error");updateState({model:selectedModel,systemPrompt:elements.systemPromptInput.value.trim(),temperature:parseFloat(elements.temperatureInput.value),quickPrompts:elements.quickPromptsInput.value.trim(),autoRead:elements.autoReadInput.checked}),switchToChat()}),elements.historyBtn.addEventListener("click",()=>{elements.historySearch.value="",renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession),toggleSidebar(!0)}),elements.closeSidebarBtn.addEventListener("click",()=>toggleSidebar(!1)),elements.historySearch.addEventListener("input",e=>{const query=e.target.value;renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession,query)}),elements.newChatBtn.addEventListener("click",()=>{generatePendingTitles(),createNewSession(),switchToChat()}),elements.clearHistoryBtn.addEventListener("click",()=>{confirm("Delete all history? This cannot be undone.")&&(state.sessions=[],createNewSession(),renderSessionList(state.sessions,state.currentSessionId,handleSwitchSession,handleDeleteSession),switchToChat())}),elements.exportBtn.addEventListener("click",()=>{const session=getCurrentSession();if(!session||0===session.messages.length)return alert("No history to export.");let menu=document.getElementById("exportMenu");if(menu)return void menu.remove();menu=document.createElement("div"),menu.id="exportMenu",menu.className="export-menu",menu.innerHTML='\n    <button class="export-option" data-format="md">üìù Markdown (.md)</button>\n    <button class="export-option" data-format="json">üì¶ JSON (.json)</button>\n  ';const rect=elements.exportBtn.getBoundingClientRect();menu.style.position="fixed",menu.style.top=rect.bottom+5+"px",menu.style.right=window.innerWidth-rect.right+"px",document.body.appendChild(menu),menu.addEventListener("click",ev=>{const format2=ev.target.dataset.format;format2&&(!function(format2){const session=getCurrentSession();if(!session||0===session.messages.length)return alert("No history to export.");let content,mimeType,extension;if("json"===format2){const exportData={title:session.title,id:session.id,exportedAt:(new Date).toISOString(),messages:session.messages.map(msg=>({role:msg.role,content:"string"==typeof msg.content?msg.content:msg.content.map(c=>"text"===c.type?{type:"text",text:c.text}:{type:c.type,description:"[media]"})}))};content=JSON.stringify(exportData,null,2),mimeType="application/json",extension="json"}else{let md=`# ${session.title}\n\n`;md+=`*Exported on ${(new Date).toLocaleString()}*\n\n---\n\n`,session.messages.forEach(msg=>{const roleLabel="user"===msg.role?"üë§ **User**":"ü§ñ **Assistant**";if(Array.isArray(msg.content)){const txt=msg.content.find(c=>"text"===c.type)?.text||"";md+=`${roleLabel}\n\n${txt}\n\n*[Media Attached]*\n\n---\n\n`}else md+=`${roleLabel}\n\n${msg.content}\n\n---\n\n`}),content=md,mimeType="text/markdown",extension="md"}const blob=new Blob([content],{type:mimeType}),url=URL.createObjectURL(blob),a=document.createElement("a");a.href=url,a.download=`${session.title.replace(/[^a-z0-9]/gi,"_").toLowerCase()}-${session.id}.${extension}`,a.click(),URL.revokeObjectURL(url)}(format2),menu.remove())}),setTimeout(()=>{document.addEventListener("click",function closeMenu(ev){menu.contains(ev.target)||ev.target===elements.exportBtn||(menu.remove(),document.removeEventListener("click",closeMenu))})},10)}),elements.fileBtn.addEventListener("click",()=>elements.fileInput.click()),elements.pageContextBtn.addEventListener("click",()=>{const checkbox=elements.includePageContent;checkbox.checked=!checkbox.checked,elements.pageContextBtn.classList.toggle("active",checkbox.checked)}),elements.fileInput.addEventListener("change",async e=>{const file=e.target.files[0];if(file){if(file.type.startsWith("image/")){const reader=new FileReader;return reader.onload=evt=>{const base64=evt.target.result;pendingAttachments.push({type:"image_url",base64:base64}),renderAttachments(pendingAttachments,index=>{pendingAttachments.splice(index,1),renderAttachments(pendingAttachments,()=>{})})},reader.readAsDataURL(file),void(e.target.value="")}if(file.type.startsWith("audio/")){const reader=new FileReader;return reader.onload=evt=>{const base64=evt.target.result;pendingAttachments.push({type:"audio",base64:base64,name:file.name}),renderAttachments(pendingAttachments,index=>{pendingAttachments.splice(index,1),renderAttachments(pendingAttachments,()=>{})})},reader.readAsDataURL(file),void(e.target.value="")}try{const text=await function(file){return new Promise((resolve,reject)=>{const reader=new FileReader;reader.onload=e=>resolve(e.target.result),reader.onerror=e=>reject(e),reader.readAsText(file)})}(file);if(/[\x00-\x08\x0E-\x1F]/.test(text))throw new Error("File content appears to be binary.");const context=`\n\n[File Attachment: ${file.name}]\n\`\`\`\n${text}\n\`\`\``;elements.messageInput.value+=context,autoResizeInput()}catch(err){alert("Failed to read file. Please upload images, audio, or text files."),console.error(err)}e.target.value=""}}),elements.fetchModelsBtn.addEventListener("click",async()=>{const provider=elements.providerSelect.value;if(!provider)return showStatus("Select a provider.","error");saveCurrentProviderCredentials();const credentials=getProviderCredentials(provider);if("ollama"!==provider&&!credentials.apiKey)return showStatus("Enter API Key.","error");showStatus("Fetching...","info");try{const models=await async function(provider,credentials){let url="",headers={},transform=data=>[];switch(provider){case"openai":url="https://api.openai.com/v1/models",headers={Authorization:`Bearer ${credentials.apiKey}`},transform=data=>data.data.map(m=>m.id).filter(id=>id.startsWith("gpt")).sort();break;case"openrouter":url="https://openrouter.ai/api/v1/models",headers={Authorization:`Bearer ${credentials.apiKey}`},transform=data=>data.data.map(m=>m.id).sort();break;case"huggingface":return["meta-llama/Llama-3.2-3B-Instruct","meta-llama/Llama-3.1-8B-Instruct","Qwen/Qwen2.5-72B-Instruct","mistralai/Mistral-7B-Instruct-v0.3","deepseek-ai/DeepSeek-R1-Distill-Qwen-32B","google/gemma-2-9b-it"];case"anthropic":return["claude-sonnet-4-20250514","claude-3-5-sonnet-20241022","claude-3-5-haiku-20241022","claude-3-opus-20240229","claude-3-haiku-20240307"];case"ollama":url=`${credentials.endpoint.replace(/\/$/,"")}/api/tags`,transform=data=>data.models.map(m=>m.name);break;default:throw new Error("Unknown provider")}const response=await fetch(url,{headers:headers});if(!response.ok){const errorData=await response.text();throw new Error(`API Error ${response.status}: ${errorData}`)}return transform(await response.json())}(provider,credentials);if(!models.length)throw new Error("No models found.");populateModelSelect(models),showStatus(`Found ${models.length} models.`,"success"),elements.modelSelectionDiv.classList.remove("hidden"),elements.advancedSettings.classList.remove("hidden"),elements.startChatBtn.classList.remove("hidden"),updateProviderCredentials(provider,{models:models})}catch(err){showStatus(`Error: ${err.message}`,"error")}}),elements.modelSelect.addEventListener("change",()=>{const provider=state.provider,selectedModel=elements.modelSelect.value;provider&&selectedModel&&updateProviderCredentials(provider,{selectedModel:selectedModel})}),elements.saveModelBtn.addEventListener("click",()=>{const modelName=elements.manualModelInput.value.trim();if(!modelName)return void showStatus("Please enter a model name.","error");const provider=state.provider;!function(model){if(Array.from(elements.modelSelect.options).find(opt=>opt.value===model))return void(elements.modelSelect.value=model);const option=document.createElement("option");option.value=model,option.textContent=model,elements.modelSelect.appendChild(option),elements.modelSelect.value=model}(modelName);const models=getProviderCredentials(provider).models||[];models.includes(modelName)||models.push(modelName),updateProviderCredentials(provider,{models:models,selectedModel:modelName}),elements.modelSelectionDiv.classList.remove("hidden"),elements.advancedSettings.classList.remove("hidden"),elements.startChatBtn.classList.remove("hidden"),elements.manualModelInput.value="",showStatus(`Model "${modelName}" added.`,"success")}),elements.manualModelInput.addEventListener("keypress",e=>{"Enter"===e.key&&(e.preventDefault(),elements.saveModelBtn.click())}),elements.temperatureInput.addEventListener("input",e=>elements.tempValueLabel.textContent=e.target.value),elements.messageInput.addEventListener("input",autoResizeInput),elements.messageInput.addEventListener("keypress",e=>{"Enter"!==e.key||e.shiftKey||(e.preventDefault(),sendMessage())}),elements.sendBtn.addEventListener("click",sendMessage),elements.stopBtn.addEventListener("click",()=>{abortController&&(abortController.abort(),abortController=null,stopSpeaking(),toggleLoading(!1))}),callbacks={onEdit:function(msgIndex,currentText){const session=getCurrentSession();session&&(elements.messageInput.value=currentText,elements.messageInput.focus(),autoResizeInput(),session.messages=session.messages.slice(0,msgIndex),updateCurrentSession(session.messages),renderChat(session.messages,state.model))},onRegenerate:async function(msgIndex){const session=getCurrentSession();if(!session||msgIndex<1)return;const userMsgIndex=msgIndex-1,userMsg=session.messages[userMsgIndex];if(!userMsg||"user"!==userMsg.role)return;session.messages=session.messages.slice(0,msgIndex),updateCurrentSession(session.messages),renderChat(session.messages,state.model);const userContent=userMsg.content,msgId="msg-"+Date.now();appendMessageToDOM("assistant",null,msgId,!0,session.messages.length),abortController=new AbortController;let fullResponse="";toggleLoading(!0);try{const stream=streamChatApi(state,userContent,abortController.signal);for await(const chunk of stream)fullResponse+=chunk,updateStreamingMessage(msgId,fullResponse);session.messages.push({role:"assistant",content:fullResponse}),updateCurrentSession(session.messages),updateTokenCount(session.messages),finalizeMessageInDOM(msgId,fullResponse),state.autoRead&&speakText(fullResponse)}catch(err){"AbortError"!==err.name?(removeMessage(msgId),appendMessageToDOM("error",`Error: ${err.message}`)):fullResponse&&(session.messages.push({role:"assistant",content:fullResponse}),updateCurrentSession(session.messages),finalizeMessageInDOM(msgId,fullResponse))}finally{toggleLoading(!1),abortController=null}}},messageCallbacks={...messageCallbacks,...callbacks};
